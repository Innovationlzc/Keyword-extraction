{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04a19a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 2304 train documents\n",
      "Sample Content Preview:\n",
      "t enhancing product recommender systems on sparse binary data a commercial recommender systems use various data mining techniques to make appropriate recommendations to users during online real time s...\n",
      "Sample Keywords: ['collaborative filtering', 'customer relationship management', 'e-commerce', 'recommender systems', 'dependency networks']\n",
      "\n",
      "Loaded 2304 test documents\n",
      "Sample Content Preview:\n",
      "t enhancing product recommender systems on sparse binary data a commercial recommender systems use various data mining techniques to make appropriate recommendations to users during online real time s...\n",
      "Sample Keywords: ['recommend system', 'depend network', 'custom relationship manag', 'collabor filter', 'e-commerc']\n",
      "\n",
      "Metric          Training   Test      \n",
      "Precision       0.0786      0.0415\n",
      "Recall          0.0446      0.0261\n",
      "F1              0.0531      0.0297\n",
      "Cos_sim         0.0569      0.0315\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # 更严格的字符过滤\n",
    "    return ' '.join(text.lower().split()).strip()  # 合并多余空格\n",
    "\n",
    "def load_data(data_path, ref_path=None, data_type='train'):\n",
    "    documents = []\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        # 训练集加载逻辑保持不变\n",
    "        for file in os.listdir(data_path):\n",
    "            if file.endswith('.txt'):\n",
    "                base_name = os.path.splitext(file)[0]\n",
    "                txt_path = os.path.join(data_path, file)\n",
    "                key_path = os.path.join(data_path, base_name + '.key')\n",
    "                \n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    content = preprocess(f.read())\n",
    "                \n",
    "                true_keywords = []\n",
    "                if os.path.exists(key_path):\n",
    "                    with open(key_path, 'r', encoding='utf-8') as f:\n",
    "                        true_keywords = [line.strip().lower() for line in f if line.strip()]\n",
    "                \n",
    "                documents.append({\n",
    "                    'doc_id': base_name,\n",
    "                    'content': content,\n",
    "                    'true_keywords': true_keywords\n",
    "                })\n",
    "    \n",
    "    elif data_type == 'test' and ref_path:\n",
    "        # 加载JSON格式的关键词引用\n",
    "        with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "            ref_data = json.load(f)\n",
    "        \n",
    "        for file in os.listdir(data_path):\n",
    "            if file.endswith('.xml'):\n",
    "                doc_id = os.path.splitext(file)[0]\n",
    "                xml_path = os.path.join(data_path, file)\n",
    "                \n",
    "                try:\n",
    "                    # 解析XML内容\n",
    "                    tree = ET.parse(xml_path)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    # 提取文本内容\n",
    "                    content_parts = []\n",
    "                    for sentence in root.findall('.//sentence'):\n",
    "                        for token in sentence.findall('.//token'):\n",
    "                            word_node = token.find('word')\n",
    "                            if word_node is not None and word_node.text:\n",
    "                                word = word_node.text.strip()\n",
    "                                if word not in ['--', '']:\n",
    "                                    content_parts.append(word)\n",
    "                    content = preprocess(' '.join(content_parts))\n",
    "                    \n",
    "                    # 从JSON加载关键词（处理多候选情况）\n",
    "                    true_keywords = []\n",
    "                    if doc_id in ref_data:\n",
    "                        # 合并所有候选关键词并去重\n",
    "                        candidates = ref_data[doc_id]\n",
    "                        true_keywords = list({kw.lower().strip() \n",
    "                                            for group in candidates \n",
    "                                            for kw in group})\n",
    "                    \n",
    "                    documents.append({\n",
    "                        'doc_id': doc_id,\n",
    "                        'content': content,\n",
    "                        'true_keywords': true_keywords\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"\\nLoaded {len(documents)} {data_type} documents\")\n",
    "    if documents:\n",
    "        sample = documents[0]\n",
    "        print(f\"Sample Content Preview:\\n{sample['content'][:200]}...\")\n",
    "        print(f\"Sample Keywords: {sample['true_keywords'][:5]}\")\n",
    "    return documents\n",
    "\n",
    "def textrank_extract(text, topk=5):\n",
    "    try:\n",
    "        if not text or len(text.split()) < 10:\n",
    "            return []\n",
    "        # 增加分句处理提升效果\n",
    "        extracted = keywords(text, \n",
    "                            words=topk*2, \n",
    "                            ratio=0.2, \n",
    "                            split=True, \n",
    "                            scores=False,\n",
    "                            pos_filter=('NN', 'NNS', 'JJ'))  # 过滤名词和形容词\n",
    "        return [kw.lower().strip() for kw in extracted][:topk]\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def evaluate(true_list, pred_list):\n",
    "    # 添加词干处理\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    true_set = {stemmer.stem(kw) for kw in true_list}\n",
    "    pred_set = {stemmer.stem(kw) for kw in pred_list}\n",
    "    \n",
    "    tp = len(true_set & pred_set)\n",
    "    \n",
    "    precision = tp / len(pred_set) if len(pred_set) > 0 else 0\n",
    "    recall = tp / len(true_set) if len(true_set) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    cos_sim = tp / (math.sqrt(len(pred_set)) * math.sqrt(len(true_set))) if len(pred_set)*len(true_set) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cos_sim': cos_sim\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    base_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\Krapivin\\krapivin-2009-pre-master\"\n",
    "    \n",
    "    # 路径配置\n",
    "    train_path = os.path.join(base_path, \"src\", \"all_docs_abstacts_refined\")\n",
    "    test_path = os.path.join(base_path, \"test\")\n",
    "    ref_path = os.path.join(base_path, \"references\", \"test.author.stem.json\")  # 使用词干化版本\n",
    "    \n",
    "    # 加载数据\n",
    "    train_docs = load_data(train_path, data_type='train')\n",
    "    test_docs = load_data(test_path, ref_path=ref_path, data_type='test')\n",
    "    \n",
    "    # 评估逻辑\n",
    "    topk = 5\n",
    "    metrics = ['precision', 'recall', 'f1', 'cos_sim']\n",
    "    \n",
    "    def calculate_avg(docs):\n",
    "        results = {m: [] for m in metrics}\n",
    "        for doc in docs:\n",
    "            pred = textrank_extract(doc['content'], topk)\n",
    "            res = evaluate(doc['true_keywords'], pred)\n",
    "            for m in metrics:\n",
    "                results[m].append(res[m])\n",
    "        return {f'avg_{m}': sum(results[m])/len(docs) for m in metrics}\n",
    "    \n",
    "    train_results = calculate_avg(train_docs)\n",
    "    test_results = calculate_avg(test_docs)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<15} {'Training':<10} {'Test':<10}\")\n",
    "    for m in metrics:\n",
    "        print(f\"{m.capitalize():<15} {train_results[f'avg_{m}']:.4f}      {test_results[f'avg_{m}']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039cde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
