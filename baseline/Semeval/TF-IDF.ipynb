{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f6f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载训练集...\n",
      "发现 350 个文本文件\n",
      "成功加载有效样本：350 个\n",
      "\n",
      "加载测试集...\n",
      "发现 100 个文本文件\n",
      "成功加载有效样本：100 个\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ------------------\n",
    "# 路径配置（根据实际情况修改）\n",
    "# ------------------\n",
    "TRAIN_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "TEST_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "# ------------------\n",
    "# 改进版数据加载\n",
    "# ------------------\n",
    "def load_scienceie_data(data_dir):\n",
    "    \"\"\"只加载有标注文件的样本\"\"\"\n",
    "    documents = []\n",
    "    true_keywords = []\n",
    "    \n",
    "    # 遍历所有txt文件\n",
    "    txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "    print(f\"发现 {len(txt_files)} 个文本文件\")\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        ann_file = txt_file.replace('.txt', '.ann')\n",
    "        ann_path = os.path.join(data_dir, ann_file)\n",
    "        \n",
    "        # 跳过无标注文件\n",
    "        if not os.path.exists(ann_path):\n",
    "            print(f\"跳过无标注文件: {txt_file}\")\n",
    "            continue\n",
    "            \n",
    "        # 读取文本\n",
    "        txt_path = os.path.join(data_dir, txt_file)\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        # 解析标注\n",
    "        keywords = parse_annotation(ann_path)\n",
    "        \n",
    "        documents.append(text)\n",
    "        true_keywords.append(keywords)\n",
    "    \n",
    "    print(f\"成功加载有效样本：{len(documents)} 个\")\n",
    "    return documents, true_keywords\n",
    "\n",
    "def parse_annotation(ann_path):\n",
    "    \"\"\"更健壮的标注解析\"\"\"\n",
    "    keywords = []\n",
    "    try:\n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # 兼容不同标注格式：T1 KeyPhrase 0 5 example 或 T1\\tKeyPhrase 0 5\\texample\n",
    "                if line.startswith('T'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        # 提取标注内容（如：\"KeyPhrase 0 5 example\"）\n",
    "                        content = parts[1]\n",
    "                        keyword = content.split(' ')[-1]  # 获取最后一个元素作为关键词\n",
    "                        keywords.append(keyword.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"解析标注文件 {ann_path} 失败：{str(e)}\")\n",
    "    return list(set(keywords))  # 去重\n",
    "\n",
    "# ------------------\n",
    "# 其他函数保持不变\n",
    "# ------------------\n",
    "# （保持与之前相同的预处理、TF-IDF提取和评估函数）\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载训练集\n",
    "    print(\"加载训练集...\")\n",
    "    train_docs, train_keywords = load_scienceie_data(TRAIN_DIR)\n",
    "    \n",
    "    # 加载测试集\n",
    "    print(\"\\n加载测试集...\")\n",
    "    test_docs, test_keywords = load_scienceie_data(TEST_DIR)\n",
    "    \n",
    "    # 验证数据一致性\n",
    "    assert len(test_docs) == len(test_keywords), \"测试集文本与标注数量不匹配！\"\n",
    "    \n",
    "    # 后续处理保持不变..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0dc183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [274, 1000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m     test_pred_keywords\u001b[38;5;241m.\u001b[39mappend([feature_names[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_indices])\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# 评估\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_science\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_keywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pred_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m测试集表现：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 156\u001b[0m, in \u001b[0;36mevaluate_science\u001b[1;34m(true_list, pred_list)\u001b[0m\n\u001b[0;32m    152\u001b[0m     y_true\u001b[38;5;241m.\u001b[39mextend(true_vec)\n\u001b[0;32m    153\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mextend(pred_vec)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmicro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    159\u001b[0m }\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2131\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1974\u001b[0m     {\n\u001b[0;32m   1975\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2000\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2001\u001b[0m ):\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \n\u001b[0;32m   2004\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2131\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1724\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \n\u001b[0;32m   1568\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m zero_division_value \u001b[38;5;241m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1724\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1727\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1501\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1501\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\apps\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [274, 1000]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# 下载NLTK资源\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ------------------\n",
    "# 自定义路径配置\n",
    "# ------------------\n",
    "TRAIN_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "TEST_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "# ------------------\n",
    "# 科学领域专用停用词\n",
    "# ------------------\n",
    "SCIENCE_STOPWORDS = set([\n",
    "    'figure', 'equation', 'table', 'method', 'result', 'study', \n",
    "    'data', 'research', 'model', 'analysis', 'experiment'\n",
    "])\n",
    "\n",
    "# ------------------\n",
    "# 数据集加载函数\n",
    "# ------------------\n",
    "def load_scienceie_data(data_dir):\n",
    "    \"\"\"\n",
    "    加载ScienceIE数据集\n",
    "    返回：(documents, true_keywords)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    true_keywords = []\n",
    "    \n",
    "    # 遍历所有txt文件\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        # 读取文本内容\n",
    "        txt_path = os.path.join(data_dir, filename)\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        # 读取对应的标注文件（假设为.ann格式）\n",
    "        ann_path = os.path.join(data_dir, filename.replace('.txt', '.ann'))\n",
    "        keywords = parse_annotation(ann_path)\n",
    "        \n",
    "        documents.append(text)\n",
    "        true_keywords.append(keywords)\n",
    "    \n",
    "    return documents, true_keywords\n",
    "\n",
    "def parse_annotation(ann_path):\n",
    "    \"\"\"\n",
    "    解析标注文件获取关键词\n",
    "    格式示例：T1\tKeyPhrase 0 5\texample\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    try:\n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 1:\n",
    "                        keywords.append(parts[1].split(' ')[0])  # 提取第一个标注字段\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return list(set(keywords))  # 去重\n",
    "\n",
    "# ------------------\n",
    "# 文本预处理\n",
    "# ------------------\n",
    "def science_preprocess(text):\n",
    "    \"\"\"\n",
    "    针对科学文献的预处理：\n",
    "    1. 保留大小写（科学术语区分大小写）\n",
    "    2. 移除通用停用词和科学专用停用词\n",
    "    3. 处理特殊符号（保留化学式如H2O）\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 过滤停用词\n",
    "    stop_words = set(stopwords.words('english')) | SCIENCE_STOPWORDS\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # 处理标点（保留包含数字和字母的组合）\n",
    "    tokens = [\n",
    "        token for token in tokens \n",
    "        if not all(c in string.punctuation for c in token)\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# ------------------\n",
    "# TF-IDF关键词提取\n",
    "# ------------------\n",
    "def extract_science_keywords(docs, top_n=10):\n",
    "    \"\"\"\n",
    "    针对科学文献优化的TF-IDF\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 3),  # 支持长术语（如\"machine learning\"）\n",
    "        max_df=0.85,         # 过滤高频词\n",
    "        min_df=2,            # 过滤低频词\n",
    "        stop_words='english',\n",
    "        token_pattern=r'(?u)\\b[A-Za-z0-9_][A-Za-z0-9_]+\\b'  # 保留带数字的词\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    keywords_list = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        row = tfidf_matrix[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-top_n:][::-1]\n",
    "        keywords = [feature_names[idx] for idx in top_indices]\n",
    "        keywords_list.append(keywords)\n",
    "    \n",
    "    return keywords_list\n",
    "\n",
    "# ------------------\n",
    "# 评估指标计算\n",
    "# ------------------\n",
    "def evaluate_science(true_list, pred_list):\n",
    "    \"\"\"\n",
    "    改进的评估方法：\n",
    "    1. 考虑部分匹配（如预测\"neural network\" vs 真实\"neural networks\"）\n",
    "    2. 加权F1-score\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for true_kws, pred_kws in zip(true_list, pred_list):\n",
    "        # 创建匹配集合\n",
    "        matched = set()\n",
    "        for pred in pred_kws:\n",
    "            for true in true_kws:\n",
    "                if pred.lower() in true.lower() or true.lower() in pred.lower():\n",
    "                    matched.add(pred)\n",
    "                    break\n",
    "        \n",
    "        # 生成二进制向量\n",
    "        true_vec = [1] * len(true_kws)\n",
    "        pred_vec = [1 if kw in matched else 0 for kw in pred_kws]\n",
    "        \n",
    "        y_true.extend(true_vec)\n",
    "        y_pred.extend(pred_vec)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision_score(y_true, y_pred, average='micro'),\n",
    "        'recall': recall_score(y_true, y_pred, average='micro'),\n",
    "        'f1': f1_score(y_true, y_pred, average='micro')\n",
    "    }\n",
    "\n",
    "# ------------------\n",
    "# 主流程\n",
    "# ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    train_docs, train_keywords = load_scienceie_data(TRAIN_DIR)\n",
    "    test_docs, test_keywords = load_scienceie_data(TEST_DIR)\n",
    "    \n",
    "    # 预处理\n",
    "    preprocessed_train = [science_preprocess(doc) for doc in train_docs]\n",
    "    preprocessed_test = [science_preprocess(doc) for doc in test_docs]\n",
    "    \n",
    "    # 训练TF-IDF模型（在训练集上）\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_df=0.85,\n",
    "        min_df=2,\n",
    "        token_pattern=r'(?u)\\b[A-Za-z0-9_][A-Za-z0-9_]+\\b'\n",
    "    ).fit(preprocessed_train)\n",
    "    \n",
    "    # 在测试集上提取关键词\n",
    "    tfidf_test = vectorizer.transform(preprocessed_test)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    test_pred_keywords = []\n",
    "    for i in range(tfidf_test.shape[0]):\n",
    "        row = tfidf_test[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-10:][::-1]\n",
    "        test_pred_keywords.append([feature_names[idx] for idx in top_indices])\n",
    "    \n",
    "    # 评估\n",
    "    metrics = evaluate_science(test_keywords, test_pred_keywords)\n",
    "    print(f\"测试集表现：\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score:  {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5379fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估结果（文档级平均）：\n",
      "Precision: 0.0070\n",
      "Recall:    0.0267\n",
      "F1-score:  0.0110\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# ------------------\n",
    "# 改进版评估函数\n",
    "# ------------------\n",
    "def evaluate_science_v2(true_list, pred_list):\n",
    "    \"\"\"\n",
    "    文档级别的评估（每个文档独立计算指标）\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for true_kws, pred_kws in zip(true_list, pred_list):\n",
    "        # 处理空预测情况\n",
    "        if len(pred_kws) == 0:\n",
    "            prec = 0.0 if len(true_kws) > 0 else 1.0\n",
    "            rec = 0.0\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "            continue\n",
    "            \n",
    "        # 计算匹配数\n",
    "        matched = set()\n",
    "        for pred in pred_kws:\n",
    "            for true in true_kws:\n",
    "                if pred.lower() == true.lower():\n",
    "                    matched.add(pred)\n",
    "        \n",
    "        tp = len(matched)\n",
    "        precision = tp / len(pred_kws)\n",
    "        recall = tp / len(true_kws) if len(true_kws) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'f1': np.mean(f1s)\n",
    "    }\n",
    "\n",
    "# ------------------\n",
    "# 关键代码调整点（主流程）\n",
    "# ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # [保持数据加载部分不变]\n",
    "    \n",
    "    # 预处理时增加空文档过滤\n",
    "    def safe_preprocess(doc):\n",
    "        processed = science_preprocess(doc)\n",
    "        return processed if len(processed) > 10 else \"[EMPTY]\"  # 过滤长度<10的无效文档\n",
    "    \n",
    "    # 预处理训练集\n",
    "    preprocessed_train = [safe_preprocess(doc) for doc in train_docs]\n",
    "    \n",
    "    # 移除训练空文档\n",
    "    valid_train_idx = [i for i, txt in enumerate(preprocessed_train) if txt != \"[EMPTY]\"]\n",
    "    preprocessed_train = [preprocessed_train[i] for i in valid_train_idx]\n",
    "    train_keywords = [train_keywords[i] for i in valid_train_idx]\n",
    "    \n",
    "    # 训练TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_df=0.85,\n",
    "        min_df=2,\n",
    "        token_pattern=r'(?u)\\b[A-Za-z0-9_][A-Za-z0-9_]+\\b'\n",
    "    ).fit(preprocessed_train)\n",
    "    \n",
    "    # 预处理测试集\n",
    "    preprocessed_test = [safe_preprocess(doc) for doc in test_docs]\n",
    "    \n",
    "    # 生成预测关键词\n",
    "    tfidf_test = vectorizer.transform(preprocessed_test)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    test_pred_keywords = []\n",
    "    for i in range(tfidf_test.shape[0]):\n",
    "        row = tfidf_test[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-10:][::-1]  # 取top10\n",
    "        keywords = [feature_names[idx] for idx in top_indices if row[idx] > 0]  # 过滤零权重词\n",
    "        test_pred_keywords.append(keywords)\n",
    "    \n",
    "    # 验证维度一致性\n",
    "    assert len(test_keywords) == len(test_pred_keywords), \\\n",
    "        f\"维度不匹配：真实标签{len(test_keywords)} vs 预测{len(test_pred_keywords)}\"\n",
    "    \n",
    "    # 使用改进评估函数\n",
    "    metrics = evaluate_science_v2(test_keywords, test_pred_keywords)\n",
    "    print(\"\\n评估结果（文档级平均）：\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score:  {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f25c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set...\n",
      "Found 350 text files\n",
      "Successfully loaded 350 valid samples\n",
      "\n",
      "Loading test set...\n",
      "Found 100 text files\n",
      "Successfully loaded 100 valid samples\n",
      "\n",
      "Evaluation results (document-level average):\n",
      "Precision: 0.0010\n",
      "Recall:    0.0008\n",
      "F1-score:  0.0009\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Path configurations (modify according to your directory structure)\n",
    "TRAIN_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "TEST_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "# Science-specific stopwords\n",
    "SCIENCE_STOPWORDS = set([\n",
    "    'figure', 'equation', 'table', 'method', 'result', 'study',\n",
    "    'data', 'research', 'model', 'analysis', 'experiment'\n",
    "])\n",
    "\n",
    "def load_scienceie_data(data_dir):\n",
    "    documents = []\n",
    "    true_keywords = []\n",
    "    \n",
    "    txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "    print(f\"Found {len(txt_files)} text files\")\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        ann_file = txt_file.replace('.txt', '.ann')\n",
    "        ann_path = os.path.join(data_dir, ann_file)\n",
    "        \n",
    "        if not os.path.exists(ann_path):\n",
    "            print(f\"Skipping unannotated file: {txt_file}\")\n",
    "            continue\n",
    "        \n",
    "        txt_path = os.path.join(data_dir, txt_file)\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        keywords = parse_annotation(ann_path)\n",
    "        \n",
    "        documents.append(text)\n",
    "        true_keywords.append(keywords)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(documents)} valid samples\")\n",
    "    return documents, true_keywords\n",
    "\n",
    "def parse_annotation(ann_path):\n",
    "    keywords = []\n",
    "    try:\n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        content = parts[1]\n",
    "                        keyword = content.split(' ')[-1]\n",
    "                        keywords.append(keyword.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse annotation file {ann_path}: {str(e)}\")\n",
    "    return list(set(keywords))\n",
    "\n",
    "def science_preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english')) | SCIENCE_STOPWORDS\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    tokens = [\n",
    "        token for token in tokens \n",
    "        if not all(c in string.punctuation for c in token)\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def evaluate_science_v2(true_list, pred_list):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for true_kws, pred_kws in zip(true_list, pred_list):\n",
    "        if len(pred_kws) == 0:\n",
    "            prec = 0.0 if len(true_kws) > 0 else 1.0\n",
    "            rec = 0.0\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "            continue\n",
    "        \n",
    "        matched = set()\n",
    "        for pred in pred_kws:\n",
    "            for true in true_kws:\n",
    "                if pred.lower() == true.lower():\n",
    "                    matched.add(pred)\n",
    "        \n",
    "        tp = len(matched)\n",
    "        precision = tp / len(pred_kws)\n",
    "        recall = tp / len(true_kws) if len(true_kws) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'f1': np.mean(f1s)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading training set...\")\n",
    "    train_docs, train_keywords = load_scienceie_data(TRAIN_DIR)\n",
    "    \n",
    "    print(\"\\nLoading test set...\")\n",
    "    test_docs, test_keywords = load_scienceie_data(TEST_DIR)\n",
    "    \n",
    "    def safe_preprocess(doc):\n",
    "        processed = science_preprocess(doc)\n",
    "        return processed if len(processed) > 10 else \"[EMPTY]\"\n",
    "    \n",
    "    preprocessed_train = [safe_preprocess(doc) for doc in train_docs]\n",
    "        # Remove empty documents from the training set\n",
    "    valid_train_idx = [i for i, txt in enumerate(preprocessed_train) if txt != \"[EMPTY]\"]\n",
    "    preprocessed_train = [preprocessed_train[i] for i in valid_train_idx]\n",
    "    train_keywords = [train_keywords[i] for i in valid_train_idx]\n",
    "    \n",
    "    # Train TF-IDF on preprocessed training data\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_df=0.85,\n",
    "        min_df=2,\n",
    "        token_pattern=r'(?u)\\b[A-Za-z0-9_][A-Za-z0-9_]+\\b'\n",
    "    ).fit(preprocessed_train)\n",
    "    \n",
    "    # Preprocess test set\n",
    "    preprocessed_test = [safe_preprocess(doc) for doc in test_docs]\n",
    "    \n",
    "    # Generate predicted keywords for the test set\n",
    "    tfidf_test = vectorizer.transform(preprocessed_test)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    test_pred_keywords = []\n",
    "    for i in range(tfidf_test.shape[0]):\n",
    "        row = tfidf_test[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-10:][::-1]  # Keep top 10\n",
    "        keywords = [feature_names[idx] for idx in top_indices if row[idx] > 0]  # Filter zero weight words\n",
    "        test_pred_keywords.append(keywords)\n",
    "    \n",
    "    # Validate dimensional consistency\n",
    "    assert len(test_keywords) == len(test_pred_keywords), \\\n",
    "        f\"Dimensional mismatch: true labels {len(test_keywords)} vs predictions {len(test_pred_keywords)}\"\n",
    "    \n",
    "    # Use the improved evaluation function\n",
    "    metrics = evaluate_science_v2(test_keywords, test_pred_keywords)\n",
    "    print(\"\\nEvaluation results (document-level average):\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score:  {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c70cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average keywords per document: 17\n",
      "\n",
      "Optimized Results:\n",
      "Precision: 0.0759\n",
      "Recall:    0.0779\n",
      "F1-score:  0.0736\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 修改为正确的路径\n",
    "TRAIN_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "TEST_DIR = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "# 移除可能误删关键信息的科学停用词\n",
    "SCIENCE_STOPWORDS = set([\n",
    "    'figure', 'equation', 'table'  # 仅保留明显非关键词的停用词\n",
    "])\n",
    "\n",
    "def load_scienceie_data(data_dir):\n",
    "    documents = []\n",
    "    true_keywords = []\n",
    "    \n",
    "    txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        ann_file = txt_file.replace('.txt', '.ann')\n",
    "        ann_path = os.path.join(data_dir, ann_file)\n",
    "        \n",
    "        if not os.path.exists(ann_path):\n",
    "            continue\n",
    "        \n",
    "        txt_path = os.path.join(data_dir, txt_file)\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        keywords = parse_annotation(ann_path)\n",
    "        \n",
    "        documents.append(text)\n",
    "        true_keywords.append(keywords)\n",
    "    \n",
    "    return documents, true_keywords\n",
    "\n",
    "def parse_annotation(ann_path):\n",
    "    keywords = []\n",
    "    try:\n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 3:\n",
    "                        # 修正关键bug：正确提取关键词位置\n",
    "                        keyword = parts[2].strip()\n",
    "                        keywords.append(keyword)\n",
    "    except Exception as e:\n",
    "        print(f\"Annotation error: {str(e)}\")\n",
    "    return list(set(keywords))\n",
    "\n",
    "def science_preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english')) | SCIENCE_STOPWORDS\n",
    "    tokens = [token.lower() for token in tokens]  # 统一小写处理\n",
    "    \n",
    "    # 更宽松的过滤策略\n",
    "    tokens = [\n",
    "        token for token in tokens \n",
    "        if token not in stop_words and\n",
    "        token not in string.punctuation and\n",
    "        len(token) > 2  # 过滤过短词\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def evaluate_science_v2(true_list, pred_list):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for true_kws, pred_kws in zip(true_list, pred_list):\n",
    "        true_set = set(kw.lower() for kw in true_kws)\n",
    "        pred_set = set(kw.lower() for kw in pred_kws)\n",
    "        \n",
    "        tp = len(true_set & pred_set)\n",
    "        precision = tp / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        recall = tp / len(true_set) if len(true_set) > 0 else 0\n",
    "        f1 = 2*(precision*recall)/(precision+recall) if (precision+recall)>0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'f1': np.mean(f1s)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    train_docs, train_keywords = load_scienceie_data(TRAIN_DIR)\n",
    "    test_docs, test_keywords = load_scienceie_data(TEST_DIR)\n",
    "\n",
    "    # 计算平均关键词数量\n",
    "    avg_keywords = int(round(np.mean([len(kws) for kws in train_keywords])))\n",
    "    print(f\"Average keywords per document: {avg_keywords}\")\n",
    "\n",
    "    # 优化后的TF-IDF参数\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_df=0.75,\n",
    "        min_df=1,\n",
    "        token_pattern=r'(?u)\\b[\\w-]+\\b',  # 支持连字符\n",
    "        stop_words=list(SCIENCE_STOPWORDS)  # 仅使用科学停用词\n",
    "    )\n",
    "    # 预处理并训练\n",
    "    preprocessed_train = [science_preprocess(doc) for doc in train_docs]\n",
    "    vectorizer.fit(preprocessed_train)\n",
    "\n",
    "    # 预处理测试集\n",
    "    preprocessed_test = [science_preprocess(doc) for doc in test_docs]\n",
    "    tfidf_test = vectorizer.transform(preprocessed_test)\n",
    "    \n",
    "    # 动态提取关键词数量\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    test_pred_keywords = []\n",
    "    for i in range(tfidf_test.shape[0]):\n",
    "        row = tfidf_test[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-avg_keywords:][::-1]\n",
    "        keywords = [feature_names[idx] for idx in top_indices if row[idx] > 0]\n",
    "        test_pred_keywords.append(keywords[:avg_keywords])  # 限制数量\n",
    "\n",
    "    # 评估\n",
    "    metrics = evaluate_science_v2(test_keywords, test_pred_keywords)\n",
    "    print(\"\\nOptimized Results:\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score:  {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b9ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
