{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a4a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded 350 training samples | Avg keywords: 12.8\n",
      "\n",
      "Loading test data...\n",
      "Loaded 100 test samples | Avg keywords: 14.1\n",
      "\n",
      "Extracting keywords...\n",
      "Doc 1 Pred: ['surfac']...\n",
      "Doc 2 Pred: ['surfac']...\n",
      "Doc 3 Pred: ['alloy']...\n",
      "\n",
      "评估结果:\n",
      "Precision: 17.44%\n",
      "Recall:    1.07%\n",
      "F1 Score:  2.01%\n",
      "\n",
      "有效标注文档: 100/100\n",
      "预测关键词示例: ['surfac']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from summa import keywords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # 确保下载必要资源\n",
    "\n",
    "# ========== 配置参数 ==========\n",
    "DOMAIN_TERMS = {\n",
    "    'laser', 'quantum', 'modulus', 'polymer', 'nanoparticle', 'spectroscopy',\n",
    "    'synthesis', 'alloy', 'composite', 'asphalt', 'cullet', 'stiffness', \n",
    "    'bitumen', 'asphaltic', 'glass', 'particle'\n",
    "}\n",
    "\n",
    "EXTENDED_STOPWORDS = {\n",
    "    'however', 'furthermore', 'conclusion', 'experiment', 'methodology',\n",
    "    'result', 'study', 'research', 'data', 'analysis', 'table', 'figure', \n",
    "    'sample', 'test', 'show', 'based', 'using'\n",
    "}\n",
    "\n",
    "# ========== 核心函数 ==========\n",
    "def load_data(data_path):\n",
    "    \"\"\"增强型数据加载（带词干化和格式验证）\"\"\"\n",
    "    texts = []\n",
    "    keywords_list = []\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for filename in sorted(os.listdir(data_path)):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        # 读取文本文件\n",
    "        text_path = os.path.join(data_path, filename)\n",
    "        ann_path = os.path.join(data_path, filename.replace(\".txt\", \".ann\"))\n",
    "        \n",
    "        try:\n",
    "            with open(text_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                texts.append(f.read().strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_path}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # 处理标注文件\n",
    "        doc_keywords = []\n",
    "        if os.path.exists(ann_path):\n",
    "            with open(ann_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\"\\t\")\n",
    "                    if len(parts) >= 3:\n",
    "                        # 解析标注类型（兼容Term/Material/Process）\n",
    "                        annot_parts = parts[1].split()\n",
    "                        if len(annot_parts) == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        annot_type = annot_parts[0]\n",
    "                        if annot_type in [\"Term\", \"Material\", \"Process\", \"T\"]:\n",
    "                            keyword = parts[2].lower().strip()\n",
    "                            # 过滤无效关键词\n",
    "                            if 2 < len(keyword) < 50 and not keyword.isnumeric():\n",
    "                                doc_keywords.append(keyword)\n",
    "        \n",
    "        # 词干化处理并去重\n",
    "        stemmed_kws = [stemmer.stem(kw) for kw in doc_keywords]\n",
    "        keywords_list.append(list(set(stemmed_kws)))\n",
    "    \n",
    "    return texts, keywords_list\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"工业级文本清洗管道\"\"\"\n",
    "    # 移除参考文献引用\n",
    "    text = re.sub(r'\\[\\d+(-\\d+)?\\]', '', text)\n",
    "    # 处理特殊符号\n",
    "    text = re.sub(r'&[a-z]+;', lambda m: {'&alpha;':'alpha','&beta;':'beta'}.get(m.group(), ''), text)\n",
    "    # 统一连字符格式\n",
    "    text = re.sub(r'(\\w+)-(\\w+)', r'\\1_\\2', text)\n",
    "    # 移除表格内容\n",
    "    text = re.sub(r'\\|.*?\\|', ' ', text)\n",
    "    # 处理数字字母组合\n",
    "    text = re.sub(r'\\b(\\d+)([a-zA-Z]+)\\b', r'\\1 \\2', text)\n",
    "    # 基础清洗\n",
    "    text = re.sub(r'[^\\w\\s_]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).lower().strip()\n",
    "\n",
    "def extract_keywords(text, ratio=0.3):\n",
    "    \"\"\"领域自适应关键词提取\"\"\"\n",
    "    try:\n",
    "        # 预处理\n",
    "        cleaned = clean_text(text)\n",
    "        if len(cleaned.split()) < 10:  # 过滤过短文本\n",
    "            return []\n",
    "        \n",
    "        # 提取基础关键词\n",
    "        kw = keywords.keywords(\n",
    "            cleaned,\n",
    "            ratio=ratio,\n",
    "            words=True,\n",
    "            split=True,\n",
    "            scores=False,\n",
    "            language=\"english\"\n",
    "        )\n",
    "        \n",
    "        # 后处理\n",
    "        processed_kws = []\n",
    "        stemmer = PorterStemmer()\n",
    "        for k in kw:\n",
    "            k_clean = k.strip().lower()\n",
    "            # 过滤条件\n",
    "            if (len(k_clean) < 3 or \n",
    "                k_clean in EXTENDED_STOPWORDS or\n",
    "                not any(c.isalpha() for c in k_clean)):\n",
    "                continue\n",
    "                \n",
    "            # 领域词优先\n",
    "            stemmed = stemmer.stem(k_clean)\n",
    "            if stemmed in DOMAIN_TERMS or k_clean in DOMAIN_TERMS:\n",
    "                processed_kws.insert(0, k_clean)\n",
    "            else:\n",
    "                processed_kws.append(k_clean)\n",
    "        \n",
    "        # 合并同类词\n",
    "        stemmed_kws = [stemmer.stem(kw) for kw in processed_kws]\n",
    "        return list(set(stemmed_kws))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"提取错误: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"词干化评估\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tp = pred = true = 0\n",
    "    for true_kws, pred_kws in zip(y_true, y_pred):\n",
    "        # 词干化处理\n",
    "        true_set = set(stemmer.stem(kw) for kw in true_kws)\n",
    "        pred_set = set(stemmer.stem(kw) for kw in pred_kws)\n",
    "        \n",
    "        if not true_set:  # 跳过无标注文档\n",
    "            continue\n",
    "            \n",
    "        common = true_set & pred_set\n",
    "        tp += len(common)\n",
    "        pred += len(pred_set)\n",
    "        true += len(true_set)\n",
    "    \n",
    "    precision = tp / pred if pred > 0 else 0\n",
    "    recall = tp / true if true > 0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision+recall) >0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# ========== 主程序 ==========\n",
    "def main():\n",
    "    # 配置路径\n",
    "    train_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "    test_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "    # 加载数据\n",
    "    print(\"Loading training data...\")\n",
    "    X_train, y_train = load_data(train_path)\n",
    "    print(f\"Loaded {len(X_train)} training samples | Avg keywords: {np.mean([len(x) for x in y_train]):.1f}\")\n",
    "\n",
    "    print(\"\\nLoading test data...\")\n",
    "    X_test, y_test = load_data(test_path)\n",
    "    print(f\"Loaded {len(X_test)} test samples | Avg keywords: {np.mean([len(x) for x in y_test]):.1f}\")\n",
    "\n",
    "    # 提取关键词\n",
    "    print(\"\\nExtracting keywords...\")\n",
    "    y_pred = []\n",
    "    for i, text in enumerate(X_test):\n",
    "        kws = extract_keywords(text)\n",
    "        y_pred.append(kws)\n",
    "        if i < 3:  # 打印前3个样本结果\n",
    "            print(f\"Doc {i+1} Pred: {kws[:5]}...\")\n",
    "\n",
    "    # 评估\n",
    "    precision, recall, f1 = evaluate(y_test, y_pred)\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"\\n评估结果:\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall:    {recall:.2%}\")\n",
    "    print(f\"F1 Score:  {f1:.2%}\")\n",
    "\n",
    "    # 诊断信息\n",
    "    valid_docs = sum(1 for x in y_test if x)\n",
    "    print(f\"\\n有效标注文档: {valid_docs}/{len(y_test)}\")\n",
    "    print(f\"预测关键词示例: {y_pred[0][:5] if y_pred else 'None'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f5127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
