{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a4a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded 350 training samples | Avg keywords: 12.8\n",
      "\n",
      "Loading test data...\n",
      "Loaded 100 test samples | Avg keywords: 14.1\n",
      "\n",
      "Extracting keywords...\n",
      "Doc 1 Pred: ['surface']...\n",
      "Doc 2 Pred: ['surface']...\n",
      "Doc 3 Pred: ['alloys']...\n",
      "\n",
      "评估结果:\n",
      "Precision@5: 17.44%\n",
      "Recall@5:    1.07%\n",
      "F1@5:        2.01%\n",
      "TF-IDF Cosine Similarity: 0.1236\n",
      "\n",
      "有效标注文档: 100/100\n",
      "预测关键词示例: ['surface']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from summa import keywords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ========== 配置参数 ==========\n",
    "DOMAIN_TERMS = {\n",
    "    'laser', 'quantum', 'modulus', 'polymer', 'nanoparticle', 'spectroscopy',\n",
    "    'synthesis', 'alloy', 'composite', 'asphalt', 'cullet', 'stiffness', \n",
    "    'bitumen', 'asphaltic', 'glass', 'particle'\n",
    "}\n",
    "\n",
    "EXTENDED_STOPWORDS = {\n",
    "    'however', 'furthermore', 'conclusion', 'experiment', 'methodology',\n",
    "    'result', 'study', 'research', 'data', 'analysis', 'table', 'figure', \n",
    "    'sample', 'test', 'show', 'based', 'using'\n",
    "}\n",
    "\n",
    "# ========== 核心函数 ==========\n",
    "def load_data(data_path):\n",
    "    \"\"\"增强型数据加载（带词干化和格式验证）\"\"\"\n",
    "    texts = []\n",
    "    keywords_list = []\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for filename in sorted(os.listdir(data_path)):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        text_path = os.path.join(data_path, filename)\n",
    "        ann_path = os.path.join(data_path, filename.replace(\".txt\", \".ann\"))\n",
    "        \n",
    "        try:\n",
    "            with open(text_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                texts.append(f.read().strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_path}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        doc_keywords = []\n",
    "        if os.path.exists(ann_path):\n",
    "            with open(ann_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\"\\t\")\n",
    "                    if len(parts) >= 3:\n",
    "                        annot_parts = parts[1].split()\n",
    "                        if len(annot_parts) == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        annot_type = annot_parts[0]\n",
    "                        if annot_type in [\"Term\", \"Material\", \"Process\", \"T\"]:\n",
    "                            keyword = parts[2].lower().strip()\n",
    "                            if 2 < len(keyword) < 50 and not keyword.isnumeric():\n",
    "                                doc_keywords.append(keyword)\n",
    "        \n",
    "        # 保留原始关键词但基于词干去重\n",
    "        seen_stems = set()\n",
    "        unique_kws = []\n",
    "        for kw in doc_keywords:\n",
    "            stem = stemmer.stem(kw)\n",
    "            if stem not in seen_stems:\n",
    "                seen_stems.add(stem)\n",
    "                unique_kws.append(kw)\n",
    "        keywords_list.append(unique_kws)\n",
    "    \n",
    "    return texts, keywords_list\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"工业级文本清洗管道\"\"\"\n",
    "    text = re.sub(r'\\[\\d+(-\\d+)?\\]', '', text)\n",
    "    text = re.sub(r'&[a-z]+;', lambda m: {'&alpha;':'alpha','&beta;':'beta'}.get(m.group(), ''), text)\n",
    "    text = re.sub(r'(\\w+)-(\\w+)', r'\\1_\\2', text)\n",
    "    text = re.sub(r'\\|.*?\\|', ' ', text)\n",
    "    text = re.sub(r'\\b(\\d+)([a-zA-Z]+)\\b', r'\\1 \\2', text)\n",
    "    text = re.sub(r'[^\\w\\s_]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).lower().strip()\n",
    "\n",
    "def extract_keywords(text, ratio=0.3):\n",
    "    \"\"\"领域自适应关键词提取（返回前5个关键词）\"\"\"\n",
    "    try:\n",
    "        cleaned = clean_text(text)\n",
    "        if len(cleaned.split()) < 10:\n",
    "            return []\n",
    "        \n",
    "        kw = keywords.keywords(\n",
    "            cleaned,\n",
    "            ratio=ratio,\n",
    "            words=True,\n",
    "            split=True,\n",
    "            scores=False,\n",
    "            language=\"english\"\n",
    "        )\n",
    "        \n",
    "        processed_kws = []\n",
    "        stemmer = PorterStemmer()\n",
    "        for k in kw:\n",
    "            k_clean = k.strip().lower()\n",
    "            if (len(k_clean) < 3 or \n",
    "                k_clean in EXTENDED_STOPWORDS or\n",
    "                not any(c.isalpha() for c in k_clean)):\n",
    "                continue\n",
    "                \n",
    "            stemmed = stemmer.stem(k_clean)\n",
    "            if stemmed in DOMAIN_TERMS or k_clean in DOMAIN_TERMS:\n",
    "                processed_kws.insert(0, k_clean)\n",
    "            else:\n",
    "                processed_kws.append(k_clean)\n",
    "        \n",
    "        # 基于词干去重并取前5\n",
    "        seen_stems = set()\n",
    "        unique_kws = []\n",
    "        for kw in processed_kws:\n",
    "            stem = stemmer.stem(kw)\n",
    "            if stem not in seen_stems:\n",
    "                seen_stems.add(stem)\n",
    "                unique_kws.append(kw)\n",
    "        return unique_kws[:5]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"提取错误: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"评估函数（使用TF-IDF计算相似度）\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tp = pred = true = 0\n",
    "    cos_sim_total = 0.0\n",
    "    valid_sim_docs = 0\n",
    "    \n",
    "    # 预处理所有关键词\n",
    "    all_true = [' '.join([stemmer.stem(kw) for kw in doc]) for doc in y_true]\n",
    "    all_pred = [' '.join([stemmer.stem(kw) for kw in doc]) for doc in y_pred]\n",
    "    \n",
    "    # 计算TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        X_true = vectorizer.fit_transform(all_true)\n",
    "        X_pred = vectorizer.transform(all_pred)\n",
    "        cos_sims = cosine_similarity(X_true, X_pred).diagonal()\n",
    "        avg_cos = np.mean(cos_sims)\n",
    "    except:\n",
    "        avg_cos = 0.0\n",
    "    \n",
    "    # 原始评估逻辑\n",
    "    for true_kws, pred_kws in zip(y_true, y_pred):\n",
    "        true_set = set(stemmer.stem(kw) for kw in true_kws)\n",
    "        pred_set = set(stemmer.stem(kw) for kw in pred_kws)\n",
    "        \n",
    "        if not true_set:\n",
    "            continue\n",
    "            \n",
    "        common = true_set & pred_set\n",
    "        tp += len(common)\n",
    "        pred += len(pred_set)\n",
    "        true += len(true_set)\n",
    "    \n",
    "    precision = tp / pred if pred > 0 else 0\n",
    "    recall = tp / true if true > 0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision+recall) >0 else 0\n",
    "    \n",
    "    return precision, recall, f1, avg_cos\n",
    "\n",
    "# ========== 主程序 ==========\n",
    "def main():\n",
    "    train_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "    test_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "    print(\"Loading training data...\")\n",
    "    X_train, y_train = load_data(train_path)\n",
    "    print(f\"Loaded {len(X_train)} training samples | Avg keywords: {np.mean([len(x) for x in y_train]):.1f}\")\n",
    "\n",
    "    print(\"\\nLoading test data...\")\n",
    "    X_test, y_test = load_data(test_path)\n",
    "    print(f\"Loaded {len(X_test)} test samples | Avg keywords: {np.mean([len(x) for x in y_test]):.1f}\")\n",
    "\n",
    "    print(\"\\nExtracting keywords...\")\n",
    "    y_pred = []\n",
    "    for i, text in enumerate(X_test):\n",
    "        kws = extract_keywords(text)\n",
    "        y_pred.append(kws)\n",
    "        if i < 3:\n",
    "            print(f\"Doc {i+1} Pred: {kws[:5]}...\")\n",
    "\n",
    "    precision, recall, f1, cos_sim = evaluate(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n评估结果:\")\n",
    "    print(f\"Precision@5: {precision:.2%}\")\n",
    "    print(f\"Recall@5:    {recall:.2%}\")\n",
    "    print(f\"F1@5:        {f1:.2%}\")\n",
    "    print(f\"TF-IDF Cosine Similarity: {cos_sim:.4f}\")\n",
    "\n",
    "    valid_docs = sum(1 for x in y_test if x)\n",
    "    print(f\"\\n有效标注文档: {valid_docs}/{len(y_test)}\")\n",
    "    print(f\"预测关键词示例: {y_pred[0][:5] if y_pred else 'None'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4f6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sphy9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded 350 training samples | Avg keywords: 12.8\n",
      "\n",
      "Loading test data...\n",
      "Loaded 100 test samples | Avg keywords: 14.1\n",
      "\n",
      "Extracting keywords...\n",
      "Doc 1 Pred: ['followed', 'vessel', 'stl surface', 'removing', 'cerebral', 'd model', 'triangulation', 'resin vero following', 'printed', 'd_dsa', 'triangular meshes', 'computational volumetric mesh', 'digital subtraction angiographic', 'prairie mn', 'pro', 'software amira version', 'file', 'ma usa']\n",
      "Doc 2 Pred: ['surface', 'fig', 'time hydrozincite', 'surfaces investigated dp', 'formation rates', 'main corrosion products', 'absorbance units', 'allows comparisons', 'initial spreading ability', 'nacl_containing droplets', 'diamond polished', 'overall', 'rate', 'compared', 'reduces', 'preformed']\n",
      "Doc 3 Pred: ['particle', 'alloys', 'corrosion', 'phases', 'relatively', 'thermomechanical', 'resulting', 'applications', 'potentials', 'particularly', 'requires surface', 'preferential cathodic sites', 'damage', 'high', 'favourable mechanical', 'localized', 'initiation', 'weight ratio', 'commonly observed', 'principle types', 'aggressive environment', 'matrix']\n",
      "提取错误: list index out of range\n",
      "\n",
      "评估结果:\n",
      "Precision@25: 0.1062\n",
      "Recall@25:    0.1431\n",
      "F1@25:        0.1219\n",
      "TF-IDF Cosine Similarity: 0.4711\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from summa import keywords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ========== 配置参数 ==========\n",
    "DOMAIN_TERMS = {\n",
    "    'laser', 'quantum', 'modulus', 'polymer', 'nanoparticle', 'spectroscopy',\n",
    "    'synthesis', 'alloy', 'composite', 'asphalt', 'cullet', 'stiffness', \n",
    "    'bitumen', 'asphaltic', 'glass', 'particle'\n",
    "}\n",
    "\n",
    "EXTENDED_STOPWORDS = {\n",
    "    'however', 'furthermore', 'conclusion', 'experiment', 'methodology',\n",
    "    'result', 'study', 'research', 'data', 'analysis', 'table', 'figure', \n",
    "    'sample', 'test', 'show', 'based', 'using'\n",
    "}\n",
    "\n",
    "# ========== 核心函数 ==========\n",
    "def load_data(data_path):\n",
    "    texts = []\n",
    "    keywords_list = []\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for filename in sorted(os.listdir(data_path)):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        text_path = os.path.join(data_path, filename)\n",
    "        ann_path = os.path.join(data_path, filename.replace(\".txt\", \".ann\"))\n",
    "        \n",
    "        try:\n",
    "            with open(text_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                texts.append(f.read().strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_path}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        doc_keywords = []\n",
    "        if os.path.exists(ann_path):\n",
    "            with open(ann_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\"\\t\")\n",
    "                    if len(parts) >= 3:\n",
    "                        annot_parts = parts[1].split()\n",
    "                        if len(annot_parts) == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        annot_type = annot_parts[0]\n",
    "                        if annot_type in [\"Term\", \"Material\", \"Process\", \"T\"]:\n",
    "                            keyword = parts[2].lower().strip()\n",
    "                            if 2 < len(keyword) < 50 and not keyword.isnumeric():\n",
    "                                doc_keywords.append(keyword)\n",
    "        \n",
    "        seen_stems = set()\n",
    "        unique_kws = []\n",
    "        for kw in doc_keywords:\n",
    "            stem = stemmer.stem(kw)\n",
    "            if stem not in seen_stems:\n",
    "                seen_stems.add(stem)\n",
    "                unique_kws.append(kw)\n",
    "        keywords_list.append(unique_kws)\n",
    "    \n",
    "    return texts, keywords_list\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[\\d+(-\\d+)?\\]', '', text)\n",
    "    text = re.sub(r'&[a-z]+;', lambda m: {'&alpha;':'alpha','&beta;':'beta'}.get(m.group(), ''), text)\n",
    "    text = re.sub(r'(\\w+)-(\\w+)', r'\\1_\\2', text)\n",
    "    text = re.sub(r'\\|.*?\\|', ' ', text)\n",
    "    text = re.sub(r'\\b(\\d+)([a-zA-Z]+)\\b', r'\\1 \\2', text)\n",
    "    text = re.sub(r'[^\\w\\s_]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).lower().strip()\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"关键修改点1：强制提取至少25个候选词\"\"\"\n",
    "    try:\n",
    "        cleaned = clean_text(text)\n",
    "        if len(cleaned.split()) < 10:\n",
    "            return []\n",
    "        \n",
    "        # 修改点：使用words=30提供足够候选词\n",
    "        kw = keywords.keywords(\n",
    "            cleaned,\n",
    "            words=30,          # 提取30个候选词用于后续过滤\n",
    "            split=True,\n",
    "            scores=False,\n",
    "            language=\"english\"\n",
    "        )\n",
    "        \n",
    "        processed_kws = []\n",
    "        stemmer = PorterStemmer()\n",
    "        for k in kw:\n",
    "            k_clean = k.strip().lower()\n",
    "            # 放宽过滤条件（关键修改点2）\n",
    "            if len(k_clean) < 3 or k_clean in EXTENDED_STOPWORDS:\n",
    "                continue\n",
    "                \n",
    "            stemmed = stemmer.stem(k_clean)\n",
    "            # 优先保留领域术语\n",
    "            if stemmed in DOMAIN_TERMS or k_clean in DOMAIN_TERMS:\n",
    "                processed_kws.insert(0, k_clean)\n",
    "            else:\n",
    "                processed_kws.append(k_clean)\n",
    "        \n",
    "        # 关键修改点3：动态调整最终数量\n",
    "        seen_stems = set()\n",
    "        unique_kws = []\n",
    "        for kw in processed_kws:\n",
    "            stem = stemmer.stem(kw)\n",
    "            if stem not in seen_stems:\n",
    "                seen_stems.add(stem)\n",
    "                unique_kws.append(kw)\n",
    "        return unique_kws[:25]  # 确保最终输出25个\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"提取错误: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tp = pred = true = 0\n",
    "    cos_sim_total = 0.0\n",
    "    \n",
    "    all_true = [' '.join([stemmer.stem(kw) for kw in doc]) for doc in y_true]\n",
    "    all_pred = [' '.join([stemmer.stem(kw) for kw in doc]) for doc in y_pred]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        X_true = vectorizer.fit_transform(all_true)\n",
    "        X_pred = vectorizer.transform(all_pred)\n",
    "        cos_sims = cosine_similarity(X_true, X_pred).diagonal()\n",
    "        avg_cos = np.mean(cos_sims)\n",
    "    except:\n",
    "        avg_cos = 0.0\n",
    "    \n",
    "    # 关键修改点4：严格确保评估前25个\n",
    "    for true_kws, pred_kws in zip(y_true, y_pred):\n",
    "        pred_set = set(stemmer.stem(kw) for kw in pred_kws[:25])  # 显式截断\n",
    "        true_set = set(stemmer.stem(kw) for kw in true_kws)\n",
    "        \n",
    "        if not true_set:\n",
    "            continue\n",
    "            \n",
    "        common = true_set & pred_set\n",
    "        tp += len(common)\n",
    "        pred += len(pred_set)\n",
    "        true += len(true_set)\n",
    "    \n",
    "    precision = tp / pred if pred > 0 else 0\n",
    "    recall = tp / true if true > 0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision+recall) >0 else 0\n",
    "    \n",
    "    return precision, recall, f1, avg_cos\n",
    "\n",
    "# ========== 主程序 ==========\n",
    "def main():\n",
    "    train_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\scienceie2017_train\\train2\"\n",
    "    test_path = r\"E:\\HKULearning\\2025 spring\\STAT8021\\group work\\scienceie.github.io-master\\resources\\semeval_articles_test\"\n",
    "\n",
    "    print(\"Loading training data...\")\n",
    "    X_train, y_train = load_data(train_path)\n",
    "    print(f\"Loaded {len(X_train)} training samples | Avg keywords: {np.mean([len(x) for x in y_train]):.1f}\")\n",
    "\n",
    "    print(\"\\nLoading test data...\")\n",
    "    X_test, y_test = load_data(test_path)\n",
    "    print(f\"Loaded {len(X_test)} test samples | Avg keywords: {np.mean([len(x) for x in y_test]):.1f}\")\n",
    "\n",
    "    print(\"\\nExtracting keywords...\")\n",
    "    y_pred = []\n",
    "    for i, text in enumerate(X_test):\n",
    "        kws = extract_keywords(text)\n",
    "        y_pred.append(kws)\n",
    "        if i < 3:\n",
    "            print(f\"Doc {i+1} Pred: {kws[:25]}\")  # 显示完整25个\n",
    "\n",
    "    precision, recall, f1, cos_sim = evaluate(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n评估结果:\")\n",
    "    print(f\"Precision@25: {precision:.4f}\")\n",
    "    print(f\"Recall@25:    {recall:.4f}\")\n",
    "    print(f\"F1@25:        {f1:.4f}\")\n",
    "    print(f\"TF-IDF Cosine Similarity: {cos_sim:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee66b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
