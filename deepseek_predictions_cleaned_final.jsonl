{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Nuclear theory devoted major efforts since 4 decades to describe thermalization in nuclear reactions, predominantly using semi-classical methods  [13,14,10], in line with similar problems in quantum liquids  [15,16]. There were attempts to develop improved molecular dynamics methods combining quantum features with a semi classical treatment of dynamical correlations  [17,18]. Still, no clear-cut quantum approach is readily available yet, in spite of numerous formal attempts [19,20,10]. The field of clusters and nano structures is far younger but fast developing in relation to the ongoing developments of lasers and imaging techniques. Semiclassical approaches were also considered in the field to include some dynamical corrections  [21,22] and could qualitatively describe dynamical processes. But such approaches are bound to simple metals with sufficiently delocalized wave functions, and thus smooth potentials justifying semiclassical approximations. The case of organic systems, in particular the much celebrated C60   [4,23], cannot be treated this way. Semi classical, and even classical approaches, can be used at very high excitations such as delivered by very intense laser pulses  [2]. In such cases the system is blown up and details of its quantum mechanical features do not matter anymore. But for less violent scenarios, quantum shell effects cannot be ignored.", "ground_truth": ["nuclear theory", "thermalization", "thermalization", "semi-classical methods", "nuclear reactions", "quantum liquids", "improved molecular dynamics methods", "combining quantum features", "field of clusters and nano structures", "lasers", "imaging techniques", "qualitatively describe dynamical processes.", "simple metals", "simple metals with sufficiently delocalized wave functions", "organic systems", "c60", "very intense laser pulses", "quantum mechanical features", "quantum shell effects", "semi classical treatment of dynamical correlations", "quantum approach"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "nuclear theory", "thermalization", "nuclear reactions", "semi-classical methods", "quantum liquids", "molecular dynamics", "quantum features", "dynamical correlations", "quantum approach", "clusters", "nano structures", "lasers", "imaging techniques", "dynamical corrections", "simple metals", "delocalized wave functions", "smooth potentials", "semiclassical approximations", "organic systems", "c60", "classical approaches", "high excitations", "intense laser pulses", "quantum mechanical features", "quantum shell effects"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The next important step might be the derivation of the Dirac equation. The Creutz model  [32] suggests that we should consider incorporating into the logical inference treatment, the additional knowledge that one has objects hopping on a lattice instead of particles moving in a space-time continuum. Recall that up to Section  2.4, the description of the measurement scenario, robustness etc. is explicitly discrete. In Section  2.4, the continuum limit was taken only because our aim was to derive the Pauli equation, which is formulated in continuum space-time. Of course, the description of the motion of the particle in Section  2.6 is entirely within a continuum description but there is no fundamental obstacle to replace this treatment by a proper treatment of objects hopping on a lattice. Therefore it seems plausible that the logical inference approach can be extended to describe massless spin-1/2 particles moving in continuum space-time by considering the continuum limit of the corresponding lattice model. An in-depth, general treatment of this problem is beyond the scope of the present paper and we therefore leave this interesting problem for future research.", "ground_truth": ["dirac equation", "pauli equation", "continuum space-time", "massless spin-1/2 particles", "objects hopping on a lattice instead of particles", "the creutz model", "measurement scenario", "motion of the particle", "future research", "particles moving in a space-time continuum", "objects", "lattice", "lattice model"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "dirac equation", "creutz model", "logical inference treatment", "objects hopping on a lattice", "particles moving in space-time continuum", "measurement scenario", "robustness", "discrete description", "continuum limit", "pauli equation", "continuum space-time", "motion of the particle", "lattice model", "massless spin-1/2 particles", "future research"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A fluctuating vacuum is a general feature of quantum fields, of which the free Maxwell field considered in  [1–12] is but one example. Fermionic fields such as that describing the electron, also undergo vacuum fluctuations, consequently one expects to find Casimir effects associated with such fields whenever they are confined in some way. Such effects were first investigated in the context of nuclear physics, within the so-called “MIT bag model” of the nucleon  [13]. In the bag-model one envisages the nucleon as a collection of fermionic fields describing confined quarks. These quarks are subject to a boundary condition at the surface of the ‘bag’ that represents the nucleon’s surface. Just as in the electromagnetic case, the bag boundary condition modifies the vacuum fluctuations of the field, which results in the appearance of a Casimir force  [14–18]. This force, although very weak at a macroscopic scale, can be significant on the small length scales encountered in nuclear physics. It therefore has important consequences for the physics of the bag-model nucleon  [19].", "ground_truth": ["fluctuating vacuum", "quantum fields", "quantum fields", "free maxwell field", "fermionic fields", "undergo vacuum fluctuations", "casimir effects", "nuclear physics", "“mit bag model” of the nucleon", "a collection of fermionic fields describing confined quarks", "the bag boundary condition modifies the vacuum fluctuations of the field", "nuclear physics", "bag-model nucleon", "nucleon", "casimir force", "such fields"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fluctuating vacuum", "quantum fields", "free maxwell field", "fermionic fields", "electron", "vacuum fluctuations", "casimir effects", "confinement", "nuclear physics", "mit bag model", "nucleon", "confined quarks", "boundary condition", "bag boundary condition", "casimir force", "macroscopic scale", "small length scales", "nuclear physics", "bag-model nucleon"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We have presented spectrally resolved femtosecond three-pulse photon echo measurements on Zn(II)–OEP, Ni(II)–OEP and Co(II)–OEP. Increased degree of freedom in scans of time delays allows one to separate and extract specific type of spectroscopic information in complex molecules by studying spectral and temporal evolution of the photon echo signals. By varying the population times, population relaxation dynamics and inhomogeneous broadening is revealed in the photon echo spectra. Time-integrated photon echo signals show two different timescales. The electronic relaxation timescale is found to be sub 50fs whereas the timescale for intramolecular vibrational relaxation, occurring in Q00 band, was found to be over a picosecond for Co(II)–OEP and Ni(II)–OEP and within a picosecond for Zn(II)–OEP.", "ground_truth": ["spectrally resolved femtosecond three-pulse photon echo measurements", "spectrally resolved femtosecond three-pulse photon echo measurements", "zn(ii)–oep", "ni(ii)–oep", "co(ii)–oep", "separate and extract specific type of spectroscopic information", "complex molecules", "spectral and temporal evolution of the photon echo signals", "photon echo signals", "population times", "population relaxation dynamics", "varying the population times", "inhomogeneous broadening", "photon echo spectra", "time-integrated photon echo signals", "time-integrated photon echo signals", "timescales", "ni(ii)–oep", "co(ii)–oep", "within a picosecond for zn(ii)–oep", "electronic relaxation timescale", "sub 50fs", "timescale", "intramolecular vibrational relaxation", "increased degree of freedom", "q00 band"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "spectrally resolved", "femtosecond", "three-pulse photon echo", "measurements", "zn(ii)–oep", "ni(ii)–oep", "co(ii)–oep", "degree of freedom", "time delays", "spectroscopic information", "complex molecules", "spectral evolution", "temporal evolution", "photon echo signals", "population times", "population relaxation dynamics", "inhomogeneous broadening", "photon echo spectra", "time-integrated", "two different timescales", "electronic relaxation", "sub 50fs", "intramolecular vibrational relaxation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Water is the most important liquid, and the nature of its structure remains a topic of keen debate and an active area of research [1–9]. Much of this debate centers around whether water has a mainly tetrahedral structure with a continuum of distorted hydrogen bonds, or if it contains a mixture of two distinct components. One major development in recent years is the application of inner-shell spectroscopic techniques, such as X-ray absorption spectroscopy (XAS) and X-ray emission spectroscopy (XES) at the oxygen K-edge to investigate the structure of water [2,10–12]. These methods can provide a direct structural probe of water, providing insight into the nature of its hydrogen bonding network. Theoretical studies play a critical role in these studies, since the analysis of the experimental data requires calculations to provide a link between the observed spectral features and the underlying structure. However, the simulation of the XAS or XES for liquid water presents a difficult challenge because it requires accurate molecular dynamics simulations to provide a correct description of the molecular structure coupled with accurate calculations of the spectral properties, i.e. excitation energies and line intensities. Furthermore, adequate sampling over molecular configurations also needs to be accounted for.", "ground_truth": ["water", "water", "distorted hydrogen bonds", "inner-shell spectroscopic techniques", "x-ray absorption spectroscopy", "x-ray emission spectroscopy", "water", "xas", "xes", "hydrogen bonding network", "accurate molecular dynamics simulations", "molecular structure", "spectral properties", "excitation energies", "line intensities", "adequate sampling", "molecular configurations", "experimental data", "calculations", "observed spectral features", "underlying structure", "direct structural probe of water", "oxygen k-edge", "liquid", "xas", "xes", "water", "liquid water"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "water", "structure", "tetrahedral structure", "hydrogen bonds", "inner-shell spectroscopic techniques", "x-ray absorption spectroscopy (xas)", "x-ray emission spectroscopy (xes)", "oxygen k-edge", "hydrogen bonding network", "theoretical studies", "molecular dynamics simulations", "spectral properties", "excitation energies", "line intensities", "molecular configurations", "sampling"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Under these experimental conditions, the observed dynamics has to occur where the probe laser induces the reactions resulting in further ionization [30]. The two-step decay model [26] was applied to explain the above-mentioned fragmentation of DCPD to CPD, shown in Figure 8a. The fitting of the rise and decay components of the transients were done by Matlab® programming using the curve fitting Levenberg–Marquardt algorithm. The best fit decay constants for the biexponential decay components of C10H12+ ion signal is τ1=35fs and τ2=240fs, while that for C5H6+ ion signal is τ1=36fs and τ2=280fs, respectively. These decay constants conform to the previously reported time constants of norbornene and norbornadiene [22,23]. The transients of the reaction fragment C5H6+ are sufficiently different from that of the parent ion C10H12+ indicating that we are studying the distinct dynamics of the neutrals and not that of the parent ion fragmentation [24]. Applying laser control principles under such experimental circumstances also confirms that we are controlling the product yield of C5H6+, resulting from the photochemical reaction of DCPD.", "ground_truth": ["observed dynamics", "probe laser", "further ionization", "reactions", "two-step decay model", "fragmentation of dcpd to cpd", "rise and decay components", "matlab® programming", "curve fitting levenberg–marquardt algorithm", "best fit decay constants", "biexponential decay components", "c10h12+ ion signal", "τ1=35fs", "τ2=240fs", "c5h6+ ion signal", "τ1=36fs", "photochemical reaction of dcpd", "controlling the product yield of c5h6+", "applying laser control principles", "laser control principles", "parent ion fragmentation", "distinct dynamics of the neutrals", "parent ion fragmentation", "norbornene", "norbornadiene", "decay constants", "parent ion", "dcpd", "cpd", "τ2=280fs", "transients", "c10h12+", "dcpd", "c5h6+"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "experimental conditions", "observed dynamics", "probe laser", "reactions", "ionization", "two-step decay model", "fragmentation", "dcpd", "cpd", "figure 8a", "fitting", "rise and decay components", "transients", "matlab® programming", "curve fitting", "levenberg–marquardt algorithm", "best fit", "decay constants", "biexponential decay", "c10h12+ ion signal", "τ1=35fs", "τ2=240fs", "c5h6+ ion"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Experimental studies of the dynamics of individual carbon atoms in graphene have been empowered by the recent progress in aberration-corrected transmission electron microscopy (AC-TEM) capable of sub-Ångstrom resolution. The examples include AC-TEM observations of the formation and annealing of Stone–Wales defects [1], edge reconstruction [2,3] and formation of a large hole in graphene sheet from a single vacancy defect [3]. The AC-TEM has been also exploited in visualization in real time of the process of self-assembly of graphene nanoribbons from molecular precursors [4,5] and formation of nanometre size hollow protrusion on the nanotube sidewall [6]. Based on AC-TEM observations of transformation of small finite graphene flake into fullerene, a new ‘top-down’ mechanism for the formation of fullerene under the electron beam radiation has been proposed [7]. The critical step in the proposed ‘top-down’ mechanism of the fullerene formation is creation of vacancies in small graphene flake as a result of knock-on damage by electrons of the imaging electron beam (e-beam). The subsequent formation of pentagons at the vacancy sites near the edge reduces the number of dangling bonds and triggers the curving process of graphene flake into a closed fullerene structure [7]. Thus, dynamic behaviour of vacancies near graphene edge plays a crucial role in explaining mechanisms of the e-beam assisted self-assembly and structural transformations in graphene-like structures.", "ground_truth": ["carbon atoms", "graphene", "aberration-corrected transmission electron microscopy", "sub-ångstrom resolution", "ac-tem observations of the formation and annealing of stone–wales defects", "edge reconstruction", "formation of a large hole in graphene sheet from a single vacancy defect", "graphene sheet", "graphene", "graphene nanoribbons", "molecular precursors", "formation of nanometre size hollow protrusion on the nanotube sidewall", "nanotube sidewall", "ac-tem observations", "small finite graphene flake", "fullerene", "process of self-assembly of graphene nanoribbons", "proposed ‘top-down’ mechanism of the fullerene formation", "creation of vacancies in small graphene flake", "knock-on damage by electrons of the imaging electron beam (e-beam)", "subsequent formation of pentagons", "‘top-down’ mechanism for the formation of fullerene", "formation of fullerene", "e-beam assisted self-assembly", "structural transformations in graphene-like structures", "graphene-like structures", "ac-tem"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "experimental studies", "carbon atoms", "graphene", "aberration-corrected transmission electron microscopy (ac-tem)", "sub-ångstrom resolution", "stone–wales defects", "edge reconstruction", "vacancy defect", "graphene sheet", "self-assembly", "graphene nanoribbons", "molecular precursors", "nanometre size hollow protrusion", "nanotube sidewall", "fullerene", "top-down mechanism", "electron beam"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The PESs here employed have already been tested in order to verify their validity for dynamical purposes. Such tests include studies of the nitrogen exchange reaction [14] both adiabatic by running trajectories on the lowest surfaces and non-adiabatic by using the trajectory surface hoping (TSH) method [22,23] for transitions to the excited state of same symmetry. It was concluded that nonadiabatic transitions could not make a significant impact on the rate coefficients, and therefore all trajectories here reported are independently integrated for each symmetry on the corresponding lowest adiabatic PES. In fact, we have tested the impact of running the trajectories starting on the upper sheets, and found no vibrational transition to take place, only small amounts of rotational energy is exchanged in this case. Also neglected are electronic transitions to the quartet state which are believed to be far less probable than the simple vibrational energy transfer here studied due to their spin-forbidden character. It should also be noted that the use of quasiclassical trajectories is justified by the large masses of the atoms involved [24].", "ground_truth": ["verify their validity for dynamical purposes", "verify their validity for dynamical purposes", "studies of the nitrogen exchange reaction", "trajectory surface hoping", "rate coefficients", "vibrational transition", "rotational energy", "use of quasiclassical trajectories", "electronic transitions to the quartet state", "independently integrated for each symmetry", "tsh", "trajectory surface hoping (tsh) method", "atoms"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "pess (potential energy surfaces)", "dynamical purposes", "nitrogen exchange reaction", "adiabatic", "trajectories", "non-adiabatic", "trajectory surface hopping (tsh)", "excited state", "symmetry", "rate coefficients", "vibrational transition", "rotational energy", "electronic transitions", "quartet state", "spin-forbidden", "vibrational energy transfer", "quasiclassical trajectories", "large masses", "atoms"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The optical properties of charged excitations are important for understanding organic semiconductor photophysics. The injection of electric charge into organic materials polarizes the surroundings and changes the bond lengths around it, such an excitation is defined as a charged polaron. Absorption of light and fluorescence quenching by polarons are important issues in the operation of organic optoelectronic devices. It is particularly relevant to the development of electrically pumped lasers. With recent advances in materials properties and optical design the lasing threshold of organic structures under optical pumping is now low enough to enable pumping by inorganic laser diodes [1–3] and LEDs [4] which is promising for fabrication of very sensitive low-cost devices for biosensing and chemosensing [5,6]. However, light absorption by injected charges has been reported to be the major obstacle to electrically pumped lasing [7]. Injected charges can also quench luminescence as they accept energy from excitons by resonant dipole–dipole interactions and this is an important loss mechanism in organic LEDs as well as in lasers. Absorption cross-sections of polarons are not known to the desired accuracy because of the difficulty of quantifying the charge density injected into the film. Previous studies used controlled electrical injection of charges in unipolar devices through contacting electrodes and field-dependent charge mobility measurements to estimate the charge densities which were compared with the values obtained by capacitance–voltage analysis and the two results differed by a factor of three [8,9].", "ground_truth": ["inorganic laser diodes", "organic semiconductor photophysics", "electric charge", "organic materials", "charged polaron", "operation of organic optoelectronic devices", "development of electrically pumped lasers", "electrically pumped lasers", "lasing threshold of organic structures under optical pumping", "optical pumping", "organic structures", "unipolar devices", "contacting electrodes", "electrically pumped lasing", "very sensitive low-cost devices", "biosensing", "chemosensing", "accept energy from excitons", "resonant dipole–dipole interactions", "organic leds", "lasers", "light absorption", "charge density", "field-dependent charge mobility measurements", "capacitance–voltage analysis", "controlled electrical injection of charges"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "optical properties", "charged excitations", "organic semiconductor", "photophysics", "electric charge", "polarization", "bond lengths", "charged polaron", "light absorption", "fluorescence quenching", "polarons", "organic optoelectronic devices", "electrically pumped lasers", "materials properties", "optical design", "lasing threshold"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this Letter we revisit the Chesnavich model Hamiltonian [37] in the light of recent developments in TST. For barrierless systems such as ion–molecule reactions, the concepts of OTS and TTS can be clearly formulated in terms of well defined phase space geometrical objects. (For work on the phase space description of OTS, see Refs. [38–40].) The first goal of the present article is the identification of these notions with well defined phase space dividing surfaces attached to NHIMs. The second and main goal is an elucidation of the roaming phenomenon in the context of the Chesnavich model Hamiltonian. The associated potential function, possessing many features associated with a realistic molecular PES, leads to dynamics which clearly reveal the origins of the roaming effect. Based on our trajectory simulations, we show how the identification of the TTS and OTS DSs with periodic orbit dividing surfaces (PODS) provides the natural framework for analysis of the roaming mechanism.", "ground_truth": ["ion–molecule reactions", "chesnavich model hamiltonian", "identification", "tst", "trajectory simulations", "periodic orbit dividing surfaces", "dynamics which clearly reveal the origins of the roaming effect", "well defined phase space dividing surfaces attached to nhims", "associated potential function", "chesnavich model hamiltonian", "pods", "ots", "ots", "tts", "space geometrical objects", "nhims"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "chesnavich model hamiltonian", "tst (transition state theory)", "barrierless systems", "ion–molecule reactions", "ots (orbitally tight states)", "tts (tight transition states)", "phase space", "geometrical objects", "nhims (normally hyperbolic invariant manifolds)", "roaming phenomenon", "potential function", "molecular pes (potential energy surface)", "dynamics", "trajectory simulations", "periodic orbit dividing surfaces (pods)", "roaming mechanism"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Within the range of temperatures chosen, alanine dipeptide exhibits very simple behaviour. This result is due to the relatively small number of physically relevant minima (seven were characterised using this force field and solvent model) and the larger potential energy spacing between the global minimum and higher energy minima. Indeed, cross-overs in the approximate global free energy minimum for this system (where the free energy of the second-lowest potential energy minimum becomes lower than that of the global potential energy minimum) in the harmonic approximation would occur at 1170K. In general, the harmonic prediction for the crossover temperature between two minima is(4)kBTxo=V1−V2ln((o2ν¯2κ)/(o1ν¯1κ)),from Eq. (3), which clearly illustrates the balance between potential energy and well entropy.", "ground_truth": ["range of temperatures chosen", "alanine dipeptide", "physically relevant minima", "larger potential energy spacing", "cross-overs in the approximate global free energy", "balance between potential energy and well entropy", "force field and solvent model"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "alanine dipeptide", "temperatures", "simple behaviour", "physically relevant minima", "force field", "solvent model", "potential energy spacing", "global minimum", "higher energy minima", "global free energy minimum", "harmonic approximation", "crossover temperature", "potential energy", "well entropy", "equation (3)", "equation (4)", "kbtxo"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Both methods of structure solution reveal a bent conformation of the central terthiophene units of the DOTT molecule as is clearly visible in all three cases in Figure 5. However, there is a fundamental difference in the conformation of the octyl side chains. Whilst for the single crystal phase at T=100K linearly extended chains are observed (Figure 5B), a defined rotation of the octyl chains relative to the terthiophene unit is found for the three thin film phases (Figure 5A). The rotation angle of about ±70° results from a twist of the first CC single bond at the link between the terthiophene unit and the octyl chain (see arrows Figure 5A). Two features of this rotated conformation are interesting. First, a molecule with rotated side chains represents the equilibrium state of an isolated single DOTT molecule as obtained by combined MD and VASP calculations [33]. Second, the rotated conformation of the octyl chains allows a dense packing of the octyl side chains for both molecules. Interestingly, the single crystal structure at room temperature shows the twisted as well as the linear conformation of the octyl side chains within one molecule (Figure 5C).", "ground_truth": ["bent conformation", "defined rotation of the octyl chains", "twist of the first cc single bond", "dense packing", "structure solution", "linearly extended chains", "combined md and vasp calculations", "rotated side chains", "dott molecule", "octyl side chains", "octyl chain", "octyl chains", "thin film", "octyl side chains", "crystal structure", "dott molecule", "terthiophene unit", "octyl chains", "octyl side chains", "central terthiophene units", "terthiophene unit"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "structure solution", "bent conformation", "central terthiophene units", "dott molecule", "figure 5", "octyl side chains", "single crystal phase", "t=100k", "linearly extended chains", "thin film phases", "rotation angle", "±70°", "cc single bond", "terthiophene unit", "rotated conformation", "equilibrium state", "isolated single dott molecule", "md (molecular dynamics)", "vasp calculations", "dense packing", "single crystal structure", "room temperature", "twisted conformation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Previous studies have shown that there are two main mechanisms for the development of radiation-induced DSBs [16,17]. For γ-ray radiation, single step is the main process to cause DSBs (see Figure 3b), which is attributed to the generation of number of ROS upon the incident of individual photon of γ-ray. Whereas photo-radiation causes DSBs through two step mechanism (Figure 3a) by reflecting that each single photon causes mostly single ROS and thus induces only single strand break. Then, when a second single strand break occurs where near the existing single strand break, DBS is caused, i.e., the two step mechanism. Summarizing the results and discussion we may conclude as that: (1) The significant protective effect of AA against photo-induced damage may reflect the effective diminish of ROS by AA. (2) For the γ-ray induced DSB, the protective effect by AA is a little bit weaker than the case of photo irradiation. This may be due to the generation of numbers of ROS by single photon of γ-ray. Surviving oxygen species against the diminishment effect by AA may cause DSBs. (3) As for the DSBs by ultrasound, damage is caused by the shockwave through the generation of cavitations [18]. Thus, the chemical effect of AA to diminish ROS is considered to be negligibly small for the protection of DSBs.", "ground_truth": ["development of radiation-induced dsbs", "radiation-induced dsbs", "γ-ray radiation", "photon of γ-ray", "photo-radiation", "dsbs", "single step", "generation of number of ros", "ros", "dsbs", "two step mechanism", "photon", "ros", "single strand break", "strand", "single strand break", "strand", "single strand break", "dbs", "two step mechanism", "aa", "photo-induced damage", "photo", "ros", "aa", "γ-ray induced dsb", "aa", "photo irradiation", "photo", "ros", "single photon of γ-ray", "aa", "dsbs", "dsbs", "ultrasound", "shockwave", "cavitations", "aa", "ros", "diminish ros", "dsbs"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "previous studies", "mechanisms", "development", "radiation-induced dsbs (double-strand breaks)", "γ-ray radiation", "single step process", "ros (reactive oxygen species)", "photon", "photo-radiation", "two-step mechanism", "single strand break", "protective effect", "aa (ascorbic acid)", "photo-induced damage", "diminish ros"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In situ oxidation, experiments were carried out using 3mm diameter discs with one surface ground and polished to a 1μm diamond finish. The 3mm discs were then oxidised in a Philips XL-30 FEG ESEM with a hot stage attachment. The oxidising atmosphere used was laboratory air at a pressure of 266Pa. During the experiment, the sample was observed and imaged using a primary beam energy of 20kV and an Everhart–Thornley secondary electron detector. The sample was heated at a rate of 100°C/min to a temperature of 700°C and held at this temperature for 8min to stabilise the stage and the microscope. The sample was then heated to a final temperature of 900°C at the same heating rate. The total time of exposure of the sample was 120min before cooling to room temperature by turning off the heating coils. The samples were then examined in the LEO 1530VP FEGSEM with chemical information gathered using EDS. Cross-sections and Transmission Electron Microscope (TEM) samples were produced using a dual beam FEI Nova Nanolab 600 for Focused Ion Beam (FIB) milling perpendicular to the phase boundaries to determine their influence on the oxide development and imaged using a Jeol 2000FX W-filament TEM. EDS maps of the TEM samples were collected using the Nanolab 600 with a Scanning TEM (STEM) detector and an EDAX Genesis EDS system at an accelerating voltage of 30kV.", "ground_truth": ["in situ oxidation", "3mm diameter discs", "ground and polished", "oxidised", "heated", "stabilise the stage and the microscope", "heated to a final temperature of 900°c", "leo 1530vp fegsem", "transmission electron microscope", "scanning tem (stem) detector", "jeol 2000fx w-filament tem", "accelerating voltage of 30kv", "tem samples", "eds", "edax genesis eds system", "scanning tem", "stem", "tem", "hot stage attachment", "philips xl-30 feg esem", "heating coils", "eds", "dual beam fei nova nanolab 600", "focused ion beam", "fib", "oxide development", "oxide", "nanolab 600"], "prediction_list": ["here is the list of extracted keywords from the given scientific paragraph:", "in situ oxidation", "3mm diameter discs", "ground and polished", "1μm diamond finish", "philips xl-30 feg esem", "hot stage attachment", "oxidising atmosphere", "laboratory air", "pressure of 266pa", "primary beam energy", "20kv", "everhart–thornley secondary electron detector", "heating rate", "100°c/min"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The study outlines a trial of transient response analysis on full-scale motorway bridge structures to obtain information concerning the steel–concrete interface and is part of a larger study to assess the long-term sustained benefits offered by Impressed Current Cathodic Protection (ICCP) after the interruption of the protective current [1]. These structures had previously been protected for 5–16years by an ICCP system prior to the start of the study. The protective current was interrupted, in order to assess the long-term benefits provided by ICCP after it has been turned off. This paper develops and examines a simplified approach for the on-site use of transient response analysis and discusses the potential advantages of the technique as a tool for the assessment of the corrosion condition of steel in reinforced concrete structures.", "ground_truth": ["transient response analysis", "steel–concrete interface", "long-term sustained benefits", "interruption of the protective current", "protective current was interrupted", "assess the long-term benefits", "develops and examines a simplified approach", "assessment of the corrosion condition", "discusses the potential advantages", "motorway bridge structures", "impressed current cathodic protection", "iccp", "iccp system", "iccp", "transient response analysis", "steel", "reinforced concrete structures"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "transient response analysis", "full-scale motorway bridge structures", "steel–concrete interface", "long-term sustained benefits", "impressed current cathodic protection (iccp)", "protective current interruption", "corrosion condition assessment", "reinforced concrete structures", "on-site use", "simplified approach", "trial study", "these keywords capture the main concepts", "methods", "and objectives discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650°C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600°C one isothermal test, having an exposure time of 1000h, has been performed to date.", "ground_truth": ["oxidation test", "test matrix", "horizontal tube furnaces", "adjusted to accommodate buoyancy effects", "weight change during these tests was monitored continually", "alumina boats", "furnaces", "determine the oxidation kinetics", "table 1 shows the time intervals chosen for examination", "tga", "thermogravimetric balance"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "oxidation test", "test matrix", "100h tests", "650°c", "thermogravimetric balance (tga)", "weight change", "buoyancy effects", "horizontal tube furnaces", "alumina boats", "intermittent weighing", "oxidation kinetics", "high temperature exposure", "time intervals", "600°c", "isothermal test", "exposure time"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The adhesion/cohesion of the coating was evaluated by the scratch test method, using a Revetest system (CSM Instruments SA, Switzerland) equipped with a H-270 diamond indentor (200μm diameter). Six scratch indentations were carried out under previously optimized conditions (linear progressive load mode 1–4N, 4Nmin−1). In order to aid in determination of location of spallation/delamination, an extended scratch length of 6mm was employed. The scratch tracks were subsequently observed by SEM to determine the locations of the first coating failure and to understand the nature of the coating failure. During the scratch tests, the loading force and penetration depth were recorded and their respective values were correlated with the observed failure locations. The surface roughness of the coating was evaluated using a surface roughness tester (TR200, Timegroup Inc.) according to ISO standard [29]. Due to the presence of the open porosity in the outer layer of the coating, a measurement length for determination of the roughness (Ra) of 0.8mm was used. In total, eight measurements were carried out in different directions.", "ground_truth": ["adhesion/cohesion", "scratch test method", "adhesion/cohesion of the coating", "revetest system (csm instruments sa", "switzerland) equipped with a h-270 diamond indentor (200μm diameter)", "scratch indentations were carried out", "determination of location of spallation/delamination", "understand the nature of the coating failure", "loading force and penetration depth were recorded", "loading force and penetration depth were recorded and their respective values were correlated with the observed failure locations", "surface roughness tester", "surface roughness of the coating was evaluated", "eight measurements", "coating", "outer layer", "ra", "roughness"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "adhesion/cohesion", "coating", "scratch test method", "revetest system", "csm instruments sa", "h-270 diamond indentor", "200μm diameter", "linear progressive load mode", "1–4n", "4nmin−1", "spallation/delamination", "scratch length", "6mm", "sem (scanning electron microscopy)", "coating failure", "loading force", "penetration depth", "surface roughness", "surface roughness tester", "tr200", "timegroup inc"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "One surface was then polished and cleaned using a protocol designed to eliminate as much preparation-related contamination as possible. This is as follows: The lead surface was polished by hand using a damp abrasive disc (BuehlerMet II ®) to remove visible surface defects and to expose a fresh metal surface. Coupons were then polished using a sequence of diamond polishes with decreasing particle sizes (6μm, 3μm, 1μm Buehler MetaDi ® polycrystalline diamond suspension). A polishing cloth (Buehler MicroCloth ®) was saturated with the appropriate diamond suspension. A custom-made jig fitted to an automatic polisher (Buehler Minimet ® 1000) was used to hold the coupons in place during automated polishing. Coupons were polished for 15min using each diamond suspension followed by rinsing with 2-propanol (99.5%, reagent grade) and cleaning in 2-propanol for 5min in an ultrasonic bath. After polishing with the 1μm diamond suspension, the coupons were ultrasonically cleaned in 2-propanol for 3×5min, with fresh propanol for each cleaning cycle. Polished coupons were stored in 2-propanol until required.", "ground_truth": ["damp abrasive disc", "surface", "metal surface", "diamond polishes", "polycrystalline diamond", "polishing cloth", "diamond", "custom-made jig", "polisher", "coupons", "automated polishing", "coupons", "polished", "diamond suspension", "diamond", "2-propanol", "2-propanol", "ultrasonic bath", "polishing", "diamond suspension", "coupons", "2-propanol", "propanol", "cleaning", "coupons", "2-propanol"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "surface", "polished", "cleaned", "protocol", "contamination", "lead surface", "abrasive disc", "buehlermet ii", "metal surface", "coupons", "diamond polishes", "particle sizes", "6μm", "3μm", "1μm", "buehler metadi", "polycrystalline diamond suspension", "pol"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Poor oxidation behavior is the major barrier to the increased use of Ti-based alloys in high-temperature structural applications. The demand to increase the service temperature of these alloys beyond 550°C (the typical temperature limit) requires careful study to understand the role that composition has on the oxidation behavior of Ti-based alloys [1–3]. The attempt to overcome this limitation in Ti-based alloys has led to the production of alloys with substantially improved oxidation resistance such as β-21S and also development of coatings and pre-oxidation techniques [1,4–6]. While it is tempting to extrapolate the oxidation behavior (e.g. oxidation rate law, depth of oxygen ingress and scale thickness) observed for a limited number of compositions under a certain oxidation condition to a broader compositional range, there are numerous examples in the literature where deviations from the expected relations are observed [7,8].", "ground_truth": ["oxidation", "ti-based alloys", "alloys", "understand the role that composition has on the oxidation behavior of ti-based alloys", "oxidation", "ti-based alloys", "ti-based alloys", "alloys", "oxidation", "β-21s", "alloys with substantially improved oxidation resistance", "development of coatings and pre-oxidation techniques", "coatings", "pre-oxidation techniques", "oxidation", "oxygen", "oxidation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "oxidation behavior", "ti-based alloys", "high-temperature structural applications", "service temperature", "550°c", "composition", "oxidation resistance", "β-21s", "coatings", "pre-oxidation techniques", "oxidation rate law", "oxygen ingress", "scale thickness", "compositional range", "deviations", "literature examples", "these keywords capture"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.", "ground_truth": ["oxide", "surface", "surface area", "oxide", "sem images", "oxidation", "surface profile", "profile", "images", "surface area", "sample", "oxide", "oxide", "oxide", "surface", "oxide", "surface", "oxide", "surface area", "samples", "oxides", "oxidation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "oxide thickness", "weight gain", "surface area", "surface profile", "sem images", "oxidation", "applied roughness", "profile lengths", "modified oxide thicknesses", "percentage difference", "maximum decrease", "surface finishes", "variation", "calculated oxide thickness", "insignificant impact", "rougher samples", "early stages of oxidation", "these keywords capture the"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "There have been relatively few attempts to observe and in some cases extract the average current density from video images taken of growing 2D pits. Frankel presented a method to directly measure the average anodic current density from the growing pit boundary velocity in Al [33], an Al alloy [34] and Ni–Fe [35] thin films. Subsequently, Ryan et al. [27,36] determined the anodic current density in pits propagating as 2D disks in stainless steel thin films by measuring the pit edge movement velocity. Ernst and Newman [11,12,37] studied stability of pit growth in detail and measured the kinetics of 2D pit propagation in depth and width and compared the results with kinetics in 1D pencil electrodes. They developed a semi-quantitative model for pit propagation which explained the lacy pit cover formation during the pit growth, although they did not measure current density within the pit. More recently, Tang and Davenport [38] tracked the pit boundary movement and computed the instantaneous but average current density in Fe-Co thin films. However, there have been no previous attempts to quantify the local current density during inhomogeneous growth of pits, although such local variation in current density has long been recognised [7].", "ground_truth": ["video images", "2d pits", "anodic current", "pit", "al", "al alloy", "ni–fe [35] thin films", "pits", "2d disks", "stainless steel thin films", "pit edge", "pit growth", "pit", "2d pit propagation", "2d pit", "1d pencil electrodes", "semi-quantitative model", "pit propagation", "measured the kinetics of 2d pit propagation", "lacy pit cover formation", "lacy pit cover", "pit", "pit", "fe-co thin films", "pits", "current", "pit growth", "pit"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "average current density", "video images", "growing 2d pits", "frankel", "anodic current density", "pit boundary velocity", "al", "al alloy", "ni–fe thin films", "ryan et al.", "stainless steel thin films", "pit edge movement velocity", "ernst and newman", "pit growth stability", "2d pit propagation", "kin"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Anodizing processes are widely used for protecting aluminium alloys against corrosion [1]. The resultant films are composed of amorphous alumina and consist of a relatively thick, porous, outer region and a thinner, non-porous, inner region [2,3]. The porous region contains the major pores of the film, which extend from the film surface to the barrier layer. Near the film surface, shorter, incipient pores are also present, whose growth stopped in the early stages of anodizing. The diameter of the major pores and the thickness of the inner, barrier region are dependent on the potential applied during anodizing, with typical proportionalities of ∼1nmV−1 [3,4]. Studies of ionic migration in barrier-type and porous anodic alumina films have usually found a transport number of O2− ions of ∼0.6 [5,6]. During the formation of porous films, the outward migrating Al3+ ions, constituting the remainder of the ionic current, are ejected to the electrolyte at the pore bases [7]. The electronic current in the barrier region is generally considered to be negligible. The thickness of the barrier region, which is relatively constant during the growth of a film under either a constant potential or constant current density, is maintained by a balance between growth of the barrier layer by continued oxidation of the aluminium substrate and thinning of the barrier layer by either field-assisted dissolution of the alumina at the pore bases [8] or field-assisted flow of alumina from the barrier layer to the pore walls [9–13]. The pores may be widened toward the film surface by chemical dissolution to an extent dependent on the anodizing conditions.", "ground_truth": ["anodizing processes", "protecting aluminium alloys", "extend from the film surface", "growth stopped in the early stages of anodizing", "ionic migration", "outward migrating al3+ ions", "ejected", "growth of the barrier layer by continued oxidation", "thinning of the barrier layer by either field-assisted dissolution", "field-assisted flow of alumina", "chemical dissolution", "corrosion", "resultant films", "amorphous alumina", "major pores", "film", "film surface", "barrier layer", "film surface", "incipient pores", "major pores", "anodizing", "anodic alumina films", "o2− ions", "porous films", "ionic current", "electrolyte", "pore bases", "film", "aluminium substrate", "oxidation", "barrier layer", "barrier layer", "alumina", "pore bases", "alumina", "barrier layer", "pore walls", "pores", "film surface", "barrier region"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "anodizing processes", "aluminium alloys", "corrosion protection", "amorphous alumina", "porous region", "non-porous region", "barrier layer", "major pores", "incipient pores", "potential applied", "ionic migration", "transport number", "o2− ions", "al3+ ions", "electrolyte", "electronic current", "constant potential", "constant current density", "field-assisted dissolution", "field-assisted flow"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1–5]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.", "ground_truth": ["structural components", "failure of structural components", "igscc", "intergranular stress corrosion cracking", "stress corrosion cracking", "scc", "alloy 600", "74% ni", "16% cr and 8% fe", "stainless steels", "pressurised water reactors", "pwr", "stress corrosion cracking", "pressurized water reactors", "pwscc", "alloy 600", "steam generator tubes", "heater sleeves", "pressurized instrument penetrations", "control rod drive mechanisms", "scc", "alloy", "plant", "safer designs of future nuclear reactors", "nuclear reactors", "safety critical components"], "prediction_list": ["here are the extracted keywords from the paragraph:", "failure", "structural components", "nuclear power industry", "safety issue", "economic performance", "stress corrosion cracking (scc)", "intergranular stress corrosion cracking (igscc)", "nuclear industry", "alloy 600", "stainless steels", "pressurised water reactors (pwr)", "pressurized water reactors (pwr)", "pwscc (stress corrosion cracking in pressurized water reactors)", "steam generator tubes", "heater sleeves", "pressurized instrument penetrations", "control rod drive mechanisms", "mechanisms"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A key part of this problem is that an inspector only has access to data from a small inspected area. In this area, there is only one minimum thickness, which does not provide enough information to build a model of the smallest thicknesses. An inspector can generate a sample of the smallest thickness measurements by partitioning the inspection data into a number of equally sized blocks. In each block the minimum thickness is recorded. This set forms a sample of the smallest thickness measurements. From this sample, one can build a model which takes into account the variations of the smallest thickness measurements. Extreme value analysis (EVA) provides a limiting form for this model. It states that, if the underlying thickness measurements in each block are taken from independent and identical distributions, then the sample of minimum thickness measurements will follow a generalized extreme value distribution (GEVD).", "ground_truth": ["model of the smallest thicknesses", "build a model of the smallest thicknesses", "sample", "partitioning the inspection data", "inspection data", "equally sized blocks", "block", "sample", "model which takes into account the variations of the smallest thickness measurements", "extreme value analysis", "eva", "generalized extreme value distribution", "gevd"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "inspector", "data", "inspected area", "minimum thickness", "model", "smallest thicknesses", "sample", "partitioning", "inspection data", "equally sized blocks", "minimum thickness measurements", "variations", "extreme value analysis (eva)", "limiting form", "independent and identical distributions", "generalized extreme value distribution (gevd)"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations. Brass is characterised by an excess enthalpy and excess entropy of mixing, both of which are negative. The enthalpic data were measured by solution calorimetry e.g., [1–3] and based on chemical potential data calculated from phase equilibrium experiments e.g., [4–6], the excess entropy of mixing could be evaluated e.g., [7–9]. This excess entropy contains both, the vibrational and the configurational parts. The excess vibrational entropy, defined as the deviation from the entropy of a mechanical mixture of the end members A and B (i.e., Smmechmix=XASmA+XBSmB), can be determined by measuring the low temperature heat capacity (5 to 300K) versus composition behaviour. The determination of the excess configurational entropy, i.e., the excess entropy coming from non-random atomic distributions and defects, is much more difficult. Here, neutron scattering investigations together with computer simulations are normally used. If, however, reliable data of the total excess entropy (from enthalpic and chemical potential data) are available, the measurement of the excess vibrational entropy enables the determination of the excess configurational entropy simply by subtraction. Since configurational and vibrational entropies may have different temperature dependencies, it is worthwhile to separate the entropic effects. This is one aim of this study. Another aim is to deliver experimental data so that first principles studies can test their models on a disordered alloy, whose structural details (short-range order) depend on temperature.", "ground_truth": ["thermodynamics of copper-zinc alloys (brass)", "excess enthalpy and excess entropy of mixing", "enthalpic data", "solution calorimetry", "chemical potential data", "phase equilibrium experiments", "excess entropy of mixing", "vibrational and the configurational parts", "measuring the low temperature heat capacity (5 to 300k) versus composition behaviour", "excess configurational entropy", "neutron scattering investigations", "computer simulations", "reliable data of the total excess entropy", "separate the entropic effects", "deliver experimental data", "brass", "brass", "copper-zinc alloys", "smmechmix=xasma+xbsmb", "enthalpic and chemical potential data", "measurement of the excess vibrational entropy", "determination of the excess configurational entropy", "subtraction", "experimental data", "disordered alloy"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "thermodynamics", "copper-zinc alloys", "brass", "excess enthalpy", "excess entropy", "mixing", "solution calorimetry", "chemical potential", "phase equilibrium", "vibrational entropy", "configurational entropy", "mechanical mixture", "low-temperature heat capacity", "composition behaviour", "non-random atomic distributions", "defects", "neutron scattering", "computer simulations", "temperature dependencies", "first principles studies", "disordered alloy", "short-range order", "structural details", "experimental data"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The algorithm allows the modelling of plasmas of arbitrary degeneracy under the binary collision approximation. It uses a numerical interpolation of the inverse cumulative density function of the Fermi–Dirac distribution to initialise simulation particles, and collisions are subject to Pauli blocking. It is not appropriate in the limit of very strong coupling because the plasma theory which the Monte Carlo code is based on breaks down. The strong coupling limit corresponds to lnΛ≲3, with lnΛ the Coulomb logarithm [10]. The code is designed for lnΛ>3 in collisional plasmas with a non-negligible level of degeneracy. It is noted that Monte Carlo techniques with degenerate capabilities have been developed for studying transport in semi-conductors [11] but no such method exists for fully-ionised plasmas. Some of the techniques described are potentially applicable to other types of codes, for example, Particle-In-Cell (PIC) codes.", "ground_truth": ["modelling of plasmas of arbitrary degeneracy", "binary collision approximation", "numerical interpolation", "pauli blocking", "fermi–dirac distribution", "plasma theory", "monte carlo code", "strong coupling limit", "collisional plasmas", "monte carlo techniques", "transport in semi-conductors", "fully-ionised plasmas", "coulomb logarithm", "simulation particles", "collisions", "coupling", "plasmas"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "algorithm", "modelling", "plasmas", "arbitrary degeneracy", "binary collision approximation", "numerical interpolation", "inverse cumulative density function", "fermi–dirac distribution", "simulation particles", "collisions", "pauli blocking", "strong coupling", "plasma theory", "monte carlo code", "coulomb logarithm (lnλ)", "collisional plasmas", "degeneracy", "monte carlo techniques", "transport", "semi-conductors", "fully-ion"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Similar numerical oscillations to those described above also emerge in the ISPM when utilising classical IBM kernels due to their lack of regularity (with discontinuous second derivatives). Furthermore, it is important to remark that the immersed structure stresses are captured in the Lagrangian description and hence, in order to compute them accurately, it is important to ensure that these spurious oscillations are not introduced via the kernel interpolation functions. In this paper, the authors have specifically designed a new family of kernel functions which do not introduce these spurious oscillations. The kernel functions are obtained by taking into account discrete reproducibility conditions as originally introduced by Peskin [14] (in our case, tailor-made for Cartesian staggered grids) and regularity requirements to prevent the appearance of spurious oscillations when computing derivatives. A Maple computer program has been developed to obtain explicit expressions for the new kernels.", "ground_truth": ["numerical oscillations", "ispm", "classical ibm kernels", "lagrangian description", "oscillations", "kernel interpolation functions", "new family of kernel functions", "do not introduce these spurious oscillations", "oscillations", "kernel functions", "cartesian staggered grids", "oscillations", "maple computer program", "obtain explicit expressions for the new kernels", "new kernels"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "numerical oscillations", "ispm (immersed structure particle method)", "classical ibm kernels (immersed boundary method kernels)", "lack of regularity", "discontinuous second derivatives", "immersed structure stresses", "lagrangian description", "kernel interpolation functions", "spurious oscillations", "new family of kernel functions", "discrete reproducibility conditions", "peskin", "cartesian staggered grids", "regularity requirements", "derivatives", "maple computer program", "explicit expressions", "new kernels"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "After all micro elements reach a relaxed steady-state, measurements are obtained using a cumulative averaging technique to reduce noise. Each micro element is divided into spatially-oriented bins in the y-direction in order to resolve the velocity and shear-stress profiles. Velocity in each bin is measured using the Cumulative Averaging Method (CAM) [24], while the stress tensor field is measured using the Irving–Kirkwood relationship [25]. A least-squares polynomial fit to the data is performed, which helps reduce noise further. The fit produces a continuous function that avoids stability issues arising from supplying highly fluctuating data to the macro solver. A least-squares fit is applied to an Nth order polynomial for the velocity profile in the core region, and an Mth order polynomial for the velocity profile in the constrained region:(16)〈ui,core〉=∑k=1Nbk,iyi′(N−k),for 0⩽yi′⩽hcore, and(17)〈ui,cs〉=∑k=1Mck,iyi″(M−k),for 0⩽yi″⩽hcs, where bk,i and ck,i are the coefficients of the polynomials used in the core micro region and constrained region respectively. An estimate of the new slip velocity uB for input to the macro solution (6) is taken directly from the compressed wall micro-element solution (16), at yi′=0.", "ground_truth": ["cumulative averaging technique", "reduce noise", "micro element", "spatially-oriented bins", "resolve the velocity and shear-stress profiles", "bin", "cumulative averaging method", "cam", "stress tensor field", "irving–kirkwood relationship", "least-squares polynomial fit", "reduce noise", "continuous function", "avoids stability issues", "highly fluctuating data", "macro solver", "least-squares fit", "nth order polynomial", "mth order polynomial", "polynomials", "core micro region", "constrained region", "coefficients", "core region", "macro solution", "compressed wall micro-element solution"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "micro elements", "relaxed steady-state", "measurements", "cumulative averaging technique", "noise reduction", "spatially-oriented bins", "y-direction", "velocity profile", "shear-stress profile", "cumulative averaging method (cam)", "stress tensor field", "irving–kirkwood relationship", "least-squares polynomial fit", "continuous function", "stability issues", "macro solver", "nth order polynomial", "mth order polynomial", "core region", "constrained region", "coefficients", "slip velocity", "macro solution", "compressed wall micro-element solution"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "It is interesting to quantify the effects of the Schmidt number and the chemical reaction rate on the bulk-mean concentration of B in water. The data could present important information on evaluating the environmental impacts of the degradation product of B, as well as acidification of water by the chemical reaction. Here, the bulk-mean concentration of B is defined by(24)CB⁎¯=∫01〈CB⁎〉(z⁎)dz⁎ Fig. 15 depicts the effect of the Schmidt and the chemical reaction rate on the bulk-mean concentration CB⁎¯. It is worth to mention here that the bulk-mean concentration of B reaches approximately 0.6 as the chemical reaction rate and the Schmidt number increase to infinite, and the concentration is smaller than the equilibrium concentration of A at the interface. This figure indicates that progress of the chemical reaction is somewhat interfered by turbulent mixing in water, and the efficiency of the chemical reaction is up to approximately 60%. The efficiency of the chemical reaction in water will be a function of the Reynolds number of the water flow, and the efficiency could increase as the Reynolds number increases. We need an extensive investigation on the efficiency of the aquarium chemical reaction in the near future to extend the results of this study further to establish practical modelling for the gas exchange between air and water.", "ground_truth": ["bulk-mean concentration", "b", "water", "chemical reaction", "degradation", "b", "acidification", "water", "chemical reaction", "bulk-mean concentration", "b", "chemical reaction", "bulk-mean concentration", "cb⁎¯", "bulk-mean concentration", "b", "chemical reaction", "a", "chemical reaction", "turbulent mixing", "water", "chemical reaction", "chemical reaction", "water flow", "water", "water", "extensive investigation on the efficiency of the aquarium chemical reaction", "aquarium chemical reaction", "establish practical modelling for the gas exchange between air and water", "practical modelling", "gas exchange", "air", "water"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "schmidt number", "chemical reaction rate", "bulk-mean concentration", "b", "water", "environmental impacts", "degradation product", "acidification", "equilibrium concentration", "turbulent mixing", "efficiency", "reynolds number", "water flow", "aquarium chemical reaction", "gas exchange", "air", "practical modelling"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Numerical simulation of the gas flow through such non-trivial internal geometries is, however, extremely challenging. This is because conventional continuum fluid dynamics, which assumes that locally a gas is close to a state of thermodynamic equilibrium, becomes invalid or inaccurate as the smallest characteristic scale of the geometry (e.g. the channel height) approaches the mean distance between molecular collisions, λ [1]. An accurate and flexible modelling alternative for these cases is the direct simulation Monte Carlo method (DSMC) [2]. However, DSMC can be prohibitively expensive for internal-flow applications, which typically have a geometry of high-aspect ratio (i.e. are extremely long, relative to their cross-section). The high-aspect ratio creates a formidable multiscale problem: processes need to be resolved occurring over the smallest characteristic scale of the geometry (e.g. a channelʼs height), as well as over the largest characteristic scale of the geometry (e.g. the length of a long channel network), simultaneously.", "ground_truth": ["conventional continuum fluid dynamics", "numerical simulation", "numerical simulation of the gas flow", "molecular collisions", "direct simulation monte carlo method", "internal-flow applications", "thermodynamic equilibrium", "channel height", "high-aspect ratio", "dsmc", "dsmc", "formidable multiscale problem", "gas"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "numerical simulation", "gas flow", "non-trivial internal geometries", "challenging", "conventional continuum fluid dynamics", "thermodynamic equilibrium", "invalid/inaccurate", "smallest characteristic scale", "channel height", "mean distance between molecular collisions (λ)", "accurate modelling", "flexible modelling", "direct simulation monte carlo method (dsmc)", "prohibitively expensive", "internal-flow applications", "high-aspect ratio", "long relative to cross-section", "multiscale problem", "resolved processes", "largest characteristic scale", "channel network"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The test cases confirm that the high-order discretisation retains exponential convergence properties with increasing geometric and expansion polynomial order if both the solution and true surface are smooth. Errors are found to saturate when the geometric errors, due to the parametrisation of the surface elements, begin to dominate the temporal and spatial discretisation errors. For the smooth solutions considered as test cases, the results show that this dominance of geometric errors quickly limits the effectiveness of further increases in the number of degrees of freedom, either through mesh refinement or higher solution polynomial orders. Increasing the order of the geometry parametrisation reduces the geometric error. The analytic test cases presented here use a coarse curvilinear mesh; for applications, meshes are typically more refined in order to capture features in the solution and so will better capture the geometry and consequently reduce this lower bound on the solution error. If the solution is not smooth, we do not expect to see rapid convergence. In the case that the solution is smooth, but the true surface is not, then exponential convergence with P and Pg can only be achieved if, and only if, the discontinuities are aligned with element boundaries. However, if discontinuities lie within an element, convergence will be limited by the geometric approximation, since the true surface cannot be captured. In the cardiac problem, we consider both the true surface and solution to be smooth.", "ground_truth": ["parametrisation of the surface elements", "mesh refinement", "higher solution polynomial orders", "reduces the geometric error", "coarse curvilinear mesh", "p", "pg", "solution", "true surface", "geometric approximation", "cardiac problem", "high-order discretisation", "mesh", "meshes", "solution", "solution", "surface", "true surface", "solution"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "test cases", "high-order discretisation", "exponential convergence", "geometric order", "expansion polynomial order", "smooth solution", "true surface", "errors", "saturation", "geometric errors", "parametrisation", "surface elements", "temporal discretisation errors", "spatial discretisation errors", "degrees of freedom", "mesh refinement", "solution polynomial orders", "geometry parametrisation", "analytic test cases", "curvilinear mesh", "applications", "refined meshes", "solution error", "rapid convergence", "discontinuities", "element boundaries"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Designers of microfluidic devices are in need of computational tools that can be used to analyse problems that involve rarefied gas flows in complex micro geometries. Numerical simulation of the gas flow through such geometries is, however, extremely challenging. Conventional continuum fluid dynamics (CFD) becomes invalid or inaccurate as the characteristic scale of the geometry (e.g. the channel height, h) approaches the molecular mean free path, λ [1,2]. When λ/h≳0.1, the error in solutions obtained from CFD may be significant, and we must consider the fluid for what it is: a collection of interacting particles. However, the computational expense of simulating the flow of a rarefied gas in high-aspect-ratio micro geometries (i.e. ones that are long, relative to their cross section) using a particle method, such as the direct simulation Monte Carlo (DSMC) method [2], can be prohibitively high [3,4]. The computational intensity of the particle method is greater still when simulating low-speed microfluidic devices where there are only small deviations from equilibrium, characterised by extremely low Mach numbers and weak temperature gradients.", "ground_truth": ["analyse problems that involve rarefied gas flows in complex micro geometries", "numerical simulation", "conventional continuum fluid dynamics", "cfd", "particle method", "direct simulation monte carlo", "dsmc", "particle method", "microfluidic devices", "microfluidic devices", "rarefied gas flows", "gas flow", "cfd", "collection of interacting particles", "flow of a rarefied gas", "high-aspect-ratio micro geometries", "long", "relative to their cross section", "small deviations from equilibrium", "extremely low mach numbers and weak temperature gradients"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "microfluidic devices", "computational tools", "rarefied gas flows", "complex micro geometries", "numerical simulation", "gas flow", "continuum fluid dynamics (cfd)", "characteristic scale", "channel height (h)", "molecular mean free path (λ)", "λ/h ratio", "interacting particles", "computational expense", "high-aspect-ratio micro geometries", "particle method", "direct simulation monte carlo (dsmc)", "prohibitively high", "low-speed microfluidic devices", "small deviations from equilibrium", "low mach numbers"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this work, we have developed a simple numerical scheme based on the Galerkin finite element method for a multi-term time fractional diffusion equation which involves multiple Caputo fractional derivatives in time. A complete error analysis of the space semidiscrete Galerkin scheme is provided. The theory covers the practically very important case of nonsmooth initial data and right hand side. The analysis relies essentially on some new regularity results of the multi-term time fractional diffusion equation. Further, we have developed a fully discrete scheme based on a finite difference discretization of the Caputo fractional derivatives. The stability and error estimate of the fully discrete scheme were established, provided that the solution is smooth. The extensive numerical experiments in one- and two-dimension fully confirmed our convergence analysis: the empirical convergence rates agree well with the theoretical predictions for both smooth and nonsmooth data.", "ground_truth": ["numerical scheme", "galerkin finite element method", "multi-term time fractional diffusion equation", "multiple caputo fractional derivatives in time", "galerkin scheme", "fully discrete scheme", "finite difference discretization of the caputo fractional derivatives", "numerical experiments", "one- and two-dimension", "empirical convergence rates", "new regularity results"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "numerical scheme", "galerkin finite element method", "multi-term time fractional diffusion equation", "caputo fractional derivatives", "error analysis", "space semidiscrete galerkin scheme", "nonsmooth initial data", "right-hand side", "regularity results", "fully discrete scheme", "finite difference discretization", "stability", "error estimate", "smooth solution", "numerical experiments"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this work, light propagation in a scattering medium with piece-wise constant refractive index using the radiative transport equation was studied. Light propagation in each sub-domain with a constant refractive index was modeled using the RTE and the equations were coupled using boundary conditions describing Fresnel reflection and transmission phenomenas on the interfaces between the sub-domains. The resulting coupled system of RTEs was numerically solved using the FEM. The proposed model was tested using simulations and was compared with the solution of the Monte Carlo method. The results show that the coupled RTE model describes light propagation accurately in comparison with the Monte Carlo method. In addition, results show that neglecting internal refractive index changes can lead to erroneous boundary measurements of scattered light. This indicates that the quality of the DOT reconstructions could possible be increased by incorporating a model for internal refractive index changes in the image reconstruction procedure.", "ground_truth": ["light propagation in a scattering medium with piece-wise constant refractive index", "radiative transport equation", "rte", "coupled system of rtes", "fem", "simulations", "solution of the monte carlo method", "rte", "monte carlo method", "image reconstruction procedure", "dot reconstructions", "fresnel reflection and transmission phenomenas", "refractive index changes"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "light propagation", "scattering medium", "piece-wise constant refractive index", "radiative transport equation (rte)", "sub-domain", "boundary conditions", "fresnel reflection", "fresnel transmission", "coupled system", "numerical solution", "finite element method (fem)", "simulations", "monte carlo method", "accuracy", "internal refractive index changes", "erroneous boundary measurements", "scattered light", "dot reconstructions (diffuse optical tomography)", "image reconstruction procedure"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The validity of semi-classical boundary conditions for the WTE as introduced in [8] is a topic under vivid debate, especially after recent works which address the non-uniqueness and the symmetry properties of the Wigner function [27,28,46]. The numerical test cases presented therein are for symmetric potentials for which we cannot provide reliable, i.e. well-resolved, results due to the presence of singular terms in the steady state Wigner functions, see Section 4.3. Other recent studies demonstrate the convergence of the WTE calculations upon increasing the size of the simulation domain [44] as well as possible improvements by adapting the boundary distribution to the physical state of the active device region [47]. Despite their approximate nature we employ inflow/outflow boundary conditions here as well and demonstrate that accurate and physically valid results can be achieved for sufficiently large values of Lres. Due to the problematics with singular terms we present simulations only for non-zero bias voltages VDS≠0 V.", "ground_truth": ["validity of semi-classical boundary conditions", "non-uniqueness and the symmetry properties of the wigner function", "numerical test cases", "symmetric potentials", "steady state wigner functions", "other recent studies", "convergence of the wte calculations", "increasing the size of the simulation domain", "adapting the boundary distribution", "employ inflow/outflow boundary conditions", "present simulations only for non-zero bias voltages", "accurate and physically valid results"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "semi-classical boundary conditions", "wte (wigner transport equation)", "validity", "vivid debate", "non-uniqueness", "symmetry properties", "wigner function", "numerical test cases", "symmetric potentials", "reliable results", "well-resolved results", "singular terms", "steady state wigner functions", "recent studies", "convergence", "simulation domain", "boundary distribution", "active device region", "inflow/outflow boundary conditions", "accurate results", "physically valid results", "lres", "bias voltages", "vds"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A multi-physics description of a multiscale system is often referred to as a ‘hybrid’ model. In fluid dynamics, a typical hybrid combines a molecular treatment (a ‘micro’ model) with a continuum-fluid one (a ‘macro’ model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1–4]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is ‘scale-separated’, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (‘continuous’ since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.", "ground_truth": ["multiscale systems comprising an arbitrary number of coupled models", "multi-physics description of a multiscale system", "‘hybrid’ model", "hybrid", "molecular treatment", "‘micro’ model", "continuum-fluid one", "macro’ model", "micro and macro models", "time-stepping method for coupled systems", "continuous asynchronous", "ca", "micro and macro models", "scale-separated", "physical (as distinct from numerical) approximation", "fluid", "obtaining the accuracy of the former with the efficiency of the latter", "fluid dynamics", "coupled models", "coupled models"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "multi-physics", "multiscale system", "hybrid model", "fluid dynamics", "molecular treatment", "micro model", "continuum-fluid", "macro model", "accuracy", "efficiency", "characteristic timescales", "time-accurate simulations", "timestep", "stable", "macro-scale time periods", "scale-separated", "physical approximation", "coupled models", "asynchronously", "macro-scale accuracy", "e et al.", "time-stepping method", "lockerby et al.", "continuous asynchronous (ca"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The particular phase field model we employ is an extension of [6], and is based on the three dimensional thermal phase field model of [7] and two dimensional thermal-solutal phase field model of [8]. One feature of the physical problem is that it is purely dissipative, or entropy increasing, as all natural relaxational phenomena are. The resulting PDEs are of Allen–Cahn [9] and Carn–Hilliard type [10]. That is to say, the model involves time derivatives of the three fields coupled to forms involving variational derivatives of some functional – typically the free energy functional. As the dendrite grows the free energy reduces monotonically with time but never achieves equilibrium if the domain boundary is far from the dendrite. Although we have listed some of the difficult aspects of this model, the relaxational aspect is typically an asset and results in stable numerical schemes: there is no convection, for example (at least in the absence of flow in the melt).", "ground_truth": ["phase field model", "three dimensional thermal phase field model", "two dimensional thermal-solutal phase field model", "time derivatives of the three fields coupled to forms involving variational derivatives", "functional", "free energy functional", "numerical schemes", "pdes", "dendrite", "dendrite", "melt"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "phase field model", "three dimensional", "thermal phase field model", "two dimensional", "thermal-solutal phase field model", "purely dissipative", "entropy increasing", "relaxational phenomena", "pdes", "allen–cahn", "cahn–hilliard type", "time derivatives", "variational derivatives", "free energy functional", "dendrite", "equilibrium", "domain boundary", "relaxational aspect", "stable numerical schemes", "convection", "melt flow"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Inspired by energy-fueled phenomena such as cortical cytoskeleton flows [46,45,32] during biological morphogenesis, the theory of active polar viscous gels has been developed [37,33]. The theory models the continuum, macroscopic mechanics of a collection of uniaxial active agents, embedded in a viscous bulk medium, in which internal stresses are induced due to dissipation of energy [41,58]. The energy-consuming uniaxial polar agents constituting the gel are modeled as unit vectors. The average of unit vectors in a small local volume at each point defines the macroscopic directionality of the agents and is described by a polarization field. The polarization field is governed by an equation of motion accounting for energy consumption and for the strain rate in the fluid. The relationship between the strain rate and the stress in the fluid is provided by a constitutive equation that accounts for anisotropic, polar agents and consumption of energy. These equations, along with conservation of momentum, provide a continuum hydrodynamic description modeling active polar viscous gels as an energy consuming, anisotropic, non-Newtonian fluid [37,33,32,41]. The resulting partial differential equations governing the hydrodynamics of active polar viscous gels are, however, in general analytically intractable.", "ground_truth": ["theory of active polar viscous gels", "models the continuum", "macroscopic mechanics", "polar viscous gels", "uniaxial active agents", "viscous bulk medium", "uniaxial polar agents", "gel", "polar viscous gels", "energy consuming", "anisotropic", "non-newtonian fluid", "active polar viscous gels", "polarization field", "polarization field", "equation of motion", "constitutive equation", "continuum hydrodynamic description", "these equations", "along with conservation of momentum", "cortical cytoskeleton flows", "biological morphogenesis", "fluid"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "energy-fueled phenomena", "cortical cytoskeleton flows", "biological morphogenesis", "active polar viscous gels", "continuum macroscopic mechanics", "uniaxial active agents", "viscous bulk medium", "internal stresses", "dissipation of energy", "energy-consuming", "unit vectors", "polarization field", "equation of motion", "energy consumption", "strain rate", "constitutive equation", "anisotropic polar agents", "conservation of momentum", "continuum hydrodynamic description", "non-newtonian fluid", "partial differential equations", "analytically intractable"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The boundary element method (BEM) has clear advantages when applied to shape optimisation of high-voltage devices, see [4–8] for an introduction to BEM. First of all, BEM relies only on a surface discretisation so that there is no need to maintain an analysis-suitable volume discretisation during the shape optimisation process. Moreover, BEM is ideal for solving problems in unbounded domains that occur in electrostatic field analysis. In gradient-based shape optimisation the shape derivative of the cost functional with respect to geometry perturbations is needed [9–11]. To this purpose, we use the adjoint approach and solve the primary and the adjoint boundary value problems with BEM. The associated linear systems of equations are dense and an acceleration technique, such as the fast multipole method [12,13], is necessary for their efficient solution. For some recent applications of fast BEM in shape optimisation and Bernoulli-type free-boundary problems we refer to [14–16].", "ground_truth": ["boundary element method", "shape optimisation of high-voltage devices", "bem", "bem", "surface discretisation", "shape optimisation", "bem", "solving problems in unbounded domains", "electrostatic field analysis", "gradient-based shape optimisation", "shape derivative", "adjoint approach", "primary and the adjoint boundary value problems", "fast multipole method", "bernoulli-type free-boundary problems", "fast bem in shape optimisation", "bem", "bem", "bem"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "boundary element method (bem)", "shape optimisation", "high-voltage devices", "surface discretisation", "volume discretisation", "unbounded domains", "electrostatic field analysis", "gradient-based shape optimisation", "shape derivative", "cost functional", "geometry perturbations", "adjoint approach", "primary boundary value problem", "adjoint boundary value problem", "linear systems of equations"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The extrapolation of the upwind value required for TVD differencing is a particular hurdle for the application on unstructured meshes. As discussed in Section 3.2, two methods to extrapolate the value at the virtual upwind node, using data readily available on unstructured meshes, are considered. Given how the virtual upwind node is incorporated in the gradient ratio rf, the extrapolation method of Darwish and Moukalled [13] is referred to as implicit extrapolation and the method introduced by Ubbink and Issa [12] as explicit extrapolation. Both methods precisely reconstruct the upwind value for equidistant, rectilinear meshes but fail to do so on non-equidistant or non-rectilinear meshes, as discussed in Section 3.2. Using the explicit extrapolation method this issue can be rectified by imposing appropriate limits on the extrapolated upwind value.", "ground_truth": ["extrapolation of the upwind value", "darwish and moukalled", "implicit extrapolation", "method introduced by ubbink and issa", "explicit extrapolation", "virtual upwind node is incorporated in the gradient ratio rf", "explicit extrapolation", "unstructured meshes", "data readily available on unstructured meshes", "equidistant", "rectilinear meshes", "non-rectilinear meshes", "non-equidistant", "tvd differencing"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "extrapolation", "upwind value", "tvd differencing", "unstructured meshes", "virtual upwind node", "gradient ratio", "rf", "darwish and moukalled", "implicit extrapolation", "ubbink and issa", "explicit extrapolation", "equidistant", "rectilinear meshes", "non-equidistant", "non-rectilinear meshes", "limits", "extrapolated upwind value"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A popular choice is to couple a set of quadrature points with an equal number of nodal Lagrange polynomials defined at the same points, leading to a collocation method. There are many examples of this throughout the literature, both in terms of the more traditionally utilised continuous Galerkin (CG) and discontinuous Galerkin (DG) formulations, as well as newer extensions such as the flux reconstruction (FR) technique as presented by Huynh [23]. In collocation methods, while most linear operators can be exactly integrated in this setting depending on the choice of quadrature, integrals of nonlinear terms typically incur numerical error. However, the computational efficiencies that can be attained through the use of a collocation formulation, especially given the presence of a diagonal mass matrix, often outweigh the numerical error that is incurred.", "ground_truth": ["collocation formulation", "especially given the presence of a diagonal mass matrix", "couple a set of quadrature points with an equal number of nodal lagrange polynomials", "collocation method", "galerkin", "cg", "discontinuous galerkin", "dg", "flux reconstruction", "newer extensions", "fr", "collocation methods", "linear operators"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "quadrature points", "nodal lagrange polynomials", "collocation method", "continuous galerkin (cg)", "discontinuous galerkin (dg)", "flux reconstruction (fr)", "huynh", "linear operators", "nonlinear terms", "numerical error", "computational efficiencies", "diagonal mass matrix"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "An inherent problem of the phase-space discretisation is the spurious separation of energy into the discretised bins. This is called the “Garden Sprinkler Effect” and has been extensively studied in [48,49,20]. (In the Boltzmann transport community this is known as the ray effect.) To showcase this effect in the angular dimension, a large spatial domain (4000km×4000km) is simulated, with a monochromatic wave propagating over a long distance in deep water (d=10000m). For the spatial discretisation a structured triangle mesh is used, with an element edge length of 67km (Fig. 11(a)). The initial wave field, located 500km from the lower and left side has a Gaussian distribution in space, with a significant wave height of Hs=2.5m and a standard deviation of 150km (Fig. 11(b)). Its mean direction is 30° with an angular distribution of cos2⁡(θ) and a frequency of 0.1Hz. The simulation is time-dependent and runs for 5 days with a time-step of 600s.", "ground_truth": ["phase-space discretisation", "garden sprinkler effect", "ray effect", "spatial domain (4000km×4000km) is simulated", "spatial discretisation", "structured triangle mesh", "wave field", "simulation", "gaussian distribution in space", "spurious separation of energy into the discretised bins", "monochromatic wave", "deep water", "wave"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "phase-space discretisation", "spurious separation of energy", "discretised bins", "garden sprinkler effect", "boltzmann transport community", "ray effect", "angular dimension", "spatial domain", "4000km×4000km", "monochromatic wave", "long distance", "deep water", "d=10000m", "spatial discretisation", "structured triangle mesh", "element edge length", "67km", "initial wave field", "gaussian distribution", "significant wave height", "hs=2.5m", "standard deviation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Multi-phase flows are common, in fact quite general, in environmental and industrial processes. Broadly these may be modelled as continuous problems where phases are mixed (e.g. oil–water homogenisation [36], sediment transport [18]) or interface problems where phases are distinct and interact at the interface (e.g. gas-assisted injection moulding [21], liquid jet breakup [40]). In some cases flows start as interface problems but as mixing occurs at the interface they become effectively continuous, at least locally. Air entrainment, perhaps due to wave breaking, is an obvious example. We consider here two-phase interface problems where the interface remains distinct and the density difference is high, e.g. air and water, and where one phase may be considered incompressible. The interface is transient and may become highly distorted and interconnected. Such problems have been tackled with mesh-based methods using periodic (or adaptive) re-meshing or additional phase tracking functions [40]. However, these approaches can be time-consuming to implement and prone to errors in surface representation [50] or mass conservation [34].", "ground_truth": ["multi-phase flows", "liquid jet breakup", "mixing", "air entrainment", "two-phase interface problems", "distorted and interconnected", "gas-assisted injection moulding", "phases are mixed", "oil–water homogenisation", "sediment transport", "interface problems", "environmental and industrial processes", "wave breaking", "air", "water", "mesh-based methods", "re-meshing", "phase tracking functions"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "multi-phase flows", "environmental processes", "industrial processes", "continuous problems", "phases mixed", "oil-water homogenisation", "sediment transport", "interface problems", "phases distinct", "gas-assisted injection moulding", "liquid jet breakup", "mixing", "air entrainment", "wave breaking", "two-phase interface problems", "density difference", "air and water", "incompressible", "transient interface", "highly distorted", "interconnected", "mesh-based methods", "periodic re-meshing", "adaptive re-m"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This section is devoted to the discretization of the advection–diffusion equation and to the analysis of dispersion and diffusion eigencurves for different polynomial orders. The spectral/hp continuous Galerkin method considered closely resembles the formulation presented in [7]. Sec. 2.1 describes in detail the derivation of the semi-discrete advection–diffusion problem as applied to wave-like solutions, from which the relevant eigencurves can be obtained. The inviscid case (linear advection) is then addressed in Sec. 2.2, where the role of primary and secondary eigencurves is discussed from the perspective introduced in [9]. The viscous case is subsequently considered in Sec. 2.3, where eigencurves are shown to feature irregular oscillations for problems strongly dominated by either convection or diffusion.", "ground_truth": ["discretization of the advection–diffusion equation", "dispersion and diffusion eigencurves", "galerkin method", "wave-like solutions", "linear advection", "inviscid case", "viscous case", "derivation of the semi-discrete advection–diffusion problem"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "discretization", "advection–diffusion equation", "analysis", "dispersion", "diffusion", "eigencurves", "polynomial orders", "spectral/hp continuous galerkin method", "formulation", "semi-discrete advection–diffusion problem", "wave-like solutions", "inviscid case", "linear advection", "primary eigencurves", "secondary eigencurves", "viscous case", "irregular oscillations", "convection", "diffusion"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Discovering that both the vacancy and interstitial defect migration pathways are confined to Ga-free regions suggests changes in recombination rates of isolated vacancy-interstitial pairs in comparison to pure Pu. The degree to which the rates are effected depends on the distribution of residual defects post a cascade event, in addition to the concentration and ordering of the Ga atoms. If vacancies and interstitials become greatly separated after the collision cascade, then pathways to recombination are likely to become restricted and recovery times will be extended. This is viable for cascades that created a vacancy rich core surrounded by dispersed interstitials, as found for the low energy cascades in Pu and PuGa [11,12]. This may also be the case for channelling events, where energetic atoms travel deep into the lattice through channels of low atomic density.", "ground_truth": ["defect migration pathways", "low energy cascades in pu and puga", "channelling events", "energetic atoms travel deep into the lattice through channels of low atomic density", "cascade event", "ga atoms", "ga", "collision cascade"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "vacancy defect", "interstitial defect", "migration pathways", "ga-free regions", "recombination rates", "vacancy-interstitial pairs", "pure pu (plutonium)", "residual defects", "cascade event", "ga (gallium) concentration", "ga ordering", "collision cascade", "recombination pathways", "recovery times", "vacancy-rich core", "dis"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Our simulations confirm experimental observations that W net erosion represents only tiny fraction (in our simulation ∼1%) of the W gross erosion. The estimated upstream W fluxes, FWupstrem, are in good agreement with the experimentally observed values ⩽1019m-2s-1 [16]. Moreover, this value is not very sensitive to the divertor plasma temperature. For low temperatures the energy of D and C ions hitting to the divertor plates is too low to sputter sufficient amount of W. With increasing energy the W sputtering increases, but the potential drop in the divertor plasma increases too. As a result, most of the W atoms are ionized in the vicinity of the divertor and return back to the plates. There are two effects leading to the observed prompt redeposition of W ions: first is the “near-divertor” ionization of W due to low ionization potential −7.86eV (for comparison the ionization potentials for D and C are 13,6 and 10.6eV), second, W+n ions have large Larmor radius ∼2/nmm, so that they are redeposited within the distance of a Larmor radius. Important to note that a significant fraction of W ions escaping this prompt redeposition are returned back due to the friction with the main ions.", "ground_truth": ["simulations", "w net erosion", "w gross erosion", "estimated upstream w fluxes", "experimentally observed values", "hitting to the divertor plates", "increasing energy", "ionized", "return", "observed prompt redeposition", "near-divertor” ionization", "large larmor radius", "friction with the main ions", "d and c ions", "fwupstrem", "w", "w fluxes", "w", "divertor plasma", "w", "w atoms", "d", "w ions", "c", "w+n ions", "w sputtering", "ionization", "divertor plates", "w ions", "divertor", "plates", "divertor", "ionization"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "simulations", "experimental observations", "w net erosion", "w gross erosion", "upstream w fluxes", "fwupstream", "divertor plasma temperature", "d ions", "c ions", "sputtering", "potential drop", "w atoms", "ionization", "divertor plates", "prompt redeposition", "near-divertor ionization", "ionization potential", "larmor radius", "w+n ions", "friction", "main ions"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The displacement cascade is a rapid process (of order picoseconds). Further migration of vacancies and SIAs, mainly by diffusion, happens over a timescale of order nanoseconds [17]. This is still short compared to operating times, so is important to consider the equilibrium result of such processes: If the vacancies and SIAs were likely to find their Frenkel partner, recombine, and annihilate, then the metal should essentially return to its original structure; however, if defects instead formed large clusters of a single type this could result in formation of voids, dislocation loops or swelling, possibly weakening the material in the process. Defects can be trapped at grain boundaries or surface, so for an ODS particle to effect the diffusion, there concentration must be such that there are many such particles in each grain.", "ground_truth": ["displacement cascade", "structure", "defects", "voids", "dislocation loops", "swelling", "diffusion", "migration of vacancies and sias", "ods particle", "grain", "grain"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "displacement cascade", "rapid process", "picoseconds", "vacancies", "sias (self-interstitial atoms)", "diffusion", "nanoseconds", "operating times", "equilibrium result", "frenkel partner", "recombination", "annihilation", "metal", "original structure", "defects", "large clusters", "voids", "dislocation loops", "swelling", "weakening", "material", "trapped", "grain boundaries", "surface", "ods particle (oxide dispersion-strengthened particle)", "concentration", "grain"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The dashed curve represents the PuO2 molar fraction on the sample surface. It shows that, following the UO2–PuO2 phase boundaries, rather well established in this compositional range (see Section 4.3 below), the newly formed liquid surface is initially enriched in plutonium dioxide. Subsequently, due to fast diffusion in the liquid phase, the initial sample composition (x(PuO2)=0.25) tends to be rapidly restored. It is however clear, from the simulation, that the fast cooling occurring after the end of the laser pulse leads to onset of solidification before the initial composition is fully recovered in the liquid. A surface solid crust forms then upon freezing before the total liquid mass has crystallised (see insets in Fig. 4). The double inflection during cooling in this case corresponds to the solidification onset on the sample surface (first inflection) and to the disappearance of the last liquid inside the material (second inflection). The highest recalescence temperature represents the solidification point of a composition very close to the initial one (approximately ±0.01 on x(PuO2) in the current example), except for small segregation effects. These latter have been studied also experimentally in the present research, by post-melting material characterisation.", "ground_truth": ["uo2–puo2 phase boundaries", "simulation", "laser pulse", "post-melting material characterisation", "small segregation effects", "puo2", "liquid surface", "plutonium dioxide", "liquid", "puo2", "enriched in plutonium dioxide", "cooling", "solidification", "liquid", "surface solid crust", "freezing", "liquid", "cooling", "solidification", "sample surface", "liquid", "solidification"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "puo2 molar fraction", "sample surface", "uo2–puo2 phase boundaries", "compositional range", "liquid surface", "plutonium dioxide (puo2)", "fast diffusion", "liquid phase", "initial sample composition", "x(puo2)=0.25", "fast cooling", "laser pulse", "solidification", "surface solid crust", "freezing"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.", "ground_truth": ["vapour pressure of carbon", "vapour phase", "number of different gases", "data taken from janaf tables", "partial pressure", "silicon", "carbon", "vapour", "silicon", "solid phase", "carbon", "carbon", "carbon", "sic grains", "triso particle", "gaseous carbon", "graphite", "sic", "precipitate on the surface of the sic grains", "carbon", "gas", "sic"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "vapour phase", "gases", "silicon", "partial pressure", "carbon containing species", "temperature range", "solid phase", "excess carbon", "precipitate", "surface", "sic grains", "temperature", "2100k", "triso particle", "sic layer", "coatings", "dense carbon", "thermodynamic equilibrium", "gaseous carbon", "graphite", "janaf tables", "fig. 1", "vapour pressure", "dominant gas phases", "sic"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The second stress state is a tri-axial tensile stress designed to represent the zone ahead of an advancing crack tip. Micro-scale lateral cracks have been observed in the oxide layer, and appear to form very close to or at the metal–oxide interface (Fig. 1). Finite element analysis by Parise et al. indicated that these cracks form as a result of localised tensile stresses above peaks in the metal–oxide interface roughness [31]. These cracks are considered separate to any nano-scale cracks that might result from the tetragonal to monoclinic phase transformation. An assumption is made here that whether the micro-scale lateral cracks form via fracture of the oxide or by de-bonding at the interface a triaxial tensile stress state will still be present. In manufactured partially stabilised zirconia cracks would be expected to destabilise the tetragonal phase. This is simulated by applying tensile stress in direction 1, 2 and 3. As this the maximum stress at the crack tip is not known, the applied tensile stresses cover a range from 0.1GPa up to a maximum stress value of 2.2GPa as it is approximately equal to three times the fracture strength of bulk fracture strength for manufactured stabilized zirconia [34]. For the biaxial compressive and triaxial tensile stress states it is the trends in behaviour rather than the absolute values that are considered of greatest importance for this work.", "ground_truth": ["biaxial compressive and triaxial tensile stress states", "second stress state", "tri-axial tensile stress", "finite element analysis", "simulated by applying tensile stress in direction 1", "2 and 3", "advancing crack tip", "oxide layer", "metal–oxide interface", "cracks", "localised tensile stresses", "manufactured partially stabilised zirconia", "manufactured stabilized zirconia", "applied tensile stresses", "metal–oxide interface", "tetragonal to monoclinic phase transformation", "oxide", "de-bonding", "triaxial tensile stress", "tetragonal phase", "fracture", "applying tensile stress", "maximum stress"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "second stress state", "tri-axial tensile stress", "crack tip", "micro-scale lateral cracks", "oxide layer", "metal–oxide interface", "finite element analysis", "parise et al.", "localised tensile stresses", "interface roughness", "nano-scale cracks", "tetragonal to monoclinic phase transformation", "fracture", "de-bonding", "partially stabilised zircon"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The early theoretical work of Catlow assessed a number of Willis type clusters and found them all to be stable using potential-based methods [6]. More recently “split interstitial” type clusters (Fig. 1) have emerged from computational studies as stable species following the potential based investigation of Govers et al. which found the 2:2:2 cluster in a UO2 supercell relaxed to a split di-interstitial [13] (Fig. 1(b)); a single VO with three Oi displaced approximately 1.6Å in 〈111〉 directions from the VO. This result was later confirmed by the LSDA+U calculations of Geng et al. [7]. The family of split interstitial clusters was extended to include tri-interstitials [8] (a di-interstitial with the fourth Oi site occupied) and quad-interstitials [9] (two di-interstitials on adjacent sites, giving a total of two VO and six Oi) (Fig. 1(d)). Following this Andersson et al. postulated a model for U4O9 based on a UO2 supercell containing multiple split quad-interstitial clusters; following the prediction of their LSDA+U calculations that the quad-interstitial is more stable than its cuboctahedral counterpart [12].", "ground_truth": ["split interstitial” type clusters", "potential-based methods", "willis type clusters", "potential based investigation", "di-interstitial", "a single vo with three oi", "split interstitial clusters", "tri-interstitials", "di-interstitial with the fourth oi site occupied", "quad-interstitials", "two di-interstitials on adjacent sites", "model for u4o9", "multiple split quad-interstitial clusters", "lsda+u", "lsda+u", "uo2 supercell", "vo", "vo", "oi", "uo2 supercell", "quad-interstitial", "cuboctahedral"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "early theoretical work", "catlow", "willis type clusters", "stable", "potential-based methods", "split interstitial clusters", "computational studies", "govers et al.", "2:2:2 cluster", "uo2 supercell", "split di-interstitial", "vo (vacancy-oxygen)", "oi (oxygen interstitial)", "〈111〉 directions", "lsda+u calculations", "geng et al.", "tri-interstitials", "di-interstitial", "quad-interstitials", "andersson"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the calculations for the formation energy, the box size is set to 30a0×30a0×30a0, where a0 is the bcc Fe lattice parameter. For all calculations periodic boundary conditions and constant volume are used. The Monte Carlo algorithm used to determine the lowest energy configuration of the cluster [28] is organised as follows. First, the energetics of voids without helium are investigated. A vacancy is introduced into the simulation cell and the system is minimised using a conjugate gradient algorithm, yielding a single vacancy formation energy Evac of 1.72eV. Next, the atom with the highest potential energy is removed from the system and again the system is minimised. This scheme is iteratively continued to create voids up to the number of target vacancies and the formation energy of each is calculated. Next, helium atoms are introduced to the vacancies. The total system energy is measured and recorded. At this point, a Metropolis MC scheme [29] is used to find the low energy configurations. Every helium in the system is randomly displaced from its site up to a maximum of rmax (4.5Å, the cut off distance for He–He interactions) in each of the x, y and z directions and then minimised using the conjugate gradient algorithm. Each bubble is continued for a minimum of 10,000 steps. After that, the searches will be terminated if the system energy does not drop within a further 10 steps. A schematic of this iterative process is shown in Fig. 1.", "ground_truth": ["formation energy", "monte carlo algorithm", "metropolis mc scheme", "conjugate gradient algorithm", "minimised", "iterative process", "box", "fe lattice", "voids without helium", "simulation cell", "conjugate gradient algorithm", "total system energy", "helium atoms", "low energy configurations", "single vacancy formation energy", "evac", "minimised", "determine the lowest energy configuration", "he–he interactions", "he", "he", "helium"], "prediction_list": ["here is the list of extracted keywords from the given scientific paragraph:", "formation energy", "box size", "bcc fe lattice parameter (a₀)", "periodic boundary conditions", "constant volume", "monte carlo algorithm", "lowest energy configuration", "cluster", "voids", "helium", "vacancy", "simulation cell", "conjugate gradient algorithm", "single vacancy formation energy (evac)", "potential energy", "target"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The class of steels known as oxide dispersion strengthened (ODS) ferritic alloys (also known as nanostructured ferritic alloys) consist of a dispersion of ultra-fine oxide particles throughout the matrix. These oxide particles serve to improve the mechanical properties of the system, particularly at high temperatures, of the system through inhibiting dislocation motion and grain boundary sliding. In nuclear applications the oxide particles have been suggested to act as point defect sinks [10,11] to improve radiation tolerance, and as preferential sites for the formation of nano-scale He bubbles therefore reducing swelling compared to non-ODS steels [12–15]. The ability of the oxide particles to improve these properties depends on the structure and composition of the particles [10,11,16,17] and their stability under irradiation. Typical compositions of ODS steels include between 9 and 14at.% Cr for oxidation resistance (most commonly 14at.%); W for solid solution hardening; Y2O3 that is put into solid solution during the initial, mechanical alloying, process but then during consolidation at high temperatures forms precipitates; and Ti to inhibit significant growth of the oxide particles; the balance being made up of Fe and impurities [18]. For this reason these steels are often referred to as 14YWT, reflecting the constituent elements.", "ground_truth": ["structure and composition of the particles", "oxide dispersion strengthened", "ods", "nanostructured", "ferritic alloys", "ferritic alloys", "dispersion of ultra-fine oxide particles throughout the matrix", "oxide particles", "oxide particles", "oxide particles", "ods steels", "9 and 14at.% cr", "w", "y2o3", "ti", "balance", "fe and impurities", "improve the mechanical properties of the system", "class of steels", "14ywt", "steels", "solid solution", "non-ods steels", "nano-scale he bubbles", "improve radiation tolerance", "radiation", "irradiation", "oxide particles", "constituent elements", "mechanical alloying", "consolidation", "precipitates", "reducing swelling", "swelling", "oxidation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "oxide dispersion strengthened (ods) ferritic alloys", "nanostructured ferritic alloys", "ultra-fine oxide particles", "mechanical properties", "high temperatures", "dislocation motion", "grain boundary sliding", "nuclear applications", "point defect sinks", "radiation tolerance", "nano-scale he bubbles", "swelling reduction", "non-ods steels", "structure and composition"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Zirconium alloys are used as cladding to encapsulate fuel pellets in pressurised and boiling water nuclear reactors. Research into oxidation of these alloys has been significant since the introduction of the material. However, the microstructure and electro-chemical processes during oxidation are complex and many questions still remain unanswered. One such issue is the formation of lateral cracks near the metal-oxide interface. Small cracks have been seen to form continuously during oxidation, with large scale networks of lateral cracks forming cyclically every ∼2μm of oxide growth. These networks of cracks can be correlated with acceleration in the corrosion kinetics [1–7]. These lateral cracks might enable the link up of nano pores along grain boundaries perpendicular to the metal/oxide interface as reported in [8,9]. Experiments using Synchrotron X-Ray Diffraction (S-XRD) by both Polatidis et al. and Petigny et al., have separately shown that oxides formed on Zircaloy-4 are composed of monoclinic and stabilised tetragonal phases, with an ∼7% reduction in the tetragonal phase fraction from 1 to 3μm oxide growth [4,10]. One theory is that the lateral cracks may destabilise the tetragonal phase close to the metal-oxide interface. The phase transformation has an ∼6% expansion associated with it, which could lead to fracture perpendicular to the metal-oxide interface, thereby generating fast ingress routes for oxygen containing species [11,12].", "ground_truth": ["zirconium alloys", "cladding", "microstructure and electro-chemical processes", "corrosion kinetics", "link up of nano pores", "synchrotron x-ray diffraction", "s-xrd", "zircaloy-4", "oxides", "monoclinic and stabilised tetragonal phases", "∼7% reduction in the tetragonal phase", "fuel pellets", "water nuclear reactors", "oxidation", "oxidation of these alloys", "alloys", "oxidation", "metal-oxide interface", "oxidation", "grain", "metal/oxide interface", "metal-oxide interface", "tetragonal phase", "phase transformation", "fracture", "metal-oxide interface", "oxygen containing species", "ingress"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "zirconium alloys", "cladding", "fuel pellets", "pressurised water reactors", "boiling water reactors", "nuclear reactors", "oxidation", "microstructure", "electro-chemical processes", "lateral cracks", "metal-oxide interface", "corrosion kinetics", "nano pores", "grain boundaries", "synchrotron x-ray diffraction (s-xrd)", "polatidis", "petigny", "zircaloy-4", "19"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Spark plasma sintering (SPS) is a relatively new sintering-based technique [17] in which the powder to be consolidated is loaded into an electrically and thermally conductive graphite mould and a large DC pulsed current (1000–5000A) is applied under a uniaxial pressure. When current passes through the graphite mould (and the powder if it is electrically conductive), the powder is heated both from the outside (the mould acts as a heating element) and inside (due to Joule heating from the intrinsic electrical resistance of the powder material). SPS is characterised by very fast heating (up to 2000°C/min) and cooling rates and short holding times (minutes) to achieve near theoretical density [17]. Thus SPS occupies a very different time–temperature–density space in powder consolidation maps when compared with conventional methods, such as hot pressing sintering and HIP with ramp rate of 50–80°C/min and a few hours holding time. Although SPS has been studied for a rapidly growing number of materials [17], there are only a small number of studies on the fabrication and microstructural characterisation of ODS steels processed by SPS, briefly reviewed below.", "ground_truth": ["fabrication and microstructural characterisation of ods steels processed by sps", "spark plasma sintering", "sps", "sintering-based technique", "electrically and thermally conductive graphite mould and a large dc pulsed current (1000–5000a) is applied under a uniaxial pressure", "sps", "sps", "hot pressing sintering", "hip", "sps", "ods steels", "powder", "powder", "powder", "powder material", "mould", "heating element", "powder", "powder consolidation maps", "conventional methods", "sps", "graphite mould"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "spark plasma sintering (sps)", "sintering-based technique", "powder consolidation", "graphite mould", "dc pulsed current", "uniaxial pressure", "joule heating", "electrical resistance", "fast heating rates", "cooling rates", "holding times", "near theoretical density", "time–temperature–density space", "powder consolidation maps", "conventional methods", "hot pressing sintering", "hip (hot isostatic pressing)", "ramp rate", "microstructural characterisation", "ods steels (oxide dispersion-streng"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "To conclude, the electrochemical reduction of uranium dioxide to uranium metal has been studied in a lithium chloride–potassium chloride eutectic molten salt at 450°C. Both electrochemical and synchrotron X-ray techniques have been utilised to deduce the electrochemical reduction potential, mechanism and reduction pathway. The electrochemical reduction potential of the UO2|U couple is dependent on the activity of oxide ions existing within the melt. The electrochemical reduction of uranium dioxide to uranium metal seems to occur in a single, 4-electron-step, process; indicated by a single reduction peak (C1) in the cyclic voltammograms and also by the exclusion of any other phases in the EDXD data. The electrochemical reduction may be impeded by an increase in oxo-acidity of the molten salt. That is, O2− ions that are liberated by the electroreduction may not react at the counter electrode and, thus, not be removed from the molten salt. This could be due to the electrode geometry and/or the inherent microstructure of the working electrode: a high tortuosity, for example, would impede the diffusion of O2− ions out of the working electrode. This could then cause an increase in the activity of oxide ions existing within the melt and hence inhibit the electrochemical reduction – exploration of the microstructure of working electrodes will be the focus of future work.", "ground_truth": ["electrochemical reduction of uranium dioxide", "electrochemical", "synchrotron x-ray", "uranium metal", "lithium chloride–potassium chloride eutectic molten salt", "uranium dioxide", "molten salt", "molten salt", "cyclic voltammograms", "edxd data", "electrochemical reduction", "electrochemical reduction", "uo2|u couple", "uranium dioxide", "deduce the electrochemical reduction potential", "mechanism and reduction pathway", "oxide ions", "uranium metal", "electron", "o2− ions", "electroreduction", "counter electrode", "electrode", "melt", "electrode", "o2− ions", "electrode", "oxide ions", "melt", "electrochemical reduction", "electrodes", "exploration of the microstructure of working electrodes", "4-electron-step"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "electrochemical reduction", "uranium dioxide", "uranium metal", "lithium chloride–potassium chloride eutectic", "molten salt", "450°c", "synchrotron x-ray techniques", "reduction potential", "reduction mechanism", "reduction pathway", "uo2|u couple", "oxide ions", "4-electron-step process", "cyclic voltammograms"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The primary benefit of using a 3D model is that it allows the application of anisotropic material properties. As a hexagonal close packed lattice structure, a single zirconium grain is plastically anisotropic due to the difficulty of activating slip with a 〈c〉 component [23–26]. Abaqus allows this to be represented by setting plasticity potential ratios. The anisotropic elastic and plastic constants are shown in Table 1. Zirconium alloys can often have a bimodal basal pole distribution, with a tilt on the basal normal or c direction of ±30° in the normal direction being quoted for recrystallized Zircaloy-4 [27,28]. However, for simplicity the basal normal or c direction has been taken as being parallel to the normal direction. As such the 1, 2 and 3 directions in Table 1 correlate with the X, Y and Z global coordinate system for the 3D simulations, with the 3 direction correlating to the c direction of a zirconium unit lattice. Table 1 also shows the elastic properties incorporated into the simulations. The oxide layer has been simulated as a purely elastic material. Although it is known that the oxide is strongly textured [29], it is still simulated as a homogenous solid therefore isotropic material properties have been used for the oxide in all simulations.", "ground_truth": ["3d model", "anisotropic material properties", "abaqus", "plasticity potential ratios", "bimodal basal pole distribution", "isotropic material properties", "anisotropic elastic and plastic constants", "zirconium grain", "zirconium", "recrystallized zircaloy-4", "3d simulations", "zirconium unit lattice", "simulations", "oxide layer", "purely elastic material", "oxide", "oxide", "simulations"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "3d model", "anisotropic material properties", "hexagonal close packed lattice structure", "zirconium grain", "plastically anisotropic", "slip activation", "〈c〉 component", "abaqus", "plasticity potential ratios", "anisotropic elastic constants", "anisotropic plastic constants", "table 1", "zirconium alloys", "bimodal basal pole distribution", "basal normal", "c direction", "±30° tilt", "recrystallized zircaloy-4", "normal direction", "global coordinate system", "x", "y", "z directions"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Zirconium alloys are commonly used as the fuel cladding for water cooled nuclear fission reactors, mainly due to their low neutron cross-section, good corrosion resistance during normal operating conditions and sufficient mechanical strength [1]. Despite high corrosion resistance at normal operating temperatures (around 300 °C) [2], Zr alloys oxidise very rapidly when exposed to temperatures a few hundred degrees higher. This is an exothermic reaction, which can further accelerate oxidation and, at temperatures beyond 1000 °C, potentially lead to disintegration of the fuel rods, as highlighted during the Fukushima Daiichi nuclear accident. For this reason new research activities have been initiated worldwide to develop accident tolerant fuels (ATF). Additionally, ATFs could also provide further enhancements in corrosion performance during normal operating conditions enabling the development of fuel assemblies for very high burn-up.", "ground_truth": ["zirconium alloys", "fuel cladding", "zr alloys", "accident tolerant fuels", "atf", "atf", "oxidation", "disintegration of the fuel rods", "develop accident tolerant fuels", "enhancements in corrosion performance", "fuel rods", "oxidise", "exothermic reaction", "development of fuel assemblies", "water cooled nuclear fission reactors", "fuel assemblies"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "zirconium alloys", "fuel cladding", "water cooled nuclear fission reactors", "low neutron cross-section", "corrosion resistance", "mechanical strength", "operating temperatures", "oxidation", "exothermic reaction", "fuel rods disintegration", "fukushima daiichi nuclear accident", "accident tolerant fuels (atf)", "corrosion performance", "high burn-up", "fuel assemblies"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In all these studies, the association between the transition and lateral cracking in the oxide layer depicts some interaction between the mechanical behaviour of the system, and its corrosion kinetics, but does not provide a clear understanding of the morphology of the metal:oxide interface during the corrosion process, at the nanometre level. Understanding why this transition behaviour happens is critical when modelling the rate of growth of oxide, and therefore to the lifetime prediction of Zr clads, and ultimately to the safety of nuclear power reactors. No model will be complete without a nanoscale understanding of what is going on during oxidation. Thus, it is essential that the oxide scale and the top layers of the metal are studied at nanometre resolution to reveal the detailed structural and chemical changes associated with diffusion of oxygen and the resulting oxidation of the metal. Whilst a number of techniques have been employed for this purpose, it is clear that various techniques within transmission electron microscopy (TEM) will be among the most versatile and informative for this purpose, although additional information can be added by techniques such as atom probe tomography.", "ground_truth": ["understanding of the morphology of the metal:oxide interface", "transmission electron microscopy", "tem", "atom probe tomography", "zr clads", "modelling the rate of growth of oxide", "oxide scale", "top layers of the metal", "oxidation", "oxidation of the metal", "nuclear power reactors", "corrosion process", "metal:oxide interface", "oxide layer", "understanding why this transition behaviour happens", "oxide", "understanding of what is going on during oxidation", "diffusion of oxygen", "oxygen", "structural and chemical changes", "the metal"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "association", "transition", "lateral cracking", "oxide layer", "interaction", "mechanical behaviour", "corrosion kinetics", "morphology", "metal:oxide interface", "corrosion process", "nanometre level", "transition behaviour", "modelling", "rate of growth", "oxide", "lifetime prediction", "zr clads", "safety", "nuclear power reactors", "nanoscale understanding", "oxidation", "oxide scale", "metal layers", "nanometre resolution", "structural changes", "chemical changes", "diffusion of oxygen", "oxidation of the metal"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Solid pieces of 23–114 mg were further used to measure the enthalpy increments using a Setaram Multi-detector High Temperature Calorimeter (MDHTC-96) using a drop detector. For more details about the technique we refer to our previous studies [9,10]. The measurements were carried out under an argon atmosphere (with an oxygen content of 7 ppm), using pure platinum ingots (64–144 mg) of 99.95 at % purity as a reference material. The temperature range of the experiment was from 430.3 K to 1088.8 K using steps of 50 K. Each isothermal run consisted of 2–4 drops of Bi2UO6 samples, each surrounded by two drops of platinum from which the sensitivity of the device was determined. The drops were separated by time intervals of 20 min, long enough to re-stabilize the monitored heat flow signal. Background subtraction and peak integration were performed using commercially available software for data processing. The reported temperatures were corrected in accordance with the calibration curve obtained prior to measurement using several high purity standard metals (Sn, Pb, Zn, Al, Ag, Ni) with various melting temperatures in order to cover the whole temperature range of the measurement. After drop calorimetric measurements at the maximum considered temperature, the material was subjected to a new XRD measurement, confirming the stability of the compound under the experimental conditions.", "ground_truth": ["enthalpy increments", "solid pieces", "setaram multi-detector high temperature calorimeter", "mdhtc-96", "drop detector", "2–4 drops of bi2uo6 samples", "platinum ingots", "reference material", "platinum", "background subtraction and peak integration", "corrected", "high purity standard metals", "calorimetric measurements", "xrd", "commercially available software for data processing", "data processing", "argon", "measure the enthalpy increments", "oxygen", "isothermal run", "compound", "sn", "pb", "zn", "al", "ag", "ni", "heat flow"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "solid pieces", "enthalpy increments", "setaram multi-detector high temperature calorimeter (mdhtc-96)", "drop detector", "argon atmosphere", "oxygen content", "platinum ingots", "purity", "reference material", "temperature range", "isothermal run", "bi2uo6 samples", "sensitivity", "heat flow signal", "background subtraction", "peak integration", "data processing", "calibration curve", "high purity standard metals (sn", "pb", "zn", "al", "ag", "ni)", "mel"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The fluence of each capsule was determined by using activation monitor sets. These monitor sets consist of different metal wire pieces that have an activation reaction at a specific energy range. The different activation energies are chosen in such a way that the spectrum can be reconstructed. In BODEX, each capsule contained a flux monitor set on the ‘back side’ (as seen from the core) and one on the front side, positioned at the central height of the capsules. Additionally, one detector was placed at the top and one at the bottom, resulting in a total of 6 monitor sets per leg. The fluence in each capsule was determined as the average between the two flux monitor located in each capsule. The sets have been analysed by determining the activation of each wire piece, which indicates the fluence of a specific energy range. Table 3 show the values of the fluences for the two capsules containing molybdenum.", "ground_truth": ["fluence of each capsule", "activation monitor sets.", "monitor sets", "metal wire pieces", "monitor", "flux monitor", "molybdenum", "capsules", "activation", "capsule", "capsule", "flux monitor set", "capsule", "capsule", "detector", "capsules", "determining the activation of each wire piece", "wire piece", "fluence", "fluences"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fluence", "capsule", "activation monitor sets", "metal wire pieces", "activation reaction", "specific energy range", "spectrum", "reconstructed", "bodex", "flux monitor set", "back side", "front side", "core", "central height", "detector", "top", "bottom", "monitor sets per leg", "average"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Following fission, noble gas atoms will be distributed in the fuel matrix initially accommodated at point defects trap sites, generally thought to be Schottky trivacancy defects [4,5,31]. Diffusion to either bubbles or grain boundaries is then facilitated by associating a further uranium vacancy defect for the gas atom to ‘hop’ into, with the original vacancy then able to loop around to ensure continued diffusion. The rate determining step in the process is not the migration of the Xe itself but rather the rearrangement of the VU defect to facilitate net Xe diffusion [6–8]. Activation energies for the overall process depend on the availability of the defect trap sites, which in turn depends on the crystal stoichiometry. For Xe diffusion in UO2−x, UO2 and UO2+x the activation energies calculated using DFT are 7.04–12.92 eV, 4.15–7.88 eV and 1.38–4.07 eV with the ranges reflecting the way the calculations were performed depending on the charge states of the defects involved and the presence of a Jahn–Teller distortion [7]. Activation energies calculated using empirical pair potentials can vary strongly depending on the choice of potential. Govers et al. examined three different potentials for UO2 (those of Basak [9], Jackson [10] and Morelon [11]) coupled with different parameterisations for the U–Xe and O–Xe interactions from Geng [12] and Nicoll [13] and recommend values of 6.5 eV, 4.5 eV and 2.4 eV [6] for the different stoichiometric regimes in very good agreement with the experimental values of 6.0 eV, 3.9 eV and 1.7 eV respectively [14].", "ground_truth": ["noble gas atoms", "fuel matrix", "fission", "point defects trap sites", "schottky trivacancy defects", "diffusion", "bubbles", "grain boundaries", "uranium", "gas atom", "‘hop’ into", "loop around", "diffusion", "migration", "xe", "vu", "rearrangement", "vu defect", "xe", "xe diffusion", "activation", "defect trap sites", "crystal stoichiometry", "crystal", "xe", "xe diffusion", "uo2−x", "uo2", "uo2+x", "dft", "charge", "jahn–teller distortion", "empirical pair potentials", "potential", "potentials", "uo2", "u–xe", "o–xe"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "fission", "noble gas atoms", "fuel matrix", "point defects", "trap sites", "schottky trivacancy defects", "diffusion", "bubbles", "grain boundaries", "uranium vacancy defect (vu defect)", "xe diffusion", "rate determining step", "activation energies", "crystal stoichiometry", "uo₂₋ₓ"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Ferritic and martensitic steels are candidate materials for use in nuclear reactors [1,2]. The transmutation-created inert gas, especially He, plays an important role in the microstructural evolution of these steels under neutron irradiation. In a previous paper [3] the mechanisms by which He in a perfect body-centred-cubic (bcc) Fe lattice, can agglomerate into bubbles was discussed. It was shown that small He interstitial clusters are highly mobile but become effectively pinned with the emission of Fe interstitials when the clusters contain 5 or more He atoms. Small bubbles up to around 1.5 nm in diameter can easily form at room temperature from such seed points but larger bubbles are more difficult to form by diffusion alone due to the induced strain in the bcc lattice which increases the energy barriers for diffusion towards the bubbles whilst reducing them in a direction away from the bubbles. Subsequent bubble enlargement can then only occur either through increased temperature or by radiation induced mechanisms which increase the number of vacancies in the bubble and reduce the lattice strain. Emission of interracial loops from such a bubble was not observed in molecular dynamics simulations.", "ground_truth": ["mechanisms by which he in a perfect body-centred-cubic (bcc) fe lattice", "can agglomerate into bubbles", "ferritic and martensitic steels", "microstructural evolution of these steels under neutron irradiation", "he interstitial clusters", "molecular dynamics simulations", "bubbles", "bubbles", "diffusion", "fe interstitials", "he atoms", "diffusion", "induced strain", "bcc lattice", "bubbles", "bubbles", "bubble", "lattice strain", "nuclear reactors", "transmutation-created inert gas", "he", "neutron irradiation", "steels", "he", "bubbles", "body-centred-cubic", "bcc", "fe lattice", "bubble", "bubble enlargement", "radiation induced mechanisms", "reduce the lattice strain", "bubble", "emission of interracial loops"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "ferritic steels", "martensitic steels", "nuclear reactors", "transmutation", "inert gas", "helium (he)", "microstructural evolution", "neutron irradiation", "body-centred-cubic (bcc) fe lattice", "he agglomeration", "bubbles", "he interstitial clusters", "mobility", "fe interstitials", "bubble formation", "room temperature", "diffusion", "lattice strain", "energy barriers", "bubble enlargement", "temperature effects", "radiation-induced mechanisms", "vacancies"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "An increase of neutron leakage from the core region can be achieved through modifications in the core geometry (usually by adopting a pan-cake geometry of the active core region at the expense of the general neutron economy). Extensive studies determined a set of core design modifications that optimised the total sodium void reactivity (becoming less positive). Among the most efficient design solutions identified is an enlarged sodium plenum above the active core region in combination with an absorber layer above the sodium plenum (to reduce neutron backscattering from the reflector region above the plenum). Fig. 19 shows the combined effect of different upper plenum thicknesses of the absorber and boron layers. It can be observed that the sequential increase of the layer's thickness converge to an asymptotic value of reactivity reduction slightly over 800pcm. The pair of values selected was 60cm for the sodium plenum and 30cm for the boron layer. These modifications implied a considerable increase in the sub-assembly length that was compensated by reducing the upper axial reflector width (Sun et al., 2013).", "ground_truth": ["core design modifications that optimised the total sodium void reactivity", "modifications in the core geometry", "pan-cake geometry of the active core region", "design solutions", "enlarged sodium plenum", "absorber layer", "reduce neutron backscattering", "core region", "neutron leakage", "neutron", "sodium", "sodium plenum", "core", "sodium plenum", "reduce neutron backscattering from the reflector region above the plenum", "plenum", "upper plenum", "absorber and boron layers", "reactivity reduction", "sodium plenum", "boron layer", "increase in the sub-assembly length", "reducing the upper axial reflector width", "axial reflector", "increase of the layer's thickness"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "neutron leakage", "core region", "core geometry", "pan-cake geometry", "active core region", "neutron economy", "core design modifications", "sodium void reactivity", "design solutions", "sodium plenum", "absorber layer", "neutron backscattering", "reflector region", "upper plenum thickness", "boron layers", "reactivity reduction", "asymptotic value", "sub-assembly length", "axial reflector", "sun et al.", "2013"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The design, and the temperature reached in the sample holders, guarantees that the Na remains liquid during operation to improve the heating transfer and avoiding solid formation (too cold working temperature) or sodium boiling (too hot working temperature). The temperature above and just below the Na surface will be monitored by six dedicated thermocouples. In order to prevent oxidation of the Na, the plenum of the 1st containment is filled with high-purity He at 0.1MPa, sealed after final assembly and kept closed during in-pile operation (no gas circulation in the 1st containment). The heat generated by fission and gamma absorption in the materials will be radially dissipated through the Na bath, the structural materials and the gas gaps by conduction and radiation to the downstream primary coolant of the TRIO wet channel.", "ground_truth": ["improve the heating transfer", "monitored by six dedicated thermocouples", "na", "na", "na bath", "sample holders", "sodium boiling", "sodium", "na surface", "oxidation", "high-purity he", "trio wet channel", "solid formation", "gas circulation", "gas", "fission and gamma absorption", "gas"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "design", "temperature", "sample holders", "na (sodium)", "liquid", "operation", "heating transfer", "solid formation", "cold working temperature", "sodium boiling", "hot working temperature", "thermocouples", "oxidation", "plenum", "1st containment", "high-purity he (helium)", "0.1mpa", "sealed", "final assembly", "in-pile operation", "gas circulation", "heat generation", "fission", "gamma absorption", "materials", "radial dissipation", "na"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Geomagnetic jerks are conspicuous yet poorly understood phenomena of Earth’s magnetic field, motivating investigations of their morphology and the theory behind their origins. Jerks are most commonly defined by their observed form at a single observatory as ‘V’ shapes in a single component of the geomagnetic secular variation (SV), the first time derivative of the main magnetic field (MF). The times of the gradient changes, which separate linear trends of several years, have associated step changes in the second time derivative of the MF (secular acceleration (SA)) and impulses in the third time derivative. The ‘V’ shape SV definition of jerks includes an implicit expectation of a ‘large’ magnitude step change in the gradient without definition of this scale or its threshold value other than the basic need for it to be observable in the data above the highly variable background noise. Jerks can be described by their amplitude, that is, the difference in the gradients of the two linear SV segments about a jerk, A=a2-a1, where a2 is the gradient after the jerk and a1 is the gradient before the jerk. This measure is essentially the best fit SA change across a jerk. Jerk amplitude is thus positive for a positive step in SA and negative for a negative step. Here we do not consider spatial extent in our definition and refer to individual features in one field component of a given observatory time series as a single jerk.", "ground_truth": ["geomagnetic jerks", "earth’s magnetic field", "jerks", "geomagnetic secular variation", "sv", "secular variation", "main magnetic field", "magnetic field", "mf", "gradient changes", "mf", "secular acceleration", "sa", "jerks", "‘large’ magnitude step change", "jerks", "sv segments", "jerk", "jerk", "jerk", "sa change", "jerk", "jerk amplitude", "sa", "jerk"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "geomagnetic jerks", "earth’s magnetic field", "morphology", "origins", "observatories", "‘v’ shapes", "geomagnetic secular variation (sv)", "first time derivative", "main magnetic field (mf)", "gradient changes", "linear trends", "second time derivative", "secular acceleration (sa)", "third time derivative", "step change", "background noise", "amplitude", "gradients", "positive step", "negative step", "spatial extent", "field component", "observatory time series"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Seismic tomography is a powerful tool to investigate the deep structure under the volcanoes. With the recently rapid development of Chinese provincial seismic networks (Zheng et al., 2009, 2010) and some portable seismic arrays (Hetland et al., 2004; Duan et al., 2009; Lei et al., 2012b) around the volcanoes, it has become possible to image the detailed 3-D velocity structure under some of these volcanoes, where seismic stations are densely spaced. In this overview, we synthesize the results from the deep seismic images of the upper mantle under the Changbaishan, Tengchong, Hainan volcanoes as well as the Datong volcano (Fig. 1). We also evaluate the advantages of recently updated seismic tomographic techniques for deriving potential information. This work updates a previous review of Zhao and Liu (2010) on this topic, with more detailed synthesis of all the available information.", "ground_truth": ["advantages of recently updated seismic tomographic techniques", "synthesize the results from the deep seismic images of the upper mantle", "seismic tomography", "investigate the deep structure under the volcanoes", "provincial seismic networks", "portable seismic arrays", "image the detailed 3-d velocity structure", "changbaishan", "tengchong", "hainan volcanoes as well as the datong", "deep seismic images", "seismic tomographic techniques", "seismic stations", "3-d velocity structure"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "seismic tomography", "deep structure", "volcanoes", "chinese provincial seismic networks", "zheng et al.", "2009", "2010", "portable seismic arrays", "hetland et al.", "2004", "duan et al.", "2009", "lei et al.", "2012b", "3-d velocity structure", "seismic stations", "deep seismic images", "upper mantle", "changbaishan", "tengchong", "hainan volcanoes", "datong volcano", "seismic tomographic techniques", "potential"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Microhardness can be related to other macroscopic mechanical properties such as yield stress, σ, and elastic modulus, E, both derived from compression testing. For work-hardened metals, Tabor derived a direct proportionality between hardness and compressive yield stress: H≈3σ [20]. However, it was soon realized that Tabor's relationship only applies to materials that exhibit full plasticity [9,10]. Deviations from this relationship have been reported for a number of metals, glasses and polymers where the elastic strains are non-negligible [9]. Hence, the different expressions describing the correlation of hardness with conventional macroscopic mechanical properties rely on the validity of the above-mentioned elasto-plastic models. In this way, hardness and yield stress no longer hold direct proportionality but their relationship depends on the specific material properties, such as Poisson's ratio and elastic modulus [9,11–13]. It has been shown that these elasto-plastic models not only satisfactorily explain an H/σ ratio of ≈2 for a number of polyethylene materials of different nature, but also theoretically account for the range of H/E ratios experimentally determined [21].", "ground_truth": ["compression testing", "work-hardened metals", "glasses", "polymers where the elastic strains", "polyethylene materials", "microhardness", "derived a direct proportionality", "materials that exhibit full plasticity", "deviations from this relationship", "the different expressions describing the correlation of hardness with conventional macroscopic mechanical properties", "specific material properties", "", "hold direct proportionality but their relationship"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "microhardness", "macroscopic mechanical properties", "yield stress (σ)", "elastic modulus (e)", "compression testing", "work-hardened metals", "tabor", "proportionality", "hardness", "compressive yield stress", "full plasticity", "elastic strains", "elasto-plastic models", "poisson's ratio", "polyethylene materials", "h/σ ratio", "h/e ratio"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Up to now, morphological studies of the multi-component polymeric materials have been carried out by various microscopic and scattering methods. Optical microscopes, transmission electron microscopes (TEMs), scanning electron microscopes (SEMs) and atomic force microscopes (AFMs) are commercially available and widely used. The biggest advantage of microscopy is that they provide intuitive real-space representations of the various morphologies. However, when it comes to “measurements”, especially in a quantitative way, microscopy sometimes lacks a statistical accuracy due to the small field of view. In contrast, the scattering methods provide much a superior statistical accuracy than that of microscopy simply because the observation volume is larger than that of the microscopes. One must remember, however, that the scattering methods normally require “(hypothesized) models” for data analysis in advance: They do not provide an intuitive insight into the morphologies as does microscopy. After all, for the complete characterization of a specific morphology, one may need to first know the morphologies from the microscopy and subsequently to evaluate the structural parameters by scattering on the basis of the morphology; the two methods are complementary.", "ground_truth": ["morphological studies", "multi-component polymeric materials", "microscopic and scattering methods", "optical microscopes", "transmission electron microscopes", "tems", "scanning electron microscopes", "sems", "atomic force microscopes", "afms", "microscopy", "microscopy", "scattering methods", "microscopy", "microscopes", "scattering methods", "“(hypothesized) models”", "data analysis", "microscopy", "microscopy"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "morphological studies", "multi-component polymeric materials", "microscopic methods", "scattering methods", "optical microscopes", "transmission electron microscopes (tems)", "scanning electron microscopes (sems)", "atomic force microscopes (afms)", "real-space representations", "morphologies", "quantitative measurements", "statistical accuracy", "field of view", "observation volume", "hypothesized models", "data analysis", "intuitive insight", "complete characterization", "structural parameters", "complementary methods"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We deal with the intensity scattered by a random mixture of deuterated/hydrogenated PE chains. The algorithm used by us to evaluate the Kratky plots by sets of parallel polymer stems is very simplified. We checked it to be adequate in the reciprocal coordinate range under investigation [0<q(=4πsinθ/λ)≤0.25Å−1] comparing the results with more precise calculations. The scattering centres are identified with pseudo-atoms repeating after a constant distance of 1.27Å along straight lines coinciding with the stem axes, 100 scattering centres being placed on each stem; the scattering by atoms belonging to chain folds is neglected. The parallel stem axes are disposed according to a hexagonal setting – a rough approximation to the monoclinic, pseudo-hexagonal structure of PE – and the scattering centres have the same axial coordinates in all the stems. Defining an integer i going from 1 to the total number ns·100 of scattering centres, we have (q<1) [9](1A)q2·I(q)=C·(bH−bD)2∑i=1ns·100∑j=1ns·1004πqsin(q·dij)dij;dij2=Δij2+(zj−zi)2;q=2πsinθλwhere bH, bD respectively are the scattering lengths of hydrogen and deuterium, dij is the distance between C atoms, 2θ is the diffraction angle and λ the wavelength. The i-th C atom coordinate along the stem axis is zi and Δij is the distance between the stem axes where the atoms i and j belong. For all the stems we have the same set of zi coordinates. The sum in Eq. (1A) is extended to all the stems of the crystalline domain, see Figs. 2 and 10 for examples.", "ground_truth": ["pe chains", "deuterated", "hydrogenated", "kratky plots", "parallel polymer stems", "calculations", "scattering centres", "pseudo-atoms", "stem axes", "stem", "scattering", "atoms", "chain folds", "parallel stem axes", "pe", "stems", "scattering centres", "hydrogen", "deuterium", "c atoms", "crystalline", "stems"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "intensity scattered", "random mixture", "deuterated/hydrogenated pe chains", "algorithm", "kratky plots", "parallel polymer stems", "reciprocal coordinate range", "q range (0<q≤0.25å⁻¹)", "scattering centres", "pseudo-atoms", "constant distance (1.27å)", "stem axes", "chain folds", "hexagonal setting", "monoclinic structure", "pseudo-hexagonal structure", "axial coordinates", "scattering lengths (bh", "bd)", "diffraction angle (2θ"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "With ever increasing computer performance, simulations in much larger systems have become feasible. However, full-atomistic approaches to polymer crystallization need extremely large computer power even in the case of simple polymers, and appropriate modeling or coarse-graining of the system is imperative. From a series of work on the development of coarse-grained models for polymers, Mayer and Muller-Plathe have build up a model of poly(vinyl alcohol) (PVA) for studying early stage of crystallization. They investigated the emergence of crystalline order from the isotropic melt by rapid quenching [51,52]. They could reproduce many elementary processes of homogenous nucleation that showed good correspondence with experiments and other simulations, in temperature dependence of lamella thickness, structure of fold surface, etc. In their work, they neglected long-range force (van der Waals attraction) to accelerate computation. Their model has the energy contribution due to intrachain interactions only and the dominant driving force for crystallization is entropic, which seems to ignore dominant driving force for polymer crystallization in conventional sense. However, their work is reminiscent of the classical solid–liquid transition in systems of repulsive spherical atoms [53] and poses an intriguing problem as to the intrinsic driving force for polymer crystallization.", "ground_truth": ["simulations", "full-atomistic approaches", "polymer crystallization", "simple polymers", "appropriate modeling", "coarse-graining of the system", "coarse-grained models", "polymers", "poly(vinyl alcohol)", "pva", "crystallization", "crystalline", "isotropic melt", "rapid quenching", "homogenous nucleation", "simulations", "lamella", "fold surface", "intrachain interactions", "crystallization", "polymer crystallization", "polymer", "classical solid–liquid transition", "repulsive spherical atoms", "polymer crystallization", "polymer", "liquid", "solid"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "computer performance", "simulations", "larger systems", "full-atomistic approaches", "polymer crystallization", "computer power", "simple polymers", "modeling", "coarse-graining", "coarse-grained models", "poly(vinyl alcohol) (pva)", "early stage of crystallization", "crystalline order", "isotropic melt", "rapid quenching", "elementary processes", "homogeneous nucleation", "experiments", "simulations", "temperature dependence", "lamella thickness", "fold surface", "long-range force", "van der waals attraction", "computation", "intrachain"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "SPM, and AFM in particular, has been widely applied to questions in polymer crystallization. The technique has several strengths that make it ideally suited for such studies. It is a high resolution technology, routinely resolving sub 10nm features [13,14], and hence allowing the fundamental length scale of the polymer lamellar crystal, its thickness, to be observed. AFM requires no staining or metal coating of the sample, so sample preparation is relatively straightforward. Also, it is non-destructive under many circumstances. This allows images to be obtained while a process such as crystal growth or melting is occurring, giving time-resolved data at lamellar or sub-lamellar resolution [15–18]. It is this final feature that provides many of the most exciting possibilities of AFM for studying polymer crystallization, as it is now possible to watch crystal growth, crystal melting, and re-organisations within crystals at the lamellar scale, seeing how structure evolves and local conditions influence kinetics. AFM has a wide range of different measuring modes, and, with the ever increasing number of functional semicrystalline polymers available (e.g. [19]), the breadth of experiments that can be carried out with a single machine is also one of the techniques attractions.", "ground_truth": ["spm", "afm", "polymer crystallization", "polymer", "high resolution technology", "resolving sub 10nm features", "polymer lamellar crystal", "afm", "staining", "metal coating", "sample preparation", "crystal growth", "melting", "crystal", "time-resolved data", "afm", "studying polymer crystallization", "polymer crystallization", "crystal growth", "crystal melting", "re-organisations within crystals at the lamellar scale", "crystals", "crystal", "crystal", "afm", "semicrystalline polymers", "questions in polymer crystallization"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "spm", "afm", "polymer crystallization", "high resolution", "sub 10nm features", "polymer lamellar crystal", "sample preparation", "non-destructive", "time-resolved data", "lamellar resolution", "sub-lamellar resolution", "crystal growth", "crystal melting", "re-organisations", "structure evolves", "local conditions", "kinetics", "measuring modes", "functional semicrystalline polymers", "experiments"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Inverse miniemulsion polymerization is a water-in-oil (W/O) heterogeneous polymerization process that forms kinetically stable macroemulsions at, below, or around the critical micellar concentration (CMC). This process contains aqueous droplets (including water-soluble monomers) stably dispersed, with the aid of oil-soluble surfactants, in a continuous organic medium. Stable inverse miniemulsions are formed under high shear by either a homogenizer or a high speed mechanical stirrer. Oil-soluble nonionic surfactants with hydrophilic-lipophilic balance (HLB) value around 4 are used to implement colloidal stability of the resulting inverse emulsion. Upon addition of radical initiators, polymerization occurs within the aqueous droplets producing colloidal particles (Fig. 2) [83]. Several reports have demonstrated the preparation of stable particles of hydrophilic and water-soluble polymers [86–89], polyaniline nanoparticles [90], and organic–inorganic hybrid particles [91–93]. This method also allows for the preparation of crosslinked microgels in the presence of difunctional crosslinkers [27,94–100]. In addition, CRP techniques including ATRP [78,79,82,101,102] and RAFT [103] in inverse miniemulsion have been explored to prepare well-defined nanoparticles and nanogels.", "ground_truth": ["inverse miniemulsion polymerization", "water-in-oil", "w/o", "heterogeneous polymerization process", "kinetically stable macroemulsions", "critical micellar concentration", "cmc", "aqueous droplets", "water-soluble monomers", "oil-soluble surfactants", "organic medium", "stable inverse miniemulsions", "homogenizer", "high speed mechanical stirrer", "oil-soluble nonionic surfactants", "implement colloidal stability of the resulting inverse emulsion", "inverse emulsion", "radical initiators", "polymerization", "aqueous droplets", "colloidal particles", "stable particles of hydrophilic and water-soluble polymers", "polyaniline nanoparticles", "organic–inorganic hybrid particles", "preparation of crosslinked microgels in the presence of difunctional crosslinkers", "crosslinked microgels", "difunctional crosslinkers", "crp techniques", "atrp", "raft", "inverse miniemulsion", "nanoparticles", "nanogels"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "inverse miniemulsion polymerization", "water-in-oil (w/o)", "heterogeneous polymerization", "macroemulsions", "critical micellar concentration (cmc)", "aqueous droplets", "water-soluble monomers", "oil-soluble surfactants", "organic medium", "high shear", "homogenizer", "high-speed mechanical stirrer", "nonionic surfactants", "hydrophilic-lipophilic balance (hlb)", "colloidal stability", "radical initiators", "colloidal particles", "hydrophilic polymers", "water-soluble polymers"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In contrast with polymers, which are typically synthesized in the liquid phase, SWNTs are produced through a variety of synthesis techniques that typically involve the reaction of a gaseous carbon feedstock to form the nanotubes on catalyst particles. MWNTs were first observed in arc discharge fullerene reactors [1,26]; this technique was later adapted to produce SWNTs [3]. Similarly, the fullerene production method of laser ablation [27] was adapted to produce SWNTs (∼1.4nm diameter) in larger quantities on metal catalyst particles [28–30]. A number of chemical vapor deposition (CVD) processes have been developed to grow SWNTs and MWNTs, all involving the reaction of a gaseous carbon compound as feedstock. These processes include fluidized bed [31], “carpet” growth of carbon nanotubes (CNTs) from catalyst particles embedded in a substrate [32–35] as shown in Fig. 3, and “catalytic gas flow CVD” [36,37]. One of the most effective, cheap, and scalable CVD techniques is the HiPco (high-pressure CO) process, which does not use pre-formed catalyst particles unlike most other CVD techniques [38].", "ground_truth": ["polymers", "synthesized in the liquid phase", "swnts", "produced through a variety of synthesis techniques", "the reaction of a gaseous carbon feedstock to form the nanotubes on catalyst particles", "observed in arc discharge fullerene reactors", "swnts", "mwnts", "this technique", "swnts", "metal catalyst particles", "the fullerene production method of laser ablation", "chemical vapor deposition", "swnts", "mwnts", "the reaction of a gaseous carbon compound as feedstock", "fluidized bed", "“carpet” growth of carbon nanotubes (cnts) from catalyst particles embedded in a substrate", "“catalytic gas flow cvd”", "the most effective", "cheap", "and scalable cvd techniques", "hipco (high-pressure co) process", "does not use pre-formed catalyst particles", "cvd techniques", "cvd", "carbon nanotubes", "cnts", "cvd techniques"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "polymers", "liquid phase", "swnts (single-walled carbon nanotubes)", "synthesis techniques", "gaseous carbon feedstock", "nanotubes", "catalyst particles", "mwnts (multi-walled carbon nanotubes)", "arc discharge fullerene reactors", "laser ablation", "metal catalyst particles", "chemical vapor deposition (cvd)", "fluidized bed", "carpet growth", "carbon nanotubes (cnts)", "substrate", "catalytic gas flow cvd", "hipco (high-pressure co) process"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "ELRs are particularly attractive for the synthesis of block copolymers that self-assemble into polymer nanostructures such as micelles. The first work in this area involved an elastin-mimetic di-block copolymer containing VPGEG–(IPGAG)4 and VPGFG–(IPGVG)4 as the hydrophilic and hydrophobic blocks, respectively [49]. The resulting micelles were studied by dynamic light scattering (DLS) and DSC was used to measure the enthalpy of self-assembly. A tri-block copolymer was subsequently synthesized and the TEM images of this polymer showed that it formed spherical aggregates [50]. Other multivalent spherical micelles have been obtained from linear elastin-like AB di-block copolymers in the temperature range 37–42°C with the aim of targeting cancer cells [51]. Bidwell et al. have also exploited the ELRs for its ability to serve as macromolecular carriers for thermally targeted delivery of drugs. Attachment of doxorubicin to ELR-based system showed enhanced cytotoxicity in uterine sarcoma cells when aggregation was induced with hyperthermia [52].", "ground_truth": ["elrs", "synthesis of block copolymers", "block copolymers", "polymer nanostructures", "micelles", "elastin-mimetic di-block copolymer", "vpgeg–(ipgag)4", "vpgfg–(ipgvg)4", "hydrophilic and hydrophobic blocks", "micelles", "dynamic light scattering", "dls", "dsc", "self-assembly", "tri-block copolymer", "tem images", "polymer", "spherical aggregates", "multivalent spherical micelles", "linear elastin-like ab di-block copolymers", "targeting cancer cells", "elrs", "macromolecular carriers", "thermally targeted delivery of drugs", "doxorubicin", "elr-based system", "cytotoxicity", "uterine sarcoma cells", "hyperthermia", "aggregation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "elrs (elastin-like recombinamers)", "block copolymers", "self-assemble", "polymer nanostructures", "micelles", "elastin-mimetic di-block copolymer", "vpgeg–(ipgag)4", "vpgfg–(ipgvg)4", "hydrophilic block", "hydrophobic block", "dynamic light scattering (dls)", "dsc (differential scanning calorimetry)", "enthal"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In general, the ion exchange capacity (IEC) is closely related to the proton conductivity of PEMs because the acid functionalities, such as sulfonic acid groups, contribute to the proton conduction in a membrane. Beyond a certain sulfonation degree, PEMs tend to absorb too much water or are even soluble in water, which negatively affect their mechanical resistance and water resistance [17,18]. Therefore, the improvement of proton conductivity using aromatic polymers with moderately adjusted IEC values has been under intense investigation [19–23]. To achieve high proton conductivity with moderate IEC values, the formation of ion channel structures, which enable effective proton conduction, has been studied. In the course of these studies, an ideal morphology has been pursued by microphase separation of segmented block copolymers in which hydrophilic sulfonated polymer segments form an interconnected three-dimensional network responsible for efficient proton transport, while a complementary network of hydrophobic non-sulfonated segments imparts a reinforcing effect, preventing excessive swelling in water and enhancing the mechanical properties. An image of the ideal morphology for PEMs is shown in Fig. 2.", "ground_truth": ["ion exchange capacity", "iec", "proton conductivity", "pems", "sulfonic acid groups", "proton conduction", "membrane", "pems", "water", "water", "water", "improvement of proton conductivity using aromatic polymers with moderately adjusted iec values", "proton conductivity", "proton", "aromatic polymers", "high proton conductivity", "iec", "formation of ion channel structures", "effective proton conduction", "proton", "ion channel structures", "segmented block copolymers", "hydrophilic sulfonated polymer segments", "interconnected three-dimensional network", "proton transport", "complementary network of hydrophobic non-sulfonated segments", "hydrophobic non-sulfonated segments", "preventing excessive swelling in water", "enhancing the mechanical properties", "pems"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "ion exchange capacity (iec)", "proton conductivity", "pems (proton exchange membranes)", "acid functionalities", "sulfonic acid groups", "proton conduction", "sulfonation degree", "water absorption", "mechanical resistance", "water resistance", "aromatic polymers", "ion channel structures", "effective proton conduction", "morphology", "microphase separation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "From a general point of view, polymerization techniques can be divided into two types of chemical reactions: step-growth polymerization and free radical polymerization. Step-growth polymerization is widely used for synthesis of polyesters, polyamide and epoxies while the synthesis of polyacrylics requires the use of free radical polymerization. These polymerization reactions can be performed either in bulk or in solution or in dispersed media. Heterophase polymerizations (i.e. emulsion, dispersion and miniemulsion polymerizations) present the advantage of easier removal of the resulting product from the reactor compared to bulk polymerization thanks to the low viscosity of the reaction medium. Polymerization in solution also induces lower viscosity but also lower reaction rates due to dilution of the reactants and higher cost and environmental impact due to the use of organic solvents. These problems are solved in the case of heterophase polymerizations where the reactants are confined inside droplets (no dilution effect) and water is used as medium. The use of surfactant molecules are usually needed for the stabilization of the monomer droplet and subsequent polymer particles in the water phase.", "ground_truth": ["polymerization techniques", "chemical reactions", "step-growth polymerization", "free radical polymerization", "step-growth polymerization", "synthesis of polyesters", "polyesters", "polyamide", "epoxies", "synthesis of polyacrylics", "polyacrylics", "free radical polymerization", "polymerization reactions", "solution", "bulk", "dispersed media", "heterophase polymerizations", "emulsion", "dispersion", "miniemulsion polymerizations", "removal of the resulting product", "reactor", "bulk polymerization", "reaction medium", "polymerization", "solution", "reaction", "dilution", "reactants", "organic solvents", "heterophase polymerizations", "reactants", "droplets", "dilution effect", "water", "surfactant molecules", "stabilization of the monomer droplet", "monomer droplet", "polymer particles", "water"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "polymerization techniques", "chemical reactions", "step-growth polymerization", "free radical polymerization", "polyesters", "polyamide", "epoxies", "polyacrylics", "bulk polymerization", "solution polymerization", "dispersed media", "heterophase polymerizations", "emulsion polymerization", "dispersion polymerization", "miniemulsion polymerization", "viscosity", "reaction medium", "reaction rates", "dilution", "organic solvents", "environmental impact", "droplets", "surfactant molecules", "monomer droplet", "polymer particles", "water phase"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Note that the quantitative introduction of a reactive functionality into the polymer chain end can be easily achieved by adopting the living ROMP technique especially using the Schrock type molybdenum alkylidene initiator [7,12,21,61–65]. The exclusive preparation of end-functionalized ring-opened polymers (realized by a living polymerization with quantitative initiation) can be applied not only to prepare block copolymers (ABCs) coupled with another living polymerization techniques [66], but also for preparation of macromonomers, as described below. In contrast, the initiation efficiency is not always perfect as seen in the molybdenum alkylidene initiators, because dissociation of ligand (PR3 etc.) should be required to generate the catalytically active species in the ROMP with the ruthenium carbene catalysts (Scheme 2) [67–69]. An equilibrium between coordination and dissociation of PR3 should be present even in the propagation process, and replacement of halogen with the other anionic ligand (and/or replacement of PR3 with the other neutral donor ligands/substrates) can also be considered as the probable side reactions. Importance of using the molybdenum catalysts should be thus emphasized for their precise preparations, although the initiators are highly sensitive to moisture and both monomers and solvent have to be thus strictly purified to avoid the catalyst decomposition (deactivation).", "ground_truth": ["quantitative introduction of a reactive functionality into the polymer chain end", "polymer chain end", "living romp technique", "schrock type molybdenum alkylidene initiator", "preparation of end-functionalized ring-opened polymers", "ring-opened polymers", "living polymerization with quantitative initiation", "block copolymers", "abcs", "living polymerization techniques", "preparation of macromonomers", "macromonomers", "molybdenum alkylidene initiators", "dissociation", "ligand", "pr3", "generate the catalytically active species in the romp", "romp", "ruthenium carbene catalysts", "coordination", "dissociation", "pr3", "propagation process", "halogen", "anionic ligand", "pr3", "neutral donor ligands/substrates", "molybdenum catalysts", "monomers", "solvent", "catalyst", "decomposition", "deactivation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "reactive functionality", "polymer chain end", "living romp technique", "schrock type molybdenum alkylidene initiator", "end-functionalized ring-opened polymers", "living polymerization", "quantitative initiation", "block copolymers (abcs)", "macromonomers", "initiation efficiency", "molybdenum alkylidene initiators", "ligand dissociation (pr₃)", "catalytically active species"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "An application of ROMP derived random copolymers is the covalent incorporation of optical sensor moieties into a polymer matrix. ROM polymers have been tested as matrix materials for the oxygen sensing phosphorescent complex platinum tetrakis(pentafluorophenyl)porphyrin. A correlation between the nature of the ROM polymer’s side chain and the optical response of the sensor molecules has been established [34]. Several works are dedicated to the synthesis of ROMP-able optical sensor molecules such as phenantroimidazoles [35,36], europium complexes [37] or xanthene dyes [38], their random copolymerization and the evaluation of their sensing profiles in the copolymers. Another application comprises random copolymers with covalently bound eosin and/or ethyl dimethylamino benzoate units which were tested as macroinitiators for the photopolymerization of acrylates aiming at an initiator/coinitiator system which combines good polymerization activity with improved migration stability [39].", "ground_truth": ["romp derived random copolymers", "covalent incorporation of optical sensor moieties into a polymer matrix", "optical sensor moieties", "polymer matrix", "rom polymers", "matrix materials", "phosphorescent complex platinum tetrakis(pentafluorophenyl)porphyrin", "oxygen", "rom polymer’s side chain", "sensor molecules", "synthesis of romp-able optical sensor molecules", "romp-able optical sensor molecules", "phenantroimidazoles", "europium complexes", "xanthene dyes", "copolymerization", "evaluation of their sensing profiles in the copolymers", "copolymers", "random copolymers with covalently bound eosin and/or ethyl dimethylamino benzoate units", "copolymers", "covalently bound eosin", "ethyl dimethylamino benzoate units", "macroinitiators", "acrylates", "initiator/coinitiator system", "polymerization"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "romp (ring-opening metathesis polymerization)", "random copolymers", "optical sensor moieties", "polymer matrix", "oxygen sensing", "phosphorescent complex", "platinum tetrakis(pentafluorophenyl)porphyrin", "side chain", "optical response", "sensor molecules", "phenantroimidazoles", "europium complexes", "xanthene dyes", "random copolymerization", "sensing profiles", "eosin", "ethyl dimethylamino benzoate", "macroinitiators", "photopolymerization"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Another choice was to graft the fluorinated groups on the copolymers with functional groups. Casazza et al. [52] synthesized an acrylic terpolymer with pendent perfluoroether segments via grafting fluorinated groups into a poly(butyl methacrylate-co-hydroxyehtyl acrylate-co-ethyl acrylate) random copolymer through hexamethylene diisocyanate functionality. Malshe et al. [53,54] studied the coating properties of fluorinated acrylic copolymers based on MMA, BA, and 2-hydroxyethyl methacrylate (HEMA). They partially esterified hydroxyl functionality of HEMA with tetrafluoro propanoic acid and cured the polymer with butylated melamine formaldehyde resin. Such methods were suited for the synthesis of copolymers containing complicated fluorinated groups or difficult to be provided directly by living polymerization.", "ground_truth": ["graft the fluorinated groups on the copolymers with functional groups", "another choice", "an acrylic terpolymer with pendent perfluoroether segments", "the coating properties of fluorinated acrylic copolymers", "mma", "ba", "2-hydroxyethyl methacrylate", "copolymers containing complicated fluorinated groups", "hydroxyl functionality of hema with tetrafluoro propanoic acid", "the polymer with butylated melamine formaldehyde resin", "grafting fluorinated groups", "poly", "butyl methacrylate-co-hydroxyehtyl acrylate-co-ethyl acrylate", "copolymer", "hexamethylene diisocyanate functionality", "hema", "living polymerization", "fluorinated acrylic copolymers"], "prediction_list": ["- fluorinated groups", "copolymers", "functional groups", "casazza et al.", "acrylic terpolymer", "pendent perfluoroether segments", "grafting", "poly(butyl methacrylate-co-hydroxyehtyl acrylate-co-ethyl acrylate)", "random copolymer", "hexamethylene diisocyanate functionality", "malshe et al.", "coating properties", "fluorinated acrylic copolymers", "mma", "ba", "2-hydroxyethyl methacrylate (hema)", "esterified hydroxyl functionality", "tetrafluoro propanoic acid", "cured", "but"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Assuming a constant cell electrical conversion efficiency of 15%, a constant fraction of the incident solar radiation would be dissipated by the solar cell for each solar radiation intensity level. From Table 2, it can be seen that for the worst scenario when the ambient temperature was 50°C with natural convection only, the predicted cell electrical conversion efficiency would have reduced to approximately 8% rather than the 15% assumed. The energy dissipated as heat from the cell would thus be 7% higher. To correct for this effect the apparent insolation level should be modified using the following formula:(3)Iact=I×0.850.85+dηwhere Iact is the actual incident solar radiation intensity, dη is solar cell efficiency difference between the initially assumed 15% and final calculated cell efficiency based on measured cell temperature.", "ground_truth": ["solar radiation", "natural convection", "predicted cell electrical conversion efficiency", "apparent insolation level", "(3)iact=i×0.850.85+dη", "solar radiation", "cell electrical conversion", "solar radiation"], "prediction_list": ["keywords:", "cell electrical conversion efficiency", "incident solar radiation", "solar radiation intensity", "ambient temperature", "natural convection", "predicted cell efficiency", "energy dissipated as heat", "apparent insolation level", "actual incident solar radiation intensity", "solar cell efficiency difference", "measured cell temperature", "formula correction"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Thermal performance of the smart window has been predicted under different simulated parameters namely, direct solar radiation intensity, ambient temperature, water inlet temperature, and water flow rate. Fig. 11 shows a sample of temperature distribution of all window components at a plane passing through the horizontal window segment based on simulation physical conditions listed on Table 3. Simulation data has been collected from all successive simulations. The effect of increasing direct solar radiation on both solar cells and water temperatures is shown in Fig. 12. Three different simulations were performed assuming direct solar radiation intensities of 400, 600, and 800W/m2 incident on the window’s front pane with set ambient temperature, water inlet temperature and water flow rate of respectively 273K, 283K and 0.01kg/s. Water temperature was found to increase by 5°C as it passed through the tube, carrying the solar cells, from left to right for the bottom most units at 800W/m2 of direct incident solar radiation.", "ground_truth": ["thermal performance of the smart window", "simulated parameters", "direct solar radiation intensity", "ambient temperature", "water inlet temperature", "water flow rate", "window components", "simulation physical conditions", "simulation data", "simulations", "simulations", "increasing direct solar radiation on both solar cells and water temperatures", "solar radiation", "window’s front pane", "tube", "solar cells", "solar radiation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "thermal performance", "smart window", "simulated parameters", "direct solar radiation intensity", "ambient temperature", "water inlet temperature", "water flow rate", "temperature distribution", "window components", "horizontal window segment", "simulation physical conditions", "simulation data", "solar cells", "water temperatures", "different simulations", "radiation intensities (400", "600", "800 w/m"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Shading can be the most detrimental factor on performance for a domestic system. The impact of shading on performance varies depending on the electrical series and parallel arrangement of cells within a module and modules within an installed array. Whilst many approaches to shading analysis have been proposed, computational efficiency is not reported despite being of high importance when incorporating shading algorithms into an overall energy yield model. The lack of consideration of the non-linear impacts of shading on smaller systems for example means that the shading loss is significantly underestimated, especially from supposedly small obstacles such as antennas or chimneys. As an example, the system shown in Fig. 1 illustrates the case where the installer may have attested a shade loss factor close to unity under UK microgeneration guidelines (Microgeneration Certification Scheme, 2013), i.e. negligible, but the performance of the system is severely compromised due to the non-linear cell mismatch effects. An effective shading sub-model therefore needs to give feedback to inform decisions of array layout in the proximity of obstructions but must not rely on high power computing.", "ground_truth": ["effective shading sub-model", "system shown in fig. 1", "shading algorithms", "shading analysis", "shading", "domestic system", "shading", "cells", "module", "modules", "installed array", "energy yield model", "shading", "shading", "antennas", "chimneys", "shade loss", "non-linear cell mismatch effects", "give feedback to inform decisions of array layout", "high power computing", "system"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "shading", "detrimental factor", "performance", "domestic system", "electrical series", "parallel arrangement", "cells", "module", "installed array", "shading analysis", "computational efficiency", "shading algorithms", "energy yield model", "non-linear impacts", "smaller systems", "shading loss", "underestimated", "obstacles (antennas", "chimneys)", "shade loss factor", "uk microgeneration guidelines", "microgeneration certification scheme", "negligible", "cell mismatch effects", "sh"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Progressive photon mapping was first proposed by Hachisuka et al. (2008) as an iterative extension of the standard static photon mapping approach as implemented in the Radiance extension. It combines multiple smaller photon maps to approximate a much larger one which may not fit into memory using the traditional approach. Through iteration, the process mitigates the noise inherent in Monte Carlo raytracing by combining successive results and averaging them. At the same time, the density estimate bandwidth1Bandwidth describes the support, or area of influence, of a filter used to weight the photons retrieved from the photon map during a nearest neighbour lookup on a surface (Jensen, 2001). The resulting irradiance is proportional to the photon density, and the bandwidth is defined by the distance (radius) to the furthest photon found. In this paper, we generalise the term to describe either the radius or the number of nearest neighbours for a density estimate, depending on the implementation.1 (radius or number of nearest photons) is gradually reduced to mitigate bias. As Hachisuka points out, the accumulated density estimates converge to an unbiased solution in the limit.", "ground_truth": ["progressive photon mapping", "iterative extension of the standard static photon mapping", "radiance extension", "monte carlo raytracing", "mitigates the noise", "combining successive results and averaging", "density estimate bandwidth1bandwidth", "support", "or area of influence", "of a filter", "describe either the radius or the number of nearest neighbours for a density estimate", "mitigate bias", "photon maps", "photon", "photon", "traditional approach", "weight the photons", "photons", "photon map", "nearest neighbour lookup", "surface", "photons", "irradiance", "photon", "photon"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "progressive photon mapping", "hachisuka et al. (2008)", "iterative extension", "standard static photon mapping", "radiance extension", "multiple smaller photon maps", "memory constraints", "traditional approach", "noise mitigation", "monte carlo raytracing", "successive results", "averaging", "density estimate bandwidth", "bandwidth", "support", "area of influence", "filter", "photon retrieval", "nearest neighbour lookup", "surface", "jensen (2001)", "irradiance", "photon density"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "For the reverse current analysis, for both scenarios (shading and short circuits) were tested on two systems, one system using standard silicon modules and another system using high efficiency modules. For the standard silicon system, a power of 50kWp was considered, with a system composed by 10 strings of 24 modules per string and an approximate system Voc of 864 [VDC]. For the high efficiency system, a power of 40kWp was considered, with a system composed by 10 strings of 18 modules per string and an approximate system Voc of 873 [VDC]. Fig. 5(a) shows the reverse current present in one string when different numbers of modules in the string are shaded by 90%. Fig. 5(b) shows the reverse current present in one string when different numbers of modules of the string are short-circuited. For both figures the continuous lines are for the standard silicon system and the dashed lines are for the high efficiency system.", "ground_truth": ["standard silicon modules", "high efficiency modules", "reverse current analysis", "system", "10 strings of 18 modules per string", "voc of 873", "vdc", "system", "high efficiency system", "vdc", "voc of 864", "10 strings of 24 modules per string", "standard silicon system", "reverse current", "different numbers of modules of the string are short-circuited", "standard silicon system", "high efficiency system"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "reverse current analysis", "shading", "short circuits", "standard silicon modules", "high efficiency modules", "power", "50kwp", "10 strings", "24 modules per string", "system voc", "864 [vdc]", "40kwp", "18 modules per string", "873 [vdc]", "fig. 5(a)", "modules shaded by 90%", "fig. 5(b)", "short-circuited modules", "continuous lines", "dashed lines", "standard"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The final contribution to the force is the van der Waals interaction. It includes the following contributions: (i) between the macroscopic Si tip of conical shape with the sphere of radius R at the end [27] and semi-infinite substrate; (ii) the dispersion forces between the atoms in the sample treated atomistically; and (iii) the interaction between the macroscopic part of the tip and the sample atoms. The first contribution is calculated analytically [27]. In fact, the macroscopic contribution to the van der Waals force is the same in each of the three systems described below, as it depends only on the tip–surface separation, macroscopic sphere radius, cone-angle and Hamaker constant of the system [27]. All these quantities are identical in each system we look at, so that the van der Waals force acts as a background attractive force independent of the microscopic properties of the system [8]. The Hamaker constant needed for the calculation of the macroscopic van der Waals force is estimated to be 0.5eV [32].", "ground_truth": ["van der waals interaction", "macroscopic si tip", "semi-infinite substrate", "dispersion forces", "atoms", "sample atoms", "macroscopic part of the tip", "van der waals force", "tip–surface separation", "van der waals force", "calculation", "macroscopic van der waals force"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "van der waals interaction", "force contributions", "macroscopic si tip", "conical shape", "sphere radius r", "semi-infinite substrate", "dispersion forces", "atomistically treated sample", "macroscopic tip-sample interaction", "analytical calculation", "tip–surface separation", "macroscopic sphere radius", "cone-angle", "hamaker constant", "background attractive force", "microscopic properties", "hamaker constant estimation", "0.5ev", "these keywords capture the main concepts", "interactions", "and parameters discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy–momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy–momentum scheme is illustrated in representative numerical examples.In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy–momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy–momentum scheme is illustrated in representative numerical examples.", "ground_truth": ["mortar finite element", "coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity", "mortar method", "isogeometric analysis", "coordinate augmentation technique", "mortar method", "mortar finite element method", "coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity", "isogeometric analysis", "framework of nurbs based shape functions", "coordinate augmentation technique", "energy–momentum scheme", "mortar method", "framework of nurbs based shape functions", "mortar method", "energy–momentum scheme", "energy–momentum scheme", "optimal convergence rates", "convergence rates", "energy–momentum scheme", "mechanical system", "constrained mechanical system"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "mortar finite element method", "coupling", "nonconforming discretized sub-domains", "nonlinear elasticity", "optimal convergence rates", "variationally consistent", "isogeometric analysis", "nurbs-based shape functions", "coordinate augmentation technique", "energy–momentum scheme", "constrained mechanical system", "numerical examples", "these keywords capture the main concepts and techniques discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this work, a numerical strategy for designing an optimal maintenance scheduling for a structure, accounting explicitly for the effects of uncertainty is suggested. This contribution, which can be regarded as an extension of the methods developed in [23], presents several novel aspects over similar approaches proposed in the literature. Firstly, the initiation and propagation of fatigue crack is modeled efficiently by means of cohesive zone elements [24–26]. The application of this class of elements allows modeling the crack initiation and propagation within a unified framework. It should be noted that cohesive zone elements have already been used for uncertainty quantification of the crack propagation phenomenon [27,28]. However its application within the context of maintenance scheduling constitutes a novelty. The second innovative aspect of this contribution refers to the assessment of the reliability sensitivity with respect to the variables that define the maintenance scheduling. The estimation of this sensitivity, which is required in order to determine the optimal maintenance schedule within the proposed framework, can be quite demanding as the model characterizing repair of a cracked structure leads to a discontinuous performance function associated with the failure probability. A new approach for modeling this function is proposed herein. The continuous and discontinuous parts respectively of the function are considered separately to estimate accurately the gradients of the failure events.", "ground_truth": ["cohesive zone elements", "uncertainty quantification of the crack propagation phenomenon", "maintenance scheduling", "assessment of the reliability sensitivity with respect to the variables that define the maintenance scheduling", "estimation of this sensitivity", "optimal maintenance schedule within the proposed framework", "continuous and discontinuous parts", "optimal maintenance scheduling for a structure", "numerical strategy", "estimate accurately the gradients of the failure events", "initiation and propagation of fatigue crack", "cohesive zone elements", "class of elements", "modeling the crack initiation and propagation within a unified framework", "accounting explicitly for the effects of uncertainty", "model characterizing repair of a cracked structure", "discontinuous performance function associated with the failure probability", "unified framework"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "numerical strategy", "optimal maintenance scheduling", "structure", "uncertainty", "extension", "methods", "novel aspects", "initiation", "propagation", "fatigue crack", "cohesive zone elements", "unified framework", "uncertainty quantification", "crack propagation phenomenon", "maintenance scheduling", "reliability sensitivity", "variables", "optimal maintenance schedule", "discontinuous performance function", "failure probability", "new approach", "continuous parts", "discontinuous parts", "gradients", "failure events"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Finite Element Tearing and Interconnecting (FETI) methods are a powerful approach to designing solvers for large-scale problems in computational mechanics. The numerical simulation problem is subdivided into a number of independent sub-problems, which are then coupled in appropriate ways. NURBS- (Non-Uniform Rational B-spline) based isogeometric analysis (IGA) applied to complex geometries requires to represent the computational domain as a collection of several NURBS geometries. Since there is a natural decomposition of the computational domain into several subdomains, NURBS-based IGA is particularly well suited for using FETI methods.This paper proposes the new IsogEometric Tearing and Interconnecting (IETI) method, which combines the advanced solver design of FETI with the exact geometry representation of IGA. We describe the IETI framework for two classes of simple model problems (Poisson and linearized elasticity) and discuss the coupling of the subdomains along interfaces (both for matching interfaces and for interfaces with T-joints, i.e. hanging nodes). Special attention is paid to the construction of a suitable preconditioner for the iterative linear solver used for the interface problem. We report several computational experiments to demonstrate the performance of the proposed IETI method.", "ground_truth": ["finite element tearing and interconnecting", "solvers for large-scale problems in computational mechanics", "numerical simulation problem", "number of independent sub-problems", "complex geometries", "isogeometric analysis", "computational domain as a collection of several nurbs geometries", "nurbs-based iga", "feti methods", "isogeometric tearing and interconnecting", "feti", "iga", "ieti framework", "coupling of the subdomains along interfaces", "interfaces with t-joints", "demonstrate the performance of the proposed ieti method", "several subdomains", "construction of a suitable preconditioner for the iterative linear solver", "simple model problems", "poisson", "linearized elasticity", "ieti", "iga", "feti", "hanging nodes", "non-uniform rational b-spline", "nurbs", "nurbs", "advanced solver design", "exact geometry representation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "finite element tearing and interconnecting (feti)", "solvers", "large-scale problems", "computational mechanics", "numerical simulation", "sub-problems", "nurbs (non-uniform rational b-spline)", "isogeometric analysis (iga)", "complex geometries", "computational domain", "subdomains", "isogeometric tearing and interconnecting (ieti)", "exact"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "To the best of authors’ knowledge, so far there are only very few papers [12,13,16,29] which address the performance of linear algebra solvers. In Ref. [16], the authors study the performance of direct solvers which are clearly not suitable for large problems, specially in three-dimensions. In Ref. [29], the tearing and interconnecting approach of finite element methods is used in the context of isogeometric analysis, and the numerical tests (in absence of any theoretical study) suggest almost optimal (with a logarithmic factor) convergence rates of the proposed isogeometric tearing and interconnecting method. The only paper which provides rigorous theoretical study, supported by extensive numerical examples, is by Beirao et al. [12] where the authors discuss the overlapping Schwarz methods. The same authors have also proposed BDDC preconditioners for isogeometric analysis in [13].", "ground_truth": ["performance of linear algebra solvers", "performance of direct solvers", "large problems", "tearing and interconnecting approach of finite element methods", "isogeometric analysis", "isogeometric tearing and interconnecting method", "extensive numerical examples", "schwarz methods", "bddc preconditioners", "isogeometric analysis", "numerical tests", "rigorous theoretical study", "theoretical study"], "prediction_list": ["keywords:", "authors’ knowledge", "few papers", "linear algebra solvers", "direct solvers", "large problems", "three-dimensions", "tearing and interconnecting approach", "finite element methods", "isogeometric analysis", "numerical tests", "theoretical study", "optimal convergence rates", "isogeometric tearing and interconnecting method", "rigorous theoretical study", "numerical examples", "beirao et al.", "overlapping schwarz methods", "bddc preconditioners"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "An attempt of a quite comprehensive answer to this question is made hereafter, within the following structure of the remaining paper: first, we introduce the mathematical systems biology of bone, starting from the work of Pivonka et al. [25,26], and extending it to mechanoregulatory feedback control (Section 2). Then, we introduce a continuum micromechanics representation adopted from Hellmich et al. [30], in order to scale elasticity and strains from the level of the extravascular bone matrix to that of cortical bone1In this paper, we restrict ourselves to cortical bone, due to its major importance in providing sufficient load-carrying capacity. However, extension of the coupled approach proposed here to trabecular bone is straightforward; it merely requires recalibration of underlying parameters.1 and vice versa (Section 3). The micromechanics formulation is fed with composition quantities derived from the systems biology approach, which, in turn, is provided with mechanical stimuli gained from the micromechanics model. We then apply the coupled approach to biochemical and mechanical conditions typical for postmenopausal osteoporosis (Section 4) and microgravity exposure (Section 5), and discuss key sensitivity features (Section 6). After emphasizing the potentials and limitations of the presented approach (Section 7), we conclude the paper in (Section 8).", "ground_truth": ["mechanoregulatory feedback control", "scale elasticity and strains from the level of the extravascular bone matrix to that of cortical bone", "mathematical systems biology of bone", "continuum micromechanics representation", "cortical bone", "providing sufficient load-carrying capacity", "coupled approach", "trabecular bone", "recalibration of underlying parameters", "systems biology approach", "mechanical stimuli", "micromechanics model", "coupled approach", "biochemical and mechanical conditions", "potentials and limitations of the presented approach", "micromechanics formulation", "composition quantities"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "mathematical systems biology", "bone", "pivonka et al.", "mechanoregulatory feedback control", "continuum micromechanics", "hellmich et al.", "elasticity", "strains", "extravascular bone matrix", "cortical bone", "load-carrying capacity", "trabecular bone", "coupled approach", "composition quantities", "me"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The choice of the interpolation functions and support point coordinates for the gradient field is crucial to ensure stability and accuracy of the formulation. For example, nodal integration and NS-FEM are unstable involving the appearance of spurious low-energy modes. They need non-physical penalty energy functions that stabilize them. The articles [2,28] numerically verify the stability, convergence and accuracy of several W2 variants including new elements which can be constructed based on the idea of assumed continuous deformation gradients. For first order hexahedral elements, [2,28] found good results for the element types C3D_8N_27C and C3D_8N_8I. The first is defined by 27 support points and a second order tensor-product interpolation of the deformation gradient by Lagrange polynomials. The latter element type is defined by 16 support points with 8 points being coincident with the nodes and 8 additional points in the element interior. Among the tested first order tetrahedra, the nodally integrated tetrahedron with an additional bubble mode in the gradients was found to be most accurate. It turned out to be even the most efficient with respect to computing time in explicit analysis [28] because the enlarged critical time step compensates the slightly increased numerical cost per restoring force assembly. Fig. 1 illustrates the positions of support points for various CAG and SFEM formulations.", "ground_truth": ["interpolation functions", "stability and accuracy of the formulation", "support point coordinates for the gradient field", "non-physical penalty energy functions", "nodal integration and ns-fem", "first order hexahedral elements", "c3d_8n_27c", "first order tetrahedra", "computing time in explicit analysis", "cag and sfem formulations", "nodally integrated tetrahedron", "stabilize them", "c3d_8n_8i", "second order tensor-product interpolation", "w2 variants", "numerically verify the stability", "convergence and accuracy of several w2 variants", "continuous deformation gradients", "element types"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "interpolation functions", "support point coordinates", "gradient field", "stability", "accuracy", "nodal integration", "ns-fem", "spurious low-energy modes", "non-physical penalty energy functions", "stability", "convergence", "accuracy", "w2 variants", "assumed continuous deformation gradients", "first order hexahedral elements", "c3d_8n_27c", "c3d_8n_8i", "27 support points", "second order tensor-product interpolation", "deformation gradient", "lagrange polynomials", "16"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Algorithms regarding distance fields go back to the level set equation. The level set method was presented by Osher and Sethian [20] who described the temporal propagation of moving interfaces by numerical methods solving the Hamilton–Jacobi equation. This is performed by a finite difference scheme working on a rectangular grid in two or three dimensions. Information on normal vectors and curvature can be obtained. The fast marching method [21] provides an efficient numerical scheme of complexity nlogn to compute the support values on the grid. It is a reinterpretation of the propagation process, i.e. the time where the interface passes a certain grid point is influenced only by those neighboring grid points which are previously passed by the interface. An overview on the theory of level set and fast marching methods and their applications to problems of various areas are given in [22,23], for example shape offsetting, computing distances, photolithography development, seismic travel times, etc. Distance fields are a special case of the level set equation where the absolute value of the advection velocity is 1.", "ground_truth": ["level set method", "level set equation", "hamilton–jacobi equation", "finite difference scheme", "rectangular grid in two or three dimensions", "fast marching method", "numerical scheme of complexity nlogn to compute the support values on the grid", "propagation process", "the time where the interface passes a certain grid point is influenced only by those neighboring grid points which are previously passed by the interface", "theory of level set", "fast marching", "distance fields", "level set", "algorithms regarding distance fields", "temporal propagation", "numerical methods", "complexity nlogn", "shape offsetting", "computing distances", "photolithography development", "seismic travel times", "areas"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "algorithms", "distance fields", "level set equation", "level set method", "osher and sethian", "temporal propagation", "moving interfaces", "numerical methods", "hamilton–jacobi equation", "finite difference scheme", "rectangular grid", "two dimensions", "three dimensions", "normal vectors", "curvature", "fast marching method", "numerical scheme", "complexity (nlogn)", "support values", "grid", "propagation process", "interface", "neighboring grid points", "theory", "applications", "shape offsetting"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this article we consider an extension to the equations of poroelasticity by modelling the flow of a slightly compressible single phase fluid in a viscoelastic porous medium. The constitutive equations therefore allow for the presence of viscoelastic relaxation effects in the porous media (but not the fluid). Fully discrete numerical schemes are derived based on a lagged and non-lagged backward Euler time stepping method applied to a mixed and Galerkin finite element spatial discretization. We show that the lagged scheme is unconditionally stable and give an optimal a priori error bound for it. Furthermore, this scheme is practical and useful in the sense that it can be easily implemented in existing poroelasticity software because the coupling between the viscous stresses and pressures and the elasticity and flow equations is ‘lagged’ by one time step. The required additional coding therefore takes the form of extra ‘right hand side loads’ together with some updating subroutines for the viscoelastic internal variables, but the solver and assembly engines remain intact. This idea of lagging has been used before for nonlinearly viscoelastic diffusion problems in [3,24] but, of course, is not new. Lagging in numerical schemes is discussed more widely by Lowrie in [14].", "ground_truth": ["modelling the flow of a slightly compressible single phase fluid in a viscoelastic porous medium", "extension to the equations of poroelasticity", "mixed and galerkin finite element spatial discretization", "lagged and non-lagged backward euler time stepping method", "fully discrete numerical schemes", "coupling", "nonlinearly viscoelastic diffusion problems", "constitutive equations", "viscoelastic relaxation effects", "updating subroutines", "porous media", "fluid", "single phase fluid", "viscoelastic porous medium", "flow equations", "assembly engines", "numerical schemes"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "poroelasticity", "slightly compressible fluid", "single phase fluid", "viscoelastic porous medium", "constitutive equations", "viscoelastic relaxation effects", "porous media", "fully discrete numerical schemes", "backward euler time stepping method", "mixed finite element", "galerkin finite element", "spatial discretization", "unconditionally stable", "a priori error bound"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Traditionally, the simulation of incompressible fluid flow by the SPH method has been through a weakly compressible SPH formulation (WCSPH). In this approach, the pressure is treated as a thermodynamic variable and is calculated using an artificial equation of state. The sound speed is set to be sufficiently high to limit density variations to within a small fraction of the actual fluid density. In practice, this high sound speed places a limitation on the maximum permissible time-step size through the Courant–Friedrichs–Lewy (CFL) constraint. A particular weakness relates to noise in the pressure field since a small perturbation in the local density will yield a large variation in the local pressure. This can make WCSPH formulations ineffective for accurate force and pressure prediction, although recent developments which create more uniform particle distributions have improved this [1,2]. A review of the SPH method can be found in [3] while a review of the classical WCSPH formulation applied to free-surface flows can be found in [4].", "ground_truth": ["simulation of incompressible fluid flow", "sph method", "weakly compressible sph formulation", "wcsph", "artificial equation of state", "wcsph", "sph method", "wcsph", "fluid flow", "fluid", "fluid", "create more uniform particle distributions", "particle", "free-surface flows"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "simulation", "incompressible fluid flow", "sph method", "weakly compressible sph (wcsph)", "thermodynamic variable", "artificial equation of state", "sound speed", "density variations", "fluid density", "time-step size", "courant–friedrichs–lewy (cfl) constraint", "pressure field", "noise", "perturbation", "local density", "local pressure", "force prediction", "pressure prediction", "uniform particle distributions", "free-surface flows", "review", "references:"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, however, we prefer the simpler ‘framed’ cell employed by Hadjiconstantinou and Patera [16], where the shear stress is generated by constraining the velocity in a ‘frame’ rather than by modifying the shape of the box. The framed cell is periodic, but we cannot simply calculate the average stress in the whole box because the presence of an external buffer would produce spurious results. We need the local stress in the core region, but this complicates the Oij term in Eq. (3). There are other methods to calculate the stress tensor such as the method of planes [32], the volume-average approach [26,14], or the method derived from the Schweitz virial relation [25], but, in general, we must choose between a complicated computational cell (i.e. Lees–Edwards cell) and simplifying the calculation of the momentum flux, or a simple cell (i.e. framed cell) and complicating the calculation of the momentum flux. The new method we propose here does not need the direct calculation of the flux, so it avoids this issue altogether: we can use the framed cell and, at the same time, avoid the calculation of the IK equation.", "ground_truth": ["calculate the stress tensor", "framed’ cell", "shear stress", "constraining the velocity", "framed cell", "framed cell", "method of planes", "calculate the stress tensor", "volume-average approach", "schweitz virial relation", "complicated computational cell", "lees–edwards cell", "simple cell", "framed cell", "ik equation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "framed cell", "shear stress", "velocity", "periodic", "average stress", "external buffer", "spurious results", "local stress", "core region", "oij term", "stress tensor", "method of planes", "volume-average approach", "schweitz virial relation", "computational cell", "lees–edwards cell", "momentum flux"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Powder metallurgy is a versatile technology for the manufacturing of components to (near) net-shape with high product quality. For a hardmetal (such as WC-Co) cold compaction of the powder to a “green body” is followed by liquid-phase sintering from the subsequent heating. This means that the binder metal Co is heated to melt in order to obtain sufficient mobility via capillary action, i.e., via surface traction, stemming from stored surface energy. The resulting flow causes gradual filling of the pore space and brings about a macroscopic shrinkage of the particle compact until a completely dense state is obtained, at least ideally. To model and quantitatively simulate the sintering process is a challenging task. The goal is to (i) estimate the final resulting quality (i.e., in terms of porosity) and (ii) to predict the final net shape and size of the sintered component.", "ground_truth": ["model and quantitatively simulate the sintering process", "estimate the final resulting quality", "to predict the final net shape and size", "porosity", "powder metallurgy", "sintered component", "hardmetal", "wc-co", "powder", "liquid-phase sintering", "green body", "co is heated", "gradual filling of the pore", "macroscopic shrinkage", "dense state", "technology for the manufacturing of components to (near) net-shape", "heating", "obtain sufficient mobility via capillary action", "surface traction", "particle", "sintering process", "co"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "powder metallurgy", "manufacturing", "components", "near net-shape", "net-shape", "high product quality", "hardmetal", "wc-co", "cold compaction", "green body", "liquid-phase sintering", "binder metal", "cobalt (co)", "melting", "mobility", "capillary action", "surface traction", "surface energy", "pore space filling", "macroscopic shrinkage", "dense state", "modeling", "quantitative simulation", "sintering process", "final quality", "porosity"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "As mentioned previously, the weakly penalized system can be thought of as a generalized formulation which can result in the PL, penalty or statically condensed PL formulations depending on the choice of the projection operator. The equivalence of these methods under the weakly penalized regime, allows us to combine and take advantage of the good characteristics of each method. For instance, the weakly penalized formulation combines the simplified structure of the penalty method with the convergence characteristics of the PL formulation. However, due to the stiffness of the linear system at high values of the bulk modulus, the penalized formulations (classic penalty/weakly penalized) exhibit deteriorated nonlinear convergence. This stands in stark contrast to the PL method which (for inf–sup stable schemes) exhibits fast convergence even for high bulk modulus. However, we observe that, when the choice of πh provides equivalence with the discrete PL method, poor nonlinear convergence is observed though, in principle, the convergence should be similar. Examining the update formulae for both weakly penalized and PL approaches (see Appendix C), we observe that deteriorated convergence stems from: (1) initial residual amplification, and (2) the amplification of the residual.", "ground_truth": ["deteriorated convergence", "initial residual amplification", "amplification of the residual", "poor nonlinear convergence", "pl", "pl", "inf–sup stable schemes", "penalized formulations", "classic penalty/weakly penalized", "pl formulation", "weakly penalized formulation", "penalty method", "weakly penalized", "generalized formulation", "pl", "penalty or statically condensed pl formulations", "nonlinear convergence", "convergence", "weakly penalized and pl approaches"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "weakly penalized system", "generalized formulation", "pl (possibly penalty-lagrange or similar)", "penalty method", "statically condensed pl formulations", "projection operator", "equivalence", "weakly penalized regime", "convergence characteristics", "simplified structure", "bulk modulus", "penalized formulations", "classic penalty", "nonlinear convergence", "inf–sup stable schemes", "fast convergence", "discrete pl method", "update formulae", "initial residual amplification", "residual amplification", "these keywords capture the main concepts and technical terms discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Energy conservation is critical to ensure stability of a numerical method, especially for contact and collision problems  [28,43]. A number of conserving schemes have been developed to ensure energy conservation. These schemes make use of the penalty regulation of normal contact constraint and inherit the conservation property from continuum problems. These conservation schemes can conveniently be combined with the finite element method to simulate frictionless  [44] and frictional  [43] contact and collision. Hesch and Betsch  [45] formulated the node-to-segment contact method and solved large deformation contact problems with the conserving scheme. More recently, an energy and momentum-conserving temporal discretization scheme  [46] was developed for adhesive contact problems without considering friction and dissipation. Even though the conserving scheme improves numerical stability, it also inherits from the penalty method the difficulty of having to determine penalty parameters. In order to remove penalty sensitivity, Chawla and Laursen  [47] proposed an energy and momentum conserving algorithm, which makes use of Lagrange multipliers instead of penalty parameters.", "ground_truth": ["energy conservation", "contact and collision problems", "energy conservation", "penalty regulation", "continuum problems", "conservation schemes", "finite element method", "simulate frictionless  [44] and frictional  [43] contact and collision", "node-to-segment contact method", "large deformation contact problems", "conserving scheme", "energy and momentum-conserving temporal discretization scheme", "adhesive contact problems", "friction", "dissipation", "conserving scheme", "energy and momentum conserving algorithm"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "energy conservation", "numerical method", "stability", "contact problems", "collision problems", "conserving schemes", "penalty regulation", "normal contact constraint", "conservation property", "continuum problems", "finite element method", "frictionless contact", "frictional contact", "node-to-segment contact method", "large deformation", "energy and momentum-conserving scheme"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this article we propose a method which adopts a different approach to the generation procedure outlined above and that helps to address the problem of generating high-order meshes for high Reynolds number flows. The method is conceptually simple, cheap to implement and does not require a dense linear boundary-layer mesh. It is based on the use of an isoparametric [17] or, in general, a transfinite interpolation [18] where a high-order coarse boundary-layer prismatic mesh is subdivided into either prisms or tetrahedra using the mapping that defines the coarse high-order prisms. The procedure is also very versatile as it permits meshes with different distributions of y+ to be generated with ease and furthermore, the validity of these meshes is guaranteed if the initial mesh is valid and the polynomial space is chosen appropriately.", "ground_truth": ["generating high-order meshes for high reynolds number flows", "generation procedure", "isoparametric [17] or", "in general", "a transfinite interpolation", "subdivided into either prisms or tetrahedra", "initial mesh", "high-order meshes", "dense linear boundary-layer mesh", "high-order coarse boundary-layer prismatic mesh", "prisms", "tetrahedra", "coarse high-order prisms", "reynolds number flows", "meshes", "meshes", "polynomial space"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "method", "generation procedure", "high-order meshes", "high reynolds number flows", "conceptually simple", "cheap to implement", "dense linear boundary-layer mesh", "isoparametric", "transfinite interpolation", "high-order coarse boundary-layer prismatic mesh", "prisms", "tetrahedra", "mapping", "versatile", "meshes", "distributions of y+", "validity", "initial mesh", "polynomial space"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We shall establish the variational format in the space–time domain S=defΩ×I, for given spatial domain Ω and time domain I=(0,T), for a quite broad class of problems involving a first order time-derivative. In particular, the coupled problem of consolidation of geomaterials falls within this class. Another interesting application is the problem of dynamics, rewritten in first-order form, i.e. through a Hamiltonian description. It is of considerable interest to note from the outset that, due to the forward transport of information in time, it is always possible to consider a set of finite time intervals, whereby the solution at the end of any such interval will act as the initial data for the next one. To this end, we introduce a partition 0=t0<t1<⋯<tN=T of the considered time domain I=(0,T) into time-intervals In=(tn−1,tn) of length Δtn=tn−tn−1.11The abbreviated notation Δt=Δtn will be used henceforth for the current time step associated with In. Hence, we define space–time slabs Sn=defΩ×In such that the space–time domain can be given as S=defΩ×I=S1∪S2⋯∪Sn.", "ground_truth": ["variational format in the space–time domain", "spatial domain ω", "first order time-derivative", "problem of dynamics", "rewritten in first-order form", "hamiltonian description", "coupled problem of consolidation of geomaterials", "space–time slabs", "sn=defω×in", "space–time domain", "s=defω×i=s1∪s2⋯∪sn", "time domain i=(0", "t)", "s=defω×i"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "variational format", "space–time domain", "spatial domain", "time domain", "first-order time-derivative", "coupled problem", "consolidation", "geomaterials", "dynamics", "first-order form", "hamiltonian description", "forward transport", "information", "finite time intervals", "initial data", "partition", "time-intervals", "space–time slabs", "current time step"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Isogeometric analysis. The central idea of isogeometric analysis is to use the same ansatz functions for the discretization of the partial differential equation at hand, as are used for the representation of the problem geometry. Usually, the problem geometry Ω is represented in computer aided design (CAD) by means of NURBS or T-splines. This concept, originally invented in  [1] for finite element methods (IGAFEM) has proved very fruitful in applications  [1,2]; see also the monograph  [3]. Since CAD directly provides a parametrization of the boundary ∂Ω, this makes the boundary element method (BEM) the most attractive numerical scheme, if applicable (i.e., provided that the fundamental solution of the differential operator is explicitly known). Isogeometric BEM (IGABEM) has first been considered for 2D BEM in  [4] and for 3D BEM in  [5]. Unlike standard BEM with piecewise polynomials which is well-studied in the literature, cf. the monographs  [6,7] and the references therein, the numerical analysis of IGABEM is essentially open. We only refer to  [2,8–10] for numerical experiments and to  [11] for some quadrature analysis. In particular, a posteriori error estimation has been well-studied for standard BEM, e.g.,  [12–18] as well as the recent overview article  [19], but has not been treated for IGABEM so far. The purpose of the present work is to shed some first light on a posteriori error analysis for IGABEM which provides some mathematical foundation of a corresponding adaptive algorithm.", "ground_truth": ["isogeometric analysis", "isogeometric analysis", "ansatz functions", "problem geometry", "problem geometry", "computer aided design", "cad", "nurbs", "t-splines", "finite element methods", "igafem", "cad", "boundary element method", "bem", "numerical scheme", "isogeometric bem", "igabem", "2d bem", "3d bem", "standard bem", "numerical analysis of igabem", "igabem", "quadrature analysis", "a posteriori error estimation", "standard bem", "igabem", "shed some first light on a posteriori error analysis for igabem", "igabem", "a posteriori error analysis", "mathematical foundation of a corresponding adaptive algorithm", "adaptive algorithm"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "isogeometric analysis", "ansatz functions", "discretization", "partial differential equation", "problem geometry", "computer aided design (cad)", "nurbs", "t-splines", "finite element methods (igafem)", "boundary element method (bem)", "fundamental solution", "isogeometric bem (igabem)", "2d bem", "3d bem", "piecewise polynomials", "numerical analysis", "quadrature analysis", "a posteriori error estimation", "adaptive"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In recent years, the Discontinuous Galerkin (DG) method has emerged as a more thorough alternative for locally solving conservation laws of the shallow water equations with higher accuracy  [21–27]. The DG method further involves finite element weak formulation to–inherently from conservation principles–shape a piecewise-polynomial solution over each local discrete cell, via local basis functions. On this basis, the DG polynomial accuracy is spanned by a set of coefficients, describing accuracy information, which are all locally evolved in time from conservation principles at the discrete level, with an arbitrary order of accuracy. A DG-based shallow water model appeals in providing higher quality solutions on very coarse meshes than a traditional finite volume counterpart, but is comparatively expensive to run and imposes a more restrictive stability condition for the CFL number  [28,29].", "ground_truth": ["discontinuous galerkin", "dg", "solving conservation laws of the shallow water equations", "shallow water equations", "dg method", "finite element weak formulation", "shape a piecewise-polynomial solution over each local discrete cell", "local basis functions", "dg polynomial accuracy", "conservation principles", "conservation principles", "dg-based shallow water model", "very coarse meshes", "cfl number"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "discontinuous galerkin (dg) method", "conservation laws", "shallow water equations", "higher accuracy", "finite element weak formulation", "piecewise-polynomial solution", "local discrete cell", "local basis functions", "dg polynomial accuracy", "coefficients", "time evolution", "arbitrary order of accuracy", "dg-based shallow water model", "coarse meshes", "finite volume", "expensive computation", "stability condition", "cfl number", "let me know if you'd like any refinements or additional details!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "FR schemes are similar to nodal DG schemes, which are arguably the most popular type of unstructured high-order method (at least in the field of computational aerodynamics). Like nodal DG schemes, FR schemes utilise a high-order (nodal) polynomial basis to approximate the solution within each element of the computational domain, and like nodal DG schemes, FR schemes do not explicitly enforce inter-element solution continuity. However, unlike nodal DG schemes, FR methods are based solely on the governing system in a differential form. A description of the FR approach in 1D is presented below. For further information see the original paper of Huynh  [2].", "ground_truth": ["fr schemes", "nodal dg schemes", "unstructured high-order method", "computational aerodynamics", "nodal dg schemes", "fr schemes", "high-order", "nodal", "approximate the solution", "nodal dg schemes", "fr schemes", "enforce inter-element solution continuity", "nodal dg schemes", "fr methods", "fr approach", "computational domain"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fr schemes", "nodal dg schemes", "unstructured high-order method", "computational aerodynamics", "high-order polynomial basis", "solution approximation", "computational domain", "inter-element solution continuity", "governing system", "differential form", "fr approach", "1d", "huynh", "original paper"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The immersed boundary method (IBM), proposed by Peskin for studying flow patterns around heart valves  [3], has been applied to a wide range of problems including arterial blood flow  [4], modelling of the cochlea  [5], modelling of red blood cells in Poiseuille flow  [6] and flows involving suspended particles  [7]. A comprehensive list of applications can be found in  [8]. The IBM is both a mathematical formulation and a numerical scheme for fluid–structure interaction problems. As mentioned above, in a classical fluid–structure interaction problem, the fluid and the structure are considered separately and then coupled together via some suitable jump conditions. In the IBM however, the structure–which is usually immersed in a Newtonian fluid–is viewed as being part of the surrounding fluid. This means that only a single equation of motion needs to be solved (i.e. a one-phase formulation). Additionally, the IBM allows the immersed structure to move freely over the underlying fluid mesh, alleviating the need for the remeshing required in a classical formulation.", "ground_truth": ["immersed boundary method", "ibm", "studying flow patterns around heart valves", "arterial blood flow", "modelling of the cochlea", "modelling of red blood cells in poiseuille flow", "flows involving suspended particles", "ibm", "fluid–structure interaction problems", "fluid–structure interaction problem", "fluid", "fluid", "structure", "structure", "coupled together", "ibm", "newtonian fluid", "surrounding fluid", "motion", "ibm", "fluid", "underlying fluid mesh", "remeshing"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "immersed boundary method (ibm)", "peskin", "flow patterns", "heart valves", "arterial blood flow", "modelling", "cochlea", "red blood cells", "poiseuille flow", "suspended particles", "mathematical formulation", "numerical scheme", "fluid–structure interaction", "newtonian fluid", "one-phase formulation", "immersed structure", "fluid mesh", "remeshing", "classical formulation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We consider the shape optimisation of two- and three-dimensional solids by combining multiresolution subdivision surfaces with immersed finite elements. As widely discussed in isogeometric analysis literature, the geometry representations used in today’s computer aided design (CAD) and finite element analysis (FEA) software are inherently incompatible  [1]. This is particularly limiting in shape optimisation during which a given CAD geometry model is to be iteratively updated based on the results of a finite element computation. The inherent shortcomings of present geometry and analysis representations have motivated the proliferation of various shape optimisation techniques. In the most prevalent approaches a surrogate geometry model  [2–8] or the analysis mesh  [9,10] instead of the true CAD model is optimised, see also  [11] and references therein. Generally, it is tedious or impossible to map the optimised surrogate geometry model or analysis mesh back to the original CAD model, which is essential for continuing with the design process and later for manufacturing purposes. Moreover, geometric design features are usually defined with respect to the CAD model and cannot be easily enforced on the surrogate model. Recently, the shape optimisation of shells, solids and other applications using isogeometric analysis has been explored; that is, through directly optimising the CAD geometry model  [12–15].", "ground_truth": ["shape optimisation", "two- and three-dimensional solids", "combining multiresolution subdivision surfaces", "immersed finite elements", "isogeometric analysis literature", "geometry representations", "computer aided design", "cad", "finite element analysis", "fea", "shape optimisation", "cad geometry model", "finite element computation", "geometry and analysis representations", "shape optimisation techniques", "surrogate geometry model", "analysis mesh", "cad model", "optimised surrogate geometry model", "analysis mesh", "cad model", "cad model", "shape optimisation", "shells", "solids", "isogeometric analysis", "cad geometry model", "directly optimising", "surrogate model"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "shape optimisation", "two-dimensional solids", "three-dimensional solids", "multiresolution subdivision surfaces", "immersed finite elements", "isogeometric analysis", "geometry representations", "computer aided design (cad)", "finite element analysis (fea)", "cad geometry model", "finite element computation", "surrogate geometry model", "analysis mesh", "design process", "manufacturing", "geometric design features", "shells", "solids", "isogeometric analysis (repeated for emphasis)", "cad geometry model (repeated for emphasis)", "let me know if"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The exquisite manipulation and exact measurement of properties of individual nanomaterials, compared with notable progress in their preparation, have not been thoroughly addressed albeit being of prime importance for the sustained development of new devices [58–61]. To date, several instruments have been designed for such goals, namely, scanning electron microscopes (SEM), atomic force microscopes (AFM) and transmission electron microscopes (TEM) [62,63]. Compared with the first two setups, which have no direct access to the material internal structure and atomic bonding information [64–67], the state-of-the-art in situ high-resolution TEM technique allows one to not only manipulate with an individual object at the nano-scale precision but to also get deep insights into its physical, chemical, and microstructural statuses [68–71]. Combining the capabilities of a conventional high-resolution TEM and AFM or STM probes produces advanced and dedicated TEM holders, which are becoming the powerful tools in nanomaterials manipulation and properties analysis. Such holders have been commercialized, for instance, by “Nanofactory Instruments AB’’, Goteborg, Sweden [72]. The full usefulness of these advanced in-situ TEM techniques is apparent with respect to mechanical and thermal property analysis of individual nanostructures, e.g., elasticity, plasticity and strength data while employing direct bent or tensile tests [73–75], probing electrical characteristics, e.g., field emission [27,76,77], electrical transport tracing [78–80], soldering [81,82], and doping [83], etc.", "ground_truth": ["nanomaterials", "exact measurement of properties", "sustained development of new devices", "scanning electron microscopes", "sem", "atomic force microscopes", "afm", "transmission electron microscopes", "tem", "material internal structure", "atomic bonding information", "tem", "manipulate with an individual object", "tem", "afm", "nanomaterials", "stm probes", "nanomaterials manipulation", "properties analysis", "tem holders", "holders", "tem techniques", "mechanical and thermal property analysis", "nanostructures", "direct bent or tensile tests", "probing electrical characteristics", "field emission", "electrical transport tracing", "soldering", "doping", "instruments"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "manipulation", "measurement", "nanomaterials", "preparation", "devices", "scanning electron microscopes (sem)", "atomic force microscopes (afm)", "transmission electron microscopes (tem)", "internal structure", "atomic bonding", "in situ high-resolution tem", "nano-scale precision", "physical status", "chemical status", "microstructural status", "tem holders", "nanomaterials manipulation", "properties analysis", "nanofactory instruments ab", "mechanical property analysis", "thermal property analysis", "elasticity", "plasticity", "strength", "bent tests"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "MINERAL (MINeral ERror AnaLysis) is a new MATLAB® based program that provides mineral formula recalculations combined with the associated propagation of the analytical uncertainties. Methods are based on the work of Giamarita and Day (1990). However, additional features have been added to provide users with greater flexibility in data reporting. Many programs exist to recalculate wt% data into formula unit cations. Some generalized programs can be used to recalculate the formula of multiple minerals e.g. CALCMIN (Brandelik, 2009) and HYPER-FORM (De Bjerg et al., 1992). Other programs are mineral specific e.g. AMPH CLASS (Esawi, 2004) and PROBE AMPH (Tindle and Webb, 1994) for the recalculation of amphibole analyses; ILMAT (Lepage, 2003) for the recalculation of magnetite and ilmenite; and PX-NOM (Sturm, 2002) for the recalculation of pyroxene analyses. MINERAL provides a rapid method for the recalculation of multiple common minerals. However, its strength lies in the fact that is the first tool to incorporate the associated uncertainty propagation calculations. As these are performed concurrently with the standard recalculations, no additional time is needed to perform uncertainty propagation. While an understanding of the underlying calculations is strongly recommended, MINERAL is designed to allow users with little or no experience operating MATLAB® and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease.", "ground_truth": ["mineral (mineral error analysis)", "a new matlab® based program", "methods", "provides mineral formula recalculations combined with the associated propagation of the analytical uncertainties", "some generalized programs", "calcmin", "hyper-form", "recalculate the formula of multiple minerals", "to provide users with greater flexibility in data reporting", "amph class", "probe amph", "ilmat", "px-nom", "recalculation of amphibole analyses", "recalculation of magnetite and ilmenite", "recalculation of pyroxene analyses", "mineral", "recalculation of multiple common minerals", "incorporate the associated uncertainty propagation calculations.", "mineral", "allow users with little or no experience operating matlab® and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "mineral", "mineral error analysis", "matlab®", "mineral formula recalculations", "analytical uncertainties", "propagation of uncertainties", "giamarita and day (1990)", "data reporting", "wt% data", "formula unit cations", "calcmin", "brandelik (2009)", "hyper-form", "de bjerg et al."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai⁎(λ), aY⁎(λ), aD⁎(λ), bX(λ), bb,X⁎, bb,Mie⁎] and the apparent optical properties of the bottom [Rib(λ), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).", "ground_truth": ["database", "wasi", "ioccg", "nasa bio-optical marine algorithm data set", "nomad", "seawifs bio-optical archive and storage system", "seabass", "ai⁎(λ", "ay⁎(λ)", "ad⁎(λ)", "bx(λ)", "bb", "x⁎", "bb", "mie⁎", "water constituents", "region or season specific information", "optical properties", "ioccg (2006) data bank", "optical properties of the bottom [rib(λ)", "bi] and the atmosphere", "in-situ measurements"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "model parameters", "regionally", "seasonally", "inherent optical properties", "water constituents", "ai⁎(λ)", "ay⁎(λ)", "ad⁎(λ)", "bx(λ)", "bb", "x⁎", "bb", "mie⁎", "apparent optical properties", "bottom", "rib(λ)", "bi", "atmosphere", "database", "wasi", "in-situ measurements", "lakes", "southern germany", "gege", "1998", "hee"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Artificial Neural Networks (ANN) have been widely used in science and engineering problems. They attempt to model the ability of biological nervous systems to recognize patterns and objects. ANN basic architecture consists of networks of primitive functions capable of receiving multiple weighted inputs that are evaluated in terms of their success at discriminating the classes in Τa. Different types of primitive functions and network configurations result in varying models (Hastie et al., 2009; Rojas, 1996). During training network connection weights are adjusted if the separation of inputs and predefined classes incurs an error. Convergence proceeds until the reduction in error between iterations reaches a decay threshold (Kotsiantis, 2007; Rojas, 1996). We use feed-forward networks with a single hidden layer of nodes, a so called Multi-Layer Perceptron (MLP) (Venables and Ripley, 2002), and select one of two possible parameters: size, the number nodes in the hidden layer.", "ground_truth": ["artificial neural networks", "science and engineering problems", "model the ability of biological nervous systems to recognize patterns and objects", "ann basic architecture", "networks of primitive functions", "receiving multiple weighted inputs", "different types of primitive functions and network configurations", "training network connection", "weights", "adjusted if the separation of inputs and predefined classes incurs an error", "feed-forward networks with a single hidden layer of nodes", "multi-layer perceptron", "select one of two possible parameters", "convergence", "the reduction in error between iterations reaches a decay threshold", "mlp", "ann"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "artificial neural networks (ann)", "science", "engineering", "biological nervous systems", "recognize patterns", "objects", "basic architecture", "networks", "primitive functions", "weighted inputs", "discriminating classes", "network configurations", "models", "hastie et al.", "2009", "rojas", "1996", "training", "network connection weights", "separation of inputs", "predefined classes", "error", "convergence", "reduction in error", "decay threshold", "kotsiantis", "2007"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Hitherto, the investigation of fossil-orientation was only used for the topmost surface of fossil mass occurrences, deposited directly on the sea floor. Due to the fast development of virtual methods (e.g., macro-CT, µ-CT, nano-CT, etc.) it became possible, to investigate the interior orientation of such fossil mass occurrences in three-dimensional detail. Although, a series of paleontological studies deal with 3D-visualization of fossil-elements, no mass occurrence has previously been reconstructed three dimensionally for investigating their interior orientation. This study illustrates an interdisciplinary approach of virtual reconstruction, analyses and interpretation of the interior orientation of an ammonoid mass occurrence. The method established herein produces clear and consistent results using planispirally coiled ammonoid shells – fossils, that so far would have been used only with caution for depositional interpretations. This method can be applied to any kind of fossil mass occurrence, or even other abundant organic elements and particles, to examine their orientation and depositional conditions to conclude on their paleoenvironment, particularly on paleocurrents.", "ground_truth": ["ossil mass occurrences", "virtual methods", "macro-ct", "µ-ct", "nano-ct", "fossil mass occurrences", "3d-visualization", "fossil-elements", "virtual reconstruction", "ammonoid mass occurrence", "planispirally coiled ammonoid shells", "fossil mass occurrence", "other abundant organic elements", "particles", "investigation of fossil-orientation", "depositional interpretations", "investigating their interior orientation", "analyses and interpretation of the interior orientation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fossil-orientation", "topmost surface", "fossil mass occurrences", "sea floor", "virtual methods", "macro-ct", "µ-ct", "nano-ct", "interior orientation", "three-dimensional detail", "paleontological studies", "3d-visualization", "fossil-elements", "virtual reconstruction", "ammonoid mass occurrence", "plan"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The above discussion summarizes the state of the art related to impacts and interpretations of communication latency between RT simulators. However, research is focused primarily on the effect of the data loss during the communication and how to mitigate it [34]. In the thermo-electric co-simulation example in [35], the time constant is larger in the thermal simulation than that of power system simulation. Thus the communication latency will not significantly affect the accuracy of co-simulation. In [36], the co-simulation is performed using resources at the same location without synthetically introduced delays, which means the communication latency between RT simulators is ignored. In [37], the authors have mentioned the communication latency as an important factor in the distributed simulation and that its effect on simulation stability will be studied as future work. An in-depth research about the role of communication latency and mitigation measure for geographically distributed RT simulations is identified as a technical gap and addressed in this paper.", "ground_truth": ["thermo-electric co-simulation", "rt simulators", "co-simulation", "distributed simulation", "role of communication latency", "communication latency", "communication latency"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "state of the art", "impacts", "interpretations", "communication latency", "rt simulators", "research", "data loss", "mitigate", "thermo-electric co-simulation", "time constant", "thermal simulation", "power system simulation", "accuracy", "co-simulation", "synthetically introduced delays", "distributed simulation", "simulation stability"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Despite the fact that SRC-HE reduces the number of FEs, audio measurements extraction based on SRC would still be not suitable for real-time applications [39]. The previous SRC-HE module is then replaced by the generalised cross correlation phase transform (GCC-PHAT) introduced in Section 2.1, as this does not involve cumbersome point function estimations. The drawback is that the basic GCC algorithm can only detect one source at a time and it is known to be sensitive to room reverberations [5], however it is still effective under moderate reverberant environments (T60≈0.3s) [40]. For these reasons, at first experiments where only a speaker is active at any given time are carried out, as it often happens in a polite conversation between two or more people. Speech segments using a voice activity detector (VAD) [41] are further extracted and processed using a GCC-PHAT step, for the signal to be more robust to reverberations. Thus, the measure vector obtained za (see Section 2.1) can now be rewritten as za={τm(t)}, where each component τm is the TDOA collected at the m-th microphone pair at each time step t. Since TDOAs are not linear in the speaker position, they must be input into an extended Kalman filter (EKF), as in [10] to get an audio position estimation.", "ground_truth": ["generalised cross correlation phase transform", "gcc-phat", "speaker", "voice activity detector", "vad", "gcc-phat step", "m-th microphone pair", "kalman filter", "ekf", "audio position estimation", "speech segments", "src-he", "gcc algorithm"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "src-he", "fes (function estimations)", "real-time applications", "generalized cross correlation phase transform (gcc-phat)", "point function estimations", "source detection", "room reverberations", "reverberant environments", "t60≈0.3s", "polite conversation", "voice activity detector (vad)", "speech segments", "signal robustness", "measure vector (za)", "tdoa (time difference of arrival)", "microphone pair", "extended kalman filter (ekf"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper we construct such a physical model with a continuous distribution of relaxations. It is based on the phenomenological theory of relaxation processes which have a long history in physics literature and was recently summarized in a monograph in which references to other relevant publications can be found, [24]; also see [25]. The present work is confined to relaxation mechanisms which result from changes in normal stresses. More specifically, we are interested in the local mechanisms of irreversible energy loss caused by uniform compression or expansion of a medium for which all components remain unchanged, rather than the losses caused by friction between different layers of a medium which move with different velocities (for a more detailed discussion of this issue see [26]). No attempt is made to model effects of shear viscosity and heat conduction beyond the conventional Navier–Stokes approach, since this topic goes far beyond the scope of this paper.", "ground_truth": ["physical model", "continuous distribution of relaxations", "phenomenological theory of relaxation processes", "relaxation mechanisms", "local mechanisms of irreversible energy loss", "shear viscosity", "heat conduction", "navier–stokes approach", "relaxation processes", "relaxations", "irreversible energy loss", "compression or expansion of a medium", "friction between different layers of a medium"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "physical model", "continuous distribution", "relaxations", "phenomenological theory", "relaxation processes", "physics literature", "monograph", "references", "relaxation mechanisms", "normal stresses", "local mechanisms", "irreversible energy loss", "uniform compression", "expansion", "medium", "components", "friction", "layers", "velocities", "shear viscosity", "heat conduction", "navier–stokes approach", "scope"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The propagation of unsteady disturbances in ducts of slowly-varying geometry, such as those typical of an aeroengine, can be successfully modelled using a multiple scales approach. From the first application [1] of multiple-scales analysis to sound propagation in ducts of rectangular and circular cross section without mean flow, more recent developments have extended the method to cases with uniform mean flow [2], mean swirling flow [3], ducts of arbitrary cross section [4] (with uniform mean flow) and strongly curved ducts [5]. The multiple-scales approach has a number of distinct advantages over full numerical methods as it is ideally suited to handle higher frequencies and the computational complexity is only marginally more than calculating the eigenmodes inside a straight parallel duct. The accuracy and usefulness of the multiple scales approach has been validated against finite-element methods [6] for realistic aeroengine configurations and acoustic frequencies [7,8].", "ground_truth": ["ducts", "propagation of unsteady disturbances", "aeroengine", "multiple scales approach", "multiple-scales analysis", "sound propagation", "ducts", "uniform mean flow", "mean swirling flow", "ducts", "uniform mean flow", "multiple-scales approach", "numerical methods", "higher frequencies", "calculating the eigenmodes", "straight parallel duct", "multiple scales approach", "finite-element methods", "aeroengine", "acoustic frequencies", "aeroengine configurations", "strongly curved ducts", "mean flow"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "propagation", "unsteady disturbances", "ducts", "slowly-varying geometry", "aeroengine", "multiple scales approach", "sound propagation", "rectangular cross section", "circular cross section", "mean flow", "uniform mean flow", "swirling flow", "arbitrary cross section", "strongly curved ducts", "numerical methods", "higher frequencies", "computational complexity", "eigenmodes", "straight parallel duct", "accuracy", "finite-element methods", "realistic aeroengine configurations", "acoustic frequencies", "let me know if you'd like any refinements or additional details"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We describe three ways to solve the reflection problem. The first way is very simple (Section  4). We exploit the consequences of shifting the semi-infinite row by one period (to the right or left). In effect, we regard the semi-infinite row as two scatterers, one of which is another semi-infinite row. This idea goes back to a series of papers by Millar in the 1960s, starting with  [2]. He used it for several two-dimensional grating problems. A similar approach was used for layered media by Shenderov  [3]. In our one-dimensional context, we obtain a quadratic equation for R; we show how to select the correct solution. We remark that there has been much recent interest in related two-dimensional waveguide problems; see, for example,  [4–6], where the shifting-by-one-period idea is again employed, leading to a quadratic equation for a certain operator.", "ground_truth": ["reflection problem", "shifting the semi-infinite row", "semi-infinite row", "two scatterers", "semi-infinite row", "two-dimensional grating problems", "layered media", "obtain a quadratic equation", "two-dimensional waveguide problems", "shifting-by-one-period idea", "quadratic equation", "waveguide problems"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "reflection problem", "semi-infinite row", "scatterers", "millar", "two-dimensional grating problems", "layered media", "shenderov", "one-dimensional context", "quadratic equation", "correct solution", "two-dimensional waveguide problems", "shifting-by-one-period idea", "operator"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Max-linear programs have been used to describe optimisation problems for multiprocessor interactive systems. In some instances the variables used in this model are required to be integer; however, no method seems to exist for finding integer solutions to max-linear programs.For a generic class of matrices, we show that integer solutions to two-sided max-linear systems and programs can be found in polynomial time. For general matrices, we adapt the existing methods for finding real solutions to obtain algorithms for finding integer solutions.", "ground_truth": ["max-linear programs", "multiprocessor interactive systems", "optimisation problems", "variables", "integer solutions", "generic class of matrices", "integer solutions", "two-sided max-linear systems and programs", "general matrices", "algorithms for finding integer solutions", "max-linear programs", "adapt the existing methods for finding real solutions"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "max-linear programs", "optimisation problems", "multiprocessor interactive systems", "integer variables", "integer solutions", "generic class of matrices", "two-sided max-linear systems", "polynomial time", "real solutions", "algorithms"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We study sequences of optimal walks of a growing length in weighted digraphs, or equivalently, sequences of entries of max-algebraic matrix powers with growing exponents. It is known that these sequences are eventually periodic when the digraphs are strongly connected. The transient of such periodicity depends, in general, both on the size of digraph and on the magnitude of the weights. In this paper, we show that some bounds on the indices of periodicity of (unweighted) digraphs, such as the bounds of Wielandt, Dulmage–Mendelsohn, Schwarz, Kim and Gregory–Kirkland–Pullman, apply to the weights of optimal walks when one of their ends is a critical node.", "ground_truth": ["digraph", "digraphs are strongly connected", "sequences of optimal walks of a growing length in weighted digraphs", "sequences of entries of max-algebraic matrix powers with growing exponents", "(unweighted) digraphs"], "prediction_list": ["here are the keywords extracted from the given scientific paragraph:", "sequences", "optimal walks", "growing length", "weighted digraphs", "max-algebraic matrix powers", "growing exponents", "eventually periodic", "strongly connected", "transient", "periodicity", "size of digraph", "magnitude of weights", "bounds", "indices of periodicity", "unweighted digraphs", "wielandt", "dulmage–mendelsohn", "schwarz", "kim", "gregory–kirkland–pullman", "critical node"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Owing to widespread availability, the most extensively adopted tomography technique utilizes the milling power of a focused ion beam (FIB) in conjunction with the imaging capabilities of high resolution FE-SEM, to provide a sequence of 2D images that can be effectively re-combined in 3D space. However, because this technique is destructive, studies of microstructural evolution are influenced by inherent sample variability. Non-destructive X-ray nano-computed tomography (CT) [9–11] provides a platform for exploring dynamic microstructural change in the absence of these possible complications and is compatible with both laboratory and synchrotron radiation. The authors have previously demonstrated a technique for preparation of optimal sample geometries for X-ray nano-CT [12], while this FIB sample preparation route will involve the selective removal of portions of the fuel cell electrode microstructure (and therefore may be destructive to the working fuel cell), the non-destructive X-ray characterization technique allows repeated, non-destructive characterization of the selected sample which facilitates the study of microstructural evolution processes in response to various environmental changes.", "ground_truth": ["tomography technique", "focused ion beam", "fib", "high resolution fe-sem", "2d images", "studies of microstructural evolution", "non-destructive x-ray nano-computed tomography", "ct", "exploring dynamic microstructural change", "laboratory and synchrotron radiation", "preparation of optimal sample geometries for x-ray nano-ct", "x-ray nano-ct", "fib sample preparation route", "fib sample", "fuel cell electrode microstructure", "working fuel cell", "non-destructive x-ray characterization technique", "selected sample", "microstructural evolution processes"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "tomography technique", "focused ion beam (fib)", "milling power", "high resolution fe-sem", "2d images", "3d space", "destructive technique", "microstructural evolution", "sample variability", "non-destructive", "x-ray nano-computed tomography (ct)", "dynamic microstructural change", "laboratory", "synchrotron radiation", "sample geometries", "fib sample preparation", "fuel cell electrode microstructure", "non-destructive x-ray characterization", "repeated characterization", "environmental changes"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In conclusion, a new approach to the “grind-free” nanoprecursor route to direct combinatorial solid state synthesis of several “difficult to make” and hitherto unknown phase-pure heterometallic Ruddlesden Popper type La4Ni3−xFexO10 materials has been described. The new approach used a high-throughput reactor and robotic automation (RAMSI) to rapidly synthesise a range of nanoparticle co-precipitate precursors in cloned libraries at a rate of 7.5 samples an hour. Each library could then be heat-treated at a different temperature and an initial powder XRD screen was used to locate and approximate phase boundary. A more focussed second synthesis and XRD characterisation of selected larger heat-treated powders was then performed to reconfirm the locations of the phase boundaries with the highest dopant level being achieved for La4Ni2FeO10 which is significantly greater Fe doping than has been achieved by anyone previously (despite several notable efforts). EXAFS data suggested that Fe3+ was located onto Ni sites in all cases and did not exist as a separate iron oxide phase.", "ground_truth": ["“grind-free” nanoprecursor route", "direct combinatorial solid state synthesis", "phase-pure heterometallic ruddlesden popper type la4ni3−xfexo10 materials", "high-throughput reactor", "robotic automation", "ramsi", "nanoparticle co-precipitate precursors", "synthesise a range of nanoparticle co-precipitate precursors", "rate of 7.5 samples an hour", "heat-treated", "powder xrd screen", "locate and approximate phase boundary", "synthesis and xrd characterisation", "heat-treated powders", "la4ni2feo10", "fe3+", "ni", "fe", "iron oxide", "exafs data", "new approach to the “grind-free” nanoprecursor route to direct combinatorial solid state synthesis"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "grind-free", "nanoprecursor route", "direct combinatorial solid state synthesis", "difficult to make", "phase-pure", "heterometallic", "ruddlesden popper type", "la4ni3−xfexo10", "high-throughput reactor", "robotic automation", "ramsi", "nanoparticle co-precipitate precursors", "cloned libraries", "heat-treated", "powder xrd screen", "phase boundary", "focussed second synthesis", "xrd characterisation", "dopant level", "la4ni2feo10", "fe doping"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "At 200 – 300°C: nuclear densities are localised in the tetrahedral volume roughly covering the 8c and 32f positions with “bulges” of nuclear densities pointing toward the 48i position, while at 400 and 500°C continuous nuclear densities forming a straight line along the <100> direction are found, indicative of oxide-ion diffusion pathway along that direction. In the literature, curved pathways along the <100> direction passing through the 48i site are generally observed in fluorite materials [20], the prevalence of curve pathway as opposed from straight pathway is explained by the repulsion between cation and anions, the curved pathway allowing the cation–anion to maintain a reasonable distance. However, a straight pathway is observed for Y0.785Ta0.215O1.715 [23], as is the case for the present material. This suggests that Ta and Re cations might play a similar role in these systems.", "ground_truth": ["“bulges”", "oxide-ion diffusion", "oxide", "ion", "fluorite materials", "cation", "anions", "cation", "anion", "present material", "ta and re cations"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "200 – 300°c", "nuclear densities", "tetrahedral volume", "8c positions", "32f positions", "bulges", "48i position", "400°c", "500°c", "continuous nuclear densities", "straight line", "<100> direction", "oxide-ion diffusion pathway", "fluorite materials", "curved pathways", "repulsion", "cation-anion", "reasonable distance", "y0.785ta0.215o1.715", "ta cations", "re cations"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "While impedance spectroscopy is a quite common method to investigate mixed conducting thin film electrodes, [6,10–12] oxygen tracer experiments are often performed on bulk samples [13–16]. Recently, several IEDP measurements of mixed conducting cathode materials were published with the oxide films being deposited on insulating substrates [17–19]. However, to the best of the authors' knowledge no study so far reported experiments with both techniques being applied on the same films at the same temperature. This contribution reports the results of a study applying EIS and IEDP to one and the same La0.6Sr0.4CoO3−δ (LSC) thin film in order to get complementary results on the resistive contributions of the oxygen reduction kinetics on such films. As electrical measurements require an oxygen ion conductor, yttria stabilized zirconia (YSZ) was used as substrate for LSC films with two different grain sizes. Quantitative material parameters are deduced from both types of experiments and comparison of the data allowed testing the appropriateness of analysis models.", "ground_truth": ["impedance spectroscopy", "mixed conducting thin film electrodes", "oxygen tracer experiments", "bulk samples", "iedp measurements", "mixed conducting cathode materials", "oxide films", "insulating substrates", "eis", "iedp", "la0.6sr0.4coo3−δ", "lsc", "resistive contributions of the oxygen reduction kinetics", "electrical measurements", "oxygen ion conductor", "yttria stabilized zirconia", "ysz", "lsc films", "quantitative material parameters", "comparison of the data", "analysis models", "appropriateness of analysis models", "films", "thin film", "oxygen reduction kinetics", "films", "oxygen", "applying eis and iedp to one and the same la0.6sr0.4coo3−δ (lsc) thin film", "grain"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "impedance spectroscopy", "mixed conducting thin film electrodes", "oxygen tracer experiments", "bulk samples", "iedp measurements", "mixed conducting cathode materials", "oxide films", "insulating substrates", "eis", "iedp", "la0.6sr0.4coo3−δ (lsc)", "thin film", "oxygen reduction kinetics", "electrical measurements", "oxygen ion conductor", "yttria stabilized zirconia (ysz)", "substrate", "grain sizes", "quantitative material parameters", "analysis models"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Thin MIEC layers of GDC and STFO on single-crystalline YSZ substrates were exposed to H2/H218O atmosphere for thermally and electrochemically driven tracer exchange experiments. Rectangular noble metal thin film current collectors were deposited on top and beneath the MIEC layer and used for polarization. The lateral distribution of the tracer revealed several interesting features: (i) In case of thermal tracer exchange, an enhanced tracer fraction is found on top of the metallic current collector due to its ionically blocking nature. At the edges of the current collector, the concentration of 18O decreases with a finite step width that is correlated with in-plane diffusion of oxygen ions. (ii) Due to the low electronic conductivity of STFO and GDC, the MIEC area that is influenced by an applied bias is restricted to a region close to the current collector. The width of this active region depends on the bias. It amounts to only 10–15μm for STFO but more than 100μm for GDC at a cathodic bias of −500mV. (iii) Not only enhanced tracer incorporation due to cathodic bias but also reduced incorporation due to anodic bias could be experimentally resolved in the active region.", "ground_truth": ["thin miec layers", "gdc", "stfo", "single-crystalline ysz substrates", "h2/h218o atmosphere", "tracer exchange experiments", "exposed to h2/h218o atmosphere", "rectangular noble metal thin film current collectors", "deposited on top and beneath the miec layer", "miec layer", "polarization", "lateral distribution of the tracer", "thermal tracer exchange", "metallic current collector", "18o", "oxygen ions", "in-plane diffusion of oxygen ions", "stfo", "gdc", "applied bias", "current collector", "width of this active region", "stfo", "gdc", "enhanced tracer incorporation", "reduced incorporation", "low electronic conductivity", "tracer fraction"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "thin miec layers", "gdc", "stfo", "single-crystalline ysz substrates", "h₂/h₂¹⁸o atmosphere", "thermally driven tracer exchange", "electrochemically driven tracer exchange", "noble metal thin film current collectors", "polarization", "lateral distribution", "tracer fraction", "ionically blocking nature", "18o concentration", "in"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Two different micro-contact set-ups were used in the experiments. The asymmetrically heated measurement set-up (Fig. 2a) allows to change the contacted electrode within seconds and thereby to gain statistical information over a large number of different microelectrodes on one and the same sample in a relatively short time. It also enables monitoring of optical changes during the measurement in real time. However, the asymmetrical heating from the bottom side and local cooling (e.g. by convection, radiation, and the contacting tip acting as a heat sink) is known to cause temperature gradients within the sample [11]. Such temperature gradients are responsible for thermo-voltages, which can lead to measurement artifacts in electrochemical experiments [24]. Moreover, in this set-up temperature cycles can hardly be performed on single microelectrodes but require subsequent contacting and de-contacting of different microelectrodes.", "ground_truth": ["micro-contact set-ups", "asymmetrically heated measurement set-up", "electrode", "change the contacted electrode", "gain statistical information", "microelectrodes", "monitoring of optical changes", "asymmetrical heating", "local cooling", "temperature gradients", "temperature gradients", "electrochemical experiments", "temperature cycles", "microelectrodes", "contacting and de-contacting of different microelectrodes", "microelectrodes", "convection", "radiation", "contacting tip", "heat sink"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "micro-contact set-ups", "asymmetrically heated measurement set-up", "contacted electrode", "statistical information", "microelectrodes", "optical changes", "real-time monitoring", "asymmetrical heating", "local cooling", "convection", "radiation", "contacting tip", "heat sink", "temperature gradients", "thermo-voltages", "measurement artifacts", "electrochemical experiments", "temperature cycles", "subsequent contacting", "de-contacting", "let me know if you'd like any modifications or additional details."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Room temperature powder X-ray diffraction (XRD) was performed on a PANalytical Empyrean diffractometer. The obtained XRD patterns were analysed with STOE Win XPOW software in order to determine phase purity, the crystal structure and the cell parameters of the samples. Thermogravimetric analysis (TGA) was performed using a Netzsch STA 449C instrument equipped with Proteus thermal analysis software. The TGA studies were carried out under reducing conditions (5% H2/Ar) from room temperature to 900°C, in order to determine the weight change of the perovskite during the reduction. The microstructure of the samples' surface was analysed using a JEOL JSM-6700 field emission 74 scanning electron microscope (FEG-SEM). The total conductivity of the samples was measured using a conventional four-terminal method. Bar samples were prepared by calcination at 1300°C for 1h. Gold wire contacts were attached to the bars, which then were cured at 850°C for 1h. The conductivity of the samples was measured under a redox cycle at 900°C. Low oxygen partial pressure was achieved by using a continuous flow of 5% H2/Ar.", "ground_truth": ["powder x-ray diffraction", "xrd", "panalytical empyrean diffractometer", "xrd patterns", "stoe win xpow software", "phase purity", "crystal structure", "cell parameters", "thermogravimetric analysis", "tga", "netzsch sta 449c instrument", "proteus thermal analysis software", "reducing conditions", "5% h2/ar", "weight change", "perovskite", "field emission 74 scanning electron microscope", "feg-sem", "total conductivity", "four-terminal method", "calcination", "gold wire contacts", "conductivity", "redox cycle", "low oxygen partial pressure", "h2/ar", "flow"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "room temperature", "powder x-ray diffraction (xrd)", "panalytical empyrean diffractometer", "xrd patterns", "stoe win xpow software", "phase purity", "crystal structure", "cell parameters", "thermogravimetric analysis (tga)", "netzsch sta 449c instrument", "proteus thermal analysis software", "reducing conditions", "5% h2/ar", "weight change", "perovskite", "reduction", "microstructure", "surface", "jeol jsm-6700"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A nanocomposite system consisting of a semiconducting matrix and embedded ferromagnetic nanostructures has been fabricated. The ferromagnetic characteristics as coercivity, remanence and magnetic anisotropy of the nanocomposite can be adjusted by the electrochemical parameters. Furthermore the spatial distribution of the metal structures within the pores can be varied which means that the magnetic interactions between the particles can be influenced. In the case of densely packed particles within the pores dipolar coupling between them occurs and results in quasi magnetic chains which offer a much larger magnetic anisotropy than non-interacting particles. By modifying the current density small Ni-particles (3–6nm) can be deposited. If the packing density of these particles is sufficiently close, Ni-tubes of a few nanometer in thickness are covering the pore walls. The presented nanocomposite is an interesting system for magnetic applications as magnetic sensor technology. Silicon as substrate renders this composite a good candidate for the integration in existing process technology.", "ground_truth": ["a nanocomposite system", "a semiconducting matrix and embedded ferromagnetic nanostructures", "the ferromagnetic characteristics", "coercivity", "remanence and magnetic anisotropy of the nanocomposite", "the electrochemical parameters", "distribution", "densely packed particles", "a much larger magnetic anisotropy", "non-interacting particles", "modifying the current density", "small ni-particles (3–6nm)", "these particles", "ni-tubes", "the presented nanocomposite", "silicon", "a good candidate for the integration"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "nanocomposite system", "semiconducting matrix", "ferromagnetic nanostructures", "ferromagnetic characteristics", "coercivity", "remanence", "magnetic anisotropy", "electrochemical parameters", "spatial distribution", "metal structures", "magnetic interactions", "densely packed particles", "dipolar coupling", "quasi magnetic chains", "current density", "ni-particles (3–6nm)", "packing density", "ni-tubes", "pore walls", "magnetic applications", "magnetic sensor technology", "silicon substrate", "process technology integration"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "As the progression towards smaller lithographic nodes continues it has become necessary to adopt thinner resist films to mitigate problems such as pattern collapse. To address the issue of reduced etch resistance of thin photoresist films the semiconductor industry has begun to develop multilayer processes where the pattern is first transferred into an intermediate organic hardmask with higher etch selectivity before final silicon pattern transfer [25–27]. In this paper we demonstrate how the introduction of such a multilayer process can also benefit nanosphere lithography by increasing achievable aspect ratios of silicon nanopillars without the need for complex etch processes requiring specialised and expensive equipment, but instead needing only a standard SF6/C4F8 inductively coupled plasma (ICP) mixed mode etch process at room temperature [28]. As intermediate layer material we used polyimide, which finds widespread use as encapsulation material for IC production. It is readily patterned in oxygen plasma and has a lower etch rate than silicon in SF6 gas. Its flexibility can also be used for the fabrication of soft polymer pillars by the same process as we will show. The multilayer process slightly increases the complexity of sample preparation but allows basic ICP etching to achieve high aspect ratio structures at smaller feature sizes that previously reported without the need for complex etching equipment.", "ground_truth": ["thinner resist films", "pattern collapse", "smaller lithographic nodes", "reduced etch resistance of thin photoresist films", "multilayer process", "nanosphere lithography", "intermediate layer material", "ic production", "fabrication of soft polymer pillars", "thin photoresist films", "multilayer processes", "intermediate organic hardmask", "silicon pattern transfer", "silicon nanopillars", "complex etch processes", "coupled plasma", "icp", "polyimide", "encapsulation material", "oxygen plasma", "silicon", "sf6 gas", "multilayer process", "sample preparation", "icp etching", "etching equipment"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "lithographic nodes", "thinner resist films", "pattern collapse", "etch resistance", "thin photoresist films", "semiconductor industry", "multilayer processes", "intermediate organic hardmask", "etch selectivity", "silicon pattern transfer", "nanosphere lithography", "aspect ratios", "silicon nanopillars", "etch processes", "specialised equipment"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A 3D finite element based (FEM) COMSOL capacitance analysis is combined with Monte Carlo single-electron circuit simulations to model device operations during single electron detection. The 3D structural data (Fig. 1b) of the nanoscale DQD pair and multiple gate electrodes are precisely input into COMSOL’s FEM-based electrostatics simulator. Capacitances between different device components are then extracted and fed into the well-tested single electron circuit simulator SETSPICE [11], based on the orthodox theory of single electron tunnelling [12]. For our target d1 of 60nm, simulation results (Fig. 1c) showed that as we sweep the voltage applied on gate G1, VG1, single electron tunnelling into the turnstile’s two QDs should generate shifts in the electrometer current, IDS, of tens of pA. This is well within the charge sensitivity of DQD electrometer [6] and consistent to the same order of magnitude with previous work in single electron detection [13]. In addition, the gate to QD capacitive coupling appear to be sufficient for the control of QD occupations down to the single electron limit, allowing for future manipulation of single electron spins in qubit research.", "ground_truth": ["model device operations during single electron detection", "target d1", "single electron detection", "3d finite element based (fem) comsol capacitance analysis", "monte carlo single-electron circuit simulations", "single electron detection", "3d structural data", "nanoscale dqd pair", "multiple gate electrodes", "comsol’s fem-based electrostatics simulator", "device components", "well-tested single electron circuit simulator", "setspice", "single electron tunnelling", "orthodox theory", "simulation", "gate g1", "vg1", "single electron tunnelling", "turnstile’s two qds", "shifts in the electrometer current", "ids", "electrometer current", "dqd electrometer", "gate", "qd capacitive coupling", "control of qd occupations", "manipulation of single electron spins", "qubit research", "single electron spins"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "3d finite element based (fem)", "comsol", "capacitance analysis", "monte carlo", "single-electron circuit simulations", "device operations", "single electron detection", "3d structural data", "nanoscale dqd pair", "gate electrodes", "fem-based electrostatics simulator", "capacitances", "single electron circuit simulator", "setspice", "orthodox theory", "single electron tunnelling", "simulation results", "gate voltage (vg1)", "single electron tunnelling", "turnstile", "quantum dots"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We evaluated three spin-on carbon hardmasks from Irresistible Materials [12]. The spin-on carbon compositions were dissolved in a suitable solvent such as chloroform or anisole with a concentration in the range 5–50g/l. In this report, film thickness measurements were made for IM-HM11-01 and IM-HM11-02 films, whilst IM-HM11-03 was used for etching; further investigations to compare the performance of the different compositions across tasks are underway. Films of the SoC were prepared by spin coating on hydrogen-terminated silicon substrates with a speed varying between 800 and 2000 RPM for 60s. After spin coating the film was baked for 2min at temperatures of up to 330°C. In order to enable further processing, the SoC should be rendered insoluble in typical solvents for resist and spin-on-hardmask to enable further processing. The elution behavior of films of IM-HM11-01 and IM-HM11-02 for thicknesses between 30 and 325nm was tested as a function of the baking temperature. Fig. 1 shows the normalized film thickness of two formulations of the SoC (IM-HM11-01 and IM-HM11-02), before and after dipping in monochlorobenzene (MCB):IPA 1:1 solution. Prior to baking the thickness of IM-HM11-01 was ∼320nm, and the thickness of IM-HM11-02 was ∼250nm. For temperatures above 190°C the IM-HM11-02 film was rendered insoluble, whilst a temperature of 260°C was required to achieve the same for IM-HM11-01. Film thickness did not affect the elution results.", "ground_truth": ["spin-on carbon compositions", "film thickness measurements", "im-hm11-01", "im-hm11-03", "etching", "compare the performance of the different compositions across tasks", "films of the soc", "spin coating", "film", "aked for 2min at temperatures of up to 330°c", "before and after dipping", "monochlorobenzene", "im-hm11-01", "im-hm11-02", "im-hm11-02 film", "im-hm11-01", "im-hm11-01 and im-hm11-02", "im-hm11-01 and im-hm11-02", "evaluated three spin-on carbon hardmasks from irresistible materials", "soc", "spin-on-hardmask", "elution behavior", "soc (", "baking", "hydrogen-terminated silicon substrates", "mcb", "im-hm11-02 films", "three spin-on carbon hardmasks", "film", "solvent", "chloroform", "anisole"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "spin-on carbon hardmasks", "irresistible materials", "solvent", "chloroform", "anisole", "concentration", "film thickness measurements", "im-hm11-01", "im-hm11-02", "im-hm11-03", "etching", "spin coating", "hydrogen-terminated silicon substrates", "baking", "330°c", "ins"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In summary, we have developed a technique for site-specific nanowire size reduction by FIB thinning. Transmission electron microscope images of a thinned tungsten composite nanowire with width reduced from 80 to 20nm show uniform shrinking along the length of the wire and high resolution images show no obvious changes of the morphology after thinning. The critical current density of the as-deposited wire and one thinned to a width of 50nm is 1.7×105 and 1.4×105A/cm2 at 4.26K, respectively, suggesting insignificant modulation of the electrical properties during thinning. These results suggest that FIB-milling is a potential approach for controllable size reduction with high resolution towards the observation of size- and quantum effects, as well as for construction of 3D superconducting nanodevices.", "ground_truth": ["site-specific nanowire size reduction", "modulation of the electrical properties", "fib-milling", "controllable size reduction with high resolution towards the observation of size- and quantum effects", "uniform shrinking", "fib thinning", "transmission electron microscope images", "thinned tungsten composite nanowire", "wire", "high resolution images", "thinning", "as-deposited wire", "thinning", "construction of 3d superconducting nanodevices", "3d superconducting nanodevices"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "technique", "site-specific", "nanowire", "size reduction", "fib thinning (focused ion beam thinning)", "transmission electron microscope (tem)", "tungsten composite nanowire", "width reduction", "uniform shrinking", "high resolution imaging", "morphology", "critical current density", "as-deposited wire", "electrical properties", "fib-milling"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "There have been suggestions that electrons can be trapped in the bulk and at surfaces of silica [15] but new models of electron trapping centres started to appear only recently. It has been suggested by Bersuker et al., who used molecular models, that electrons can be trapped by Si–O bonds in a-SiO2 leading to their weakening and thus facilitating Si–O bond dissociation [16]. Further calculations by Camellone et al. have shown that electrons can spontaneously trap in non-defective continuum random network model of a-SiO2 [17]. Recent calculations have also demonstrated that the two dominant neutral paramagnetic defects at surfaces of a-SiO2, the non-bridging oxygen centre and the silicon dangling bond, are deep electron traps and can form the corresponding negatively charged defects [18]. However, these theoretical predictions have not yet been confirmed experimentally, emphasising the challenges for identifying defect centres.", "ground_truth": ["molecular models", "electrons", "electrons", "trapped in the bulk and at surfaces of silica", "electron trapping centres", "electrons", "spontaneously trap", "identifying defect centres", "silica", "bulk", "si–o bonds", "a-sio2 leading", "weakening", "trapped", "si–o bond dissociation", "si–o bond", "non-defective continuum random network model", "a-sio2", "a-sio2", "neutral paramagnetic defects", "non-bridging oxygen centre", "silicon dangling bond", "deep electron traps", "negatively charged defects", "surfaces of a-sio2"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "electrons", "trapped", "bulk", "surfaces", "silica", "models", "electron trapping centres", "bersuker et al.", "molecular models", "si–o bonds", "a-sio2", "weakening", "dissociation", "camellone et al.", "calculations", "non-defective continuum random network model", "neutral paramagnetic defects", "non-bridging oxygen centre", "silicon dangling bond", "deep electron traps", "negatively charged defects", "theoretical predictions", "experimentally confirmed", "defect centres"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Ever since the identification of the paramagnetic E′ centre in SiO2 as an unpaired electron localised in an sp3 hybrid orbital of an Si atom backbonded to three oxygen atoms, a number of attempts has been made at explaining the optical and electronic properties of SiO2 in the presence of E′ centres. The irradiation or hole injection induces trapping of positive charge in thin layers of a-SiO2 grown on silicon surfaces by thermal oxidation. This effect has been correlated with paramagnetic E′ centre signals and led to the initial assignment of the neutral oxygen vacancy as the major hole trap in a-SiO2 [1–3]. In this model, originally proposed for E′ centres in α-quartz, upon trapping a hole, one Si atom from the two Si atoms constituting the vacancy remains neutral and hosts the localised unpaired electron while its counterpart becomes positively charged. Although this model has initially been accepted widely for its simplicity, it fails to account for a number of observations, such as the positive charge trapping without generation of E′ centres [4], the formation of high density of E′ centres without the corresponding density of positive charge [5], and the absence of correlation between the decrease of the E′ centre density and the density of positive charge upon post-irradiation electron injection in SiO2 [6].", "ground_truth": ["explaining the optical and electronic properties of sio2 in the presence of e′ centres", "irradiation", "trapping of positive charge in thin layers of a-sio2 grown on silicon surfaces", "paramagnetic e′ centre signals", "e′ centres in α-quartz", "identification of the paramagnetic e′ centre in sio2", "paramagnetic e′ centre", "sio2", "unpaired electron", "sp3 hybrid orbital", "si atom", "oxygen atoms", "sio2", "e′ centres", "hole injection", "thermal oxidation", "a-sio2", "silicon surfaces", "neutral oxygen vacancy", "major hole trap", "a-sio2", "si atom", "si atoms", "localised unpaired electron", "positive charge trapping", "e′ centres", "high density of e′ centres", "positive charge", "e′ centre", "post-irradiation electron injection", "sio2"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "paramagnetic e′ centre", "sio₂ (silicon dioxide)", "unpaired electron", "sp³ hybrid orbital", "si atom (silicon atom)", "oxygen atoms", "optical properties", "electronic properties", "irradiation", "hole injection", "positive charge trapping", "a-sio₂ (amorphous silicon dioxide)", "thermal oxidation", "neutral oxygen vacancy", "hole"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Ge (100) wafers (n- and p-type) were cleaned in ultra high vacuum (<10−6mbar) at 500°C and 600°C for 10min to evaporate any native oxide and so achieve an oxide free surface. Subsequently, wafers were exposed to an Al flux for a range of times to deposit ultrathin Al layers. The samples were then oxidized at ambient temperatures in the MBE load lock to produce Al2O3 layers. The samples were transferred within 1min to an Oxford Instruments OpAL reactor and thin films of HfO2 were deposited on the Al2O3 using atomic layer deposition (ALD). The HfO2 depositions used a [(CpMe)2HfOMeMe] precursor coupled with an O2 plasma as the oxidizing species. Between 30 and 130 ALD cycles were used to grow HfO2 thicknesses from 1.6 to 7nm at 250°C. For electrical measurements, circular gold contacts of area 1.96×10−3cm2 were deposited onto the films to form MOS gate electrodes and Al was deposited on the back of the Ge wafers to provide an ohmic contact. After preliminary measurements, the samples were annealed in forming gas (FGA) at 350°C for 30min. The oxide leakage current was measured using a Keithley 230B voltage source and Keithley 617B electrometer. The HP 4192A low frequency (LF) impedance analyzer at small signal frequencies between 100Hz to 1MHz was used to perform high frequency capacitance–voltage (HF CV) measurements.", "ground_truth": ["ge (100) wafers", "wafers", "samples", "samples", "electrical measurements", "samples", "oxide leakage current", "measured", "hp 4192a low frequency (lf) impedance analyzer", "to perform high frequency capacitance–voltage (hf cv) measurements", "n- and p-type", "cleaned in ultra high vacuum", "ultra high vacuum", "evaporate any native oxide", "native oxide", "oxide free surface", "al flux", "ultrathin al layers", "oxidized", "mbe load lock", "al2o3 layers", "produce al2o3 layers", "oxford instruments opal reactor", "thin films of hfo2", "al2o3", "atomic layer deposition", "ald", "hfo2", "hfo2 depositions", "[(cpme)2hfomeme] precursor", "o2 plasma", "oxidizing species", "grow hfo2 thicknesses", "hfo2", "circular gold contacts", "mos gate electrodes", "films", "al", "ge wafers", "ohmic contact", "annealed in forming gas", "forming gas", "fga", "keithley 230b voltage source", "keithley 617b electrometer", "high frequency capacitance–voltage (hf cv) measurements"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "ge (100) wafers", "n-type", "p-type", "ultra high vacuum (uhv)", "<10⁻⁶ mbar", "500°c", "600°c", "native oxide", "oxide-free surface", "al flux", "ultrathin al layers", "oxidized", "ambient temperatures", "mbe load lock", "al₂o₃ layers", "oxford instruments opal reactor", "hfo₂", "atomic layer deposition (ald)", "[(cpme)₂hfom"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We have demonstrated a new approach to the manufacture of self-folding hydrogel scaffolds by the use of readily available and fast throughput methods. The process shows effective pattern transfer by first embossing a sacrificial layer and using it as a soluble mould in the fabrication process. The use of a sacrificial layer of PAA imparts environmental sensitivity to the hydrogel film on only one surface. The subsequent swelling of the PAA inter-penetrating network (IPN) in elevated pH causes a swelling differential across the film, causing it to roll to accommodate the difference in surface area between the two surfaces. The surface functionalization and patterning stages are thus combined into one photolithographic operation. The net result is a method of producing environmentally triggered self-folding all hydrogel scaffolds by a, to the authors’ knowledge, novel use of sacrificial layer embossing. The patterned hydrogel films can be triggered consecutively allowing for successive rolling and unrolling depending on the aqueous pH. The choice of PEGDMA hydrogel provides a versatile platform for creating a variety of hydrogel scaffolds, and while being non-fouling and nontoxic it is permeable to proteins. Furthermore PEGDMA can be modified to produce biodegradable and cell adhesive hydrogels for a variety of biomedical applications.", "ground_truth": ["manufacture of self-folding hydrogel scaffolds", "sacrificial layer of paa", "producing environmentally triggered self-folding all hydrogel scaffolds", "novel use of sacrificial layer embossing", "patterned hydrogel films", "triggered consecutively", "pegdma hydrogel", "pegdma", "produce biodegradable and cell adhesive hydrogels", "self-folding hydrogel scaffolds", "readily available and fast throughput methods", "effective pattern transfer", "embossing a sacrificial layer", "using it as a soluble mould in the fabrication process", "fabrication process", "soluble mould", "hydrogel film", "surface", "swelling of the paa inter-penetrating network", "paa inter-penetrating network", "ipn", "elevated ph", "swelling differential", "film", "roll", "accommodate the difference in surface area between the two surfaces", "surfaces", "photolithographic operation", "rolling", "unrolling", "aqueous ph", "versatile platform for creating a variety of hydrogel scaffolds", "hydrogel scaffolds", "proteins", "hydrogels", "biomedical applications"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "self-folding hydrogel scaffolds", "manufacture", "fast throughput methods", "pattern transfer", "embossing", "sacrificial layer", "soluble mould", "fabrication process", "paa (polyacrylic acid)", "environmental sensitivity", "hydrogel film", "swelling differential", "inter-penetrating network (ipn)", "elevated ph", "surface functionalization"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The number of experiments conducted was reduced by selecting the four most important parameters for variation, Table 1 while the remaining parameters were kept constant. The O2 flow rate (QO2) was keep constant at 99sccm, while the SF6 flow rate (QSF6) was varied between 0 and 20sccm. The pressure in the etch chamber was controlled to keep the gas density stable. Since the pressure has a pronounced effect on etch characteristics, the pressure (p) was varied between 20 and 40mTorr. It should be noted that the system was run in automatic pressure control mode, which continuously adjusts the throttle valve to keep a constant pressure during etch. The coil power (PC) was fixed at 1000W, while the bias power (PB) was varied between 0 and 30W. Finally, the substrate chuck temperature (T) was controlled between 10 and 50°C. This design resulted in a full factorial screening in four parameters, where three center points were used to check for quadratic curvature, where the quadratic term of a parameter is needed to generate a valid model. The total number of experiments in this setup is 19, which were processed for 20min each. The experiments in the design were carried out in random order.", "ground_truth": ["pressure in the etch chamber", "keep the gas density stable", "factorial screening", "check for quadratic curvature", "etch chamber", "gas", "automatic pressure control", "adjusts the throttle valve", "throttle valve"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "experiments", "parameters", "variation", "o2 flow rate (qo2)", "sf6 flow rate (qsf6)", "pressure", "gas density", "etch characteristics", "automatic pressure control mode", "throttle valve", "coil power (pc)", "bias power (pb)", "substrate chuck temperature (t)", "full factorial screening", "center points", "quadratic curvature", "quadratic term", "valid model", "number of experiments", "random order", "these keywords capture the main technical terms and concepts discussed in the"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We used 2μm of ultra-nanocrystalline diamond (UNCD) grown by chemical vapour deposition (CVD) on a ∼520μm silicon carrier wafer from Advanced Diamond Technologies Ltd. Detailed information about the material and the stamp fabrication can be found in our earlier paper [16]. The UNCD wafer was scribed into 1×1cm2 samples and subjected to RCA cleaning (SC-1), followed by ultrasonic solvent cleaning. Nanofeature stamps were then created from the samples using conventional electron beam lithography (EBL) with negative tone electron sensitive resist, hydrogen silsesquioxane (HSQ). An Al discharge layer was required above the resist to prevent e-beam deflection due to charge build-up on the surface [17]. Several stamps were produced with this process and the pattern written varied in design but consisted of arrays of circular pillars. After EBL and HSQ development, the HSQ was used as an etch mask for RIE with a mixture of oxygen and argon gas. The etched diamond nanopillars were typically 225nm high. Fig. 1 displays a scanning electron micrograph of some typical stamp features.", "ground_truth": ["uncd wafer", "several stamps were produced", "nanofeature stamps were then created from the samples", "al discharge layer", "prevent e-beam deflection due to charge build-up on the surface", "ebl and hsq development", "t", "ultra-nanocrystalline diamond", "uncd", "chemical vapour deposition", "cvd", "vapour", "rca cleaning", "ultrasonic solvent cleaning", "conventional electron beam lithography", "ebl", "negative tone electron sensitive resist", "hydrogen silsesquioxane", "hsq", "circular pillars", "hsq", "etch mask", "rie", "oxygen", "argon gas", "etched diamond nanopillars", "scanning electron micrograph"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "ultra-nanocrystalline diamond (uncd)", "chemical vapour deposition (cvd)", "silicon carrier wafer", "advanced diamond technologies ltd", "stamp fabrication", "rca cleaning (sc-1)", "ultrasonic solvent cleaning", "nanofeature stamps", "electron beam lithography (ebl)", "negative tone electron sensitive resist", "hydrogen silsesquioxane (hsq)", "al discharge layer", "e-beam deflection", "charge build-up"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Copper electro-chemical deposition (ECD) of through silicon via (TSV) is a key challenge of 3D integration. This paper presents a numerical modeling of TSV filling concerning the influence of the accelerator and the suppressor. The diffusion–adsorption model was used in the simulation and effects of the additives were incorporated in the model. The boundary conditions were derived from a set of experimental Tafel curves with different concentrations of additives, which provided a quick and accurate way for copper ECD process prediction without complicated surface kinetic parameters fitting. The level set method (LSM) was employed to track the copper and electrolyte interface. The simulation results were in good agreement with the experiments. For a given feature size, the current density for superfilling could be predicted, which provided a guideline for ECD process optimization.", "ground_truth": ["copper electro-chemical deposition (ecd) of through silicon via (tsv)", "3d integration", "numerical modeling of tsv filling", "accelerator and the suppressor.", "diffusion–adsorption model", "level set method", "track the copper and electrolyte interface", "set of experimental tafel curves", "copper ecd process prediction", "copper electro-chemical deposition", "ecd", "through silicon via", "tsv", "tsv filling", "simulation and effects of the additives", "additives", "lsm", "ecd process optimization", "simulation", "superfilling", "additives"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "copper electro-chemical deposition (ecd)", "through silicon via (tsv)", "3d integration", "numerical modeling", "tsv filling", "accelerator", "suppressor", "diffusion–adsorption model", "additives", "boundary conditions", "experimental tafel curves", "process prediction", "surface kinetic parameters", "level set method (lsm)", "copper-electrolyte interface"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "To restrict pollen tube growth to a single focal plane is an important subject to enable their accurate growth analysis under microscopic observation. In the conventional method to assay pollen tube growth, the pollen tubes grow in a disorderly manner on solid medium, rendering it impossible to observe their growth in detail. Here, we present a new method to assay pollen tube growth using poly-dimethylsiloxane microchannel device to isolate individual pollen tubes. The growth of the pollen tube is confined to the microchannel and to the same focal plane, allowing accurate microscopic observations. This methodology has the potential for analyses of pollen tube growth in microfluidic environments in response to chemical products and signaling molecules, which paves the way for various experiments on plant reproduction.", "ground_truth": ["restrict pollen tube growth to a single focal plane", "growth analysis under microscopic observation", "assay pollen tube growth", "", "observe their growth in detail.", "accurate microscopic observations", "pollen tube growth", "pollen tubes grow", "assay pollen tube", "assay pollen tube growth", "assay pollen tube", "poly-dimethylsiloxane microchannel device", "isolate individual pollen tubes", "pollen tubes", "new method to assay pollen tube growth", "pollen tube", "pollen tube growth", "pollen tube", "signaling molecules", "chemical products", "plant reproduction", "analyses of pollen tube growth", "experiments on plant reproduction"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "pollen tube growth", "focal plane", "microscopic observation", "conventional method", "solid medium", "poly-dimethylsiloxane (pdms)", "microchannel device", "isolate individual pollen tubes", "confined growth", "accurate observations", "microfluidic environments", "chemical products", "signaling molecules", "plant reproduction", "let me know if you'd like any refinements or additional details!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In order to study the mechanical behavior of metal films on compliant polymer substrates, fragmentation testing is often employed [8–12]. During fragmentation testing, the film-substrate couple is strained under uni-axial tension and observed with light microscopy (LM) or scanning electron microscopy (SEM). Brittle metals or ceramic films fracture, forming through thickness cracks (channel cracks) at low strain perpendicular to the straining direction. On the other hand, ductile metal films will first deform locally in the form of necks at low strains (Fig. 1a) and with increased strain through thickness cracks (TTC) can evolve (Fig. 1b). Fragmentation testing is best performed in-situ with LM or SEM so that the strain when the first crack forms can be observed. The initial fracture strain of the film, also known as the crack onset strain, can then be used to determine the interfacial fracture shear stress with knowledge of the crack spacing at saturation, λ, film thickness, h, and the fracture stress, σf=Efilmεf, where εf is the fracture strain, using the shear lag model [8,13,14]. In-situ fragmentation testing with LM or SEM allows for the crack spacing evolution to be observed as a function of applied strain (Fig. 1c). Under tensile straining conditions, a brittle film will initially fracture at very low strains (<1%) and then with further strain continue to form cracks until the saturation crack spacing is reached. After the saturation spacing has been reached, cracks can no longer form between existing crack fragments and the film could delaminate via buckling.", "ground_truth": ["metal films", "polymer substrates", "fragmentation testing", "fragmentation testing", "mechanical behavior", "film-substrate couple", "strained under uni-axial tension", "light microscopy", "scanning electron microscopy", "brittle metals", "ceramic films", "metal films", "deform locally", "fragmentation testing", "lm", "sem", "σf=efilmεf", "shear lag model", "fracture strain", "study the mechanical behavior", "strain", "performed in-situ with lm or sem", "shear stress", "crack spacing evolution", "function of applied strain", "tensile straining", "brittle film", "form cracks", "buckling", "delaminate", "lm", "sem", "lm", "sem", "fracture", "film", "film", "fracture strain", "fracture"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "mechanical behavior", "metal films", "compliant polymer substrates", "fragmentation testing", "uni-axial tension", "light microscopy (lm)", "scanning electron microscopy (sem)", "brittle metals", "ceramic films", "through thickness cracks", "channel cracks", "ductile metal films", "necks", "in-situ", "crack onset strain", "interfacial fracture shear stress", "crack spacing", "saturation", "film thickness", "fracture stress", "shear lag model", "crack spacing evolution", "tensile straining", "saturation crack spacing", "delamination"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "PDMS (Polydimethylsiloxane) has become by far the most popular material in the academic microfluidics community because it is inexpensive, easy to fabricate by replication of molds made using rapid prototyping or other techniques, flexible, optically transparent, biocompatible and its fabrication does not require high capital investment and cleanroom conditions. Various techniques have been adapted to fabricate microfluidic structures in PDMS, including wet and dry etching [20–22], photolithographic patterning of a photosensitive PDMS [23], and laser ablation [24]. But, it was the “soft-lithography” techniques [25] introduced by Whitesides et al. that enabled the widespread use of PDMS and opened up the era of PDMS-based microfluidics in the late 1990s. Replica molding, which is the casting of prepolymer against a master and generating a replica of the master in PDMS, has become a standard fabrication technique available in almost every research laboratory. Detailed overviews of soft-lithography techniques and their applications can be found from the reviews by McDonald et al. [26] and Sia et al. [27]. Nowadays, many tools dedicated for this purpose are available and can be purchased as a complete set (e.g. SoftLithoBox® provided by Elveflow (USA) [28]). Moreover, companies, such as FlowJEM (Canada) [29], Microfluidic Innovations (USA) [30], and Scientific Device Laboratory (USA) [31] provide rapid prototyping service for PDMS-based LOC devices.", "ground_truth": ["pdms", "polydimethylsiloxane", "replication of molds", "rapid prototyping", "fabricate microfluidic structures", "pdms", "wet and dry etching", "photolithographic patterning", "laser ablation", "“soft-lithography” techniques", "pdms", "pdms-based microfluidics", "replica molding", "casting of prepolymer against a master and generating a replica of the master in pdms", "softlithobox®", "fabricate", "soft-lithography techniques", "pdms-based loc devices", "rapid prototyping service", "microfluidics", "microfluidic", "pdms", "microfluidics", "pdms", "fabrication technique", "prepolymer", "pdms"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "pdms (polydimethylsiloxane)", "microfluidics", "academic community", "inexpensive", "easy to fabricate", "replication of molds", "rapid prototyping", "flexible", "optically transparent", "biocompatible", "low capital investment", "cleanroom conditions", "wet etching", "dry etching", "photolithographic patterning", "photosensitive pdms", "laser ablation", "soft-lithography", "whitesides et al.", "replica molding", "prepolymer", "standard fabrication technique"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Unlike conventional materials used in nerve tissue engineering, PAs can be directly injected in vivo into models and spontaneously self-assemble into nanofibers in aqueous solutions. Furthermore, PAs can function as biomimetic materials exemplified by collagen-mimetic PAs [92]. Conventional materials often rely on electrospinning as a manufacturing method to achieve fiber-like structures suitable for use in nerve regeneration. The self-assembly nature of PAs allows them to circumvent costly manufacturing methods. However, in contrast to conventional manufacturing methods like electrospinning where quality and batch-to-batch variability can be tightly controlled, merely relying on self-assembly as a method of large-scale commercial production is still an experimental concept. Perhaps the next step would be to carefully compare and contrast the robustness of self-assemled PAs to electrospun nanofibers. Given that the constituent elements in PAs and external factors like pH can affect its structural assembly, parameters must be finely tuned and optimized in order for PA nanofibers to be used as a full-fledged commercialized medical product [93].", "ground_truth": ["pas", "injected in vivo into models and spontaneously self-assemble into nanofibers in aqueous solutions", "pas", "biomimetic materials", "collagen-mimetic pas", "electrospinning", "a manufacturing method", "achieve fiber-like structures", "pas", "costly manufacturing methods", "conventional manufacturing methods", "electrospinning", "self-assembly", "large-scale commercial production", "compare and contrast", "robustness of self-assemled pas to electrospun nanofibers", "pas", "tuned and optimized", "pa nanofibers", "a full-fledged commercialized medical product"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "nerve tissue engineering", "pas (peptide amphiphiles)", "in vivo injection", "self-assembly", "nanofibers", "aqueous solutions", "biomimetic materials", "collagen-mimetic pas", "conventional materials", "electrospinning", "manufacturing method", "nerve regeneration", "cost-effective", "batch-to-batch variability", "large-scale commercial production"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Mice bearing the orthotopic model were treated starting from day 21 after NB cell implant; mice with the pseudo-metastatic model received the first treatment 4h after NB cell injection. These therapeutic schedules were designed to test the effects of our targeted formulations against both established and pseudo-metastatic preclinical models of human NB, as described [16,19]. Animals were treated i.v. once a week for 3 weeks with untargeted (SL[DXR]) or peptide-targeted SL[DXR] (5mg/kg). Scrambled peptide-functionalized liposomes were used as a control, and in every experiment a group of control mice received HEPES-buffered saline. Survival times were used as the main criterion for determining treatment efficacy. In the orthotopic model, time-dependent anti-tumor activity was also evaluated by bioluminescence imaging (BLI) and X-ray analyses. For this purpose, the GI-LI-N cell line was infected with a retrovirus expressing the firefly luciferase gene, as previously reported [17]; luciferase activity of retrovirally-transduced cells was visualized in vivo by BLI (IVIS Caliper Life Sciences, Hopkinton, MA) after a 10min incubation with 150μg/mL of d-luciferin (Caliper Life Sciences), as described [17]. X-ray analysis was superimposed to the luminescence for a better visualization of the tumors.", "ground_truth": ["mice bearing the orthotopic model", "mice with the pseudo-metastatic model", "test the effects", "our targeted formulations", "these therapeutic schedules", "preclinical models", "animals", "untargeted", "peptide-targeted sl[dxr]", "hepes-buffered saline", "scrambled peptide-functionalized liposomes", "survival times", "determining treatment efficacy", "time-dependent anti-tumor activity", "orthotopic model", "d-luciferin", "the gi-li-n cell line", "infected", "visualized", "luciferase activity", "retrovirally-transduced cells", "sl[dxr]"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "mice", "orthotopic model", "nb cell implant", "pseudo-metastatic model", "therapeutic schedules", "targeted formulations", "preclinical models", "human nb", "animals", "intravenous (i.v.)", "untargeted sl[dxr]", "peptide-targeted sl[dxr]", "scrambled peptide-functionalized liposomes", "control", "hepes-buffered saline", "survival times", "treatment efficacy", "anti-tumor activity", "bioluminescence imaging (bli)", "x"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A limitation of the pharmacyte approach is the one-time nature of the intervention: ACT T-cells can only be loaded once with a cargo of adjuvant drug prior to transfer, and the duration of stimulation is inherently limited by expansion of the cell population in vivo, since cell-bound particles are diluted with each cell division. We hypothesized that a strategy to target supporting drugs to T-cells with nanoparticle drug carriers directly in vivo would enable transferred lymphocytes to be repeatedly stimulated with supporting adjuvant drugs, and thereby provide continuous supporting signals over the prolonged durations that might be necessary for elimination of large tumor burdens. Such “re-arming” of T-cells with supporting drugs could be achieved by repeated administration of targeted particles, allowing adoptively-transferred T-cells to be restimulated multiple times directly in vivo, while the use of internalizing targeting ligands would minimize the likelihood of immune responses against the nanoparticle carrier. To our knowledge, only two prior studies have attempted to target nanoparticles to T-cells in vivo [17,18]. In both of these studies, particles were targeted to T-cells via peptide-MHC ligands that bind to specific T-cell receptors. However, peptide-MHC-functionalized nanoparticles have recently been shown to deliver an anergizing/tolerizing signal to T-cells [18,19] — which is ideal for treating graft rejection or autoimmunity, but runs counter to the goals of cancer immunotherapy.", "ground_truth": ["pharmacyte approach", "act t-cells", "adjuvant drug", "stimulation", "expansion of the cell population in vivo", "cell-bound particles", "cell division", "cell", "supporting drugs", "t-cells", "nanoparticle drug carriers", "transferred lymphocytes", "supporting adjuvant drugs", "elimination of large tumor burdens", "“re-arming” of t-cells with supporting drugs", "t-cells", "supporting drugs", "targeted particles", "administration of targeted particles", "adoptively-transferred t-cells", "internalizing targeting ligands", "nanoparticle carrier", "nanoparticles", "t-cells", "t-cells", "peptide-mhc ligands", "t-cell receptors", "peptide-mhc-functionalized nanoparticles", "t-cells", "treating graft rejection or autoimmunity", "cancer immunotherapy"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "pharmacyte approach", "one-time intervention", "act t-cells", "cargo of adjuvant drug", "cell-bound particles", "cell division", "nanoparticle drug carriers", "in vivo targeting", "repeated stimulation", "supporting adjuvant drugs", "continuous supporting signals", "large tumor burdens", "re-arming t-cells", "internalizing targeting ligands", "immune responses", "nanop"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The α-ω-aminohexylcarbamate derivative of cyanocobalamin was prepared using a method described previously [18]. Briefly, solid CDI (260mg, 0.32mmol) was added to cyanocobalamin (1.0g, 0.148mmol) previously dissolved in anhydrous dimethyl sulfoxide. The mixture was stirred for up to 2h at 30°C, followed by the addition of dry 1,6-hexanediamine (314mg, 0.54mmol) and stirring of the mixture at room temperature over 24h. The mixture was poured into ethyl acetate (30ml) and left to stand. Following centrifugation and decanting of the supernatant, the residue was sonicated for 5min in acetone (50ml). The resulting precipitate was filtered and the solid washed in acetone. The crude product was purified by silica column chromatography (45% v/v 2-propanol, 30% v/v n-butanol, 2% v/v ammonia and 25% v/v water) followed by lyophilisation.", "ground_truth": ["α-ω-aminohexylcarbamate derivative of cyanocobalamin", "solid cdi", "cyanocobalamin", "anhydrous dimethyl sulfoxide", "dry 1", "6-hexanediamine", "ethyl acetate", "centrifugation", "decanting", "supernatant", "residue", "acetone", "precipitate", "solid", "acetone", "crude product", "silica column chromatography", "propanol", "butanol", "ammonia", "water", "lyophilisation"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "α-ω-aminohexylcarbamate", "cyanocobalamin", "cdi (carbonyldiimidazole)", "anhydrous dimethyl sulfoxide", "1", "6-hexanediamine", "ethyl acetate", "centrifugation", "sonication", "acetone", "silica column chromatography", "2-propanol", "n-butanol", "ammonia", "water", "lyophilisation", "derivative", "synthesis", "purification", "these keywords capture the"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Immunopotentiators activate innate immunity directly (for example, cytokines) or through pattern-recognition receptors (PRRs, such as those for bacterial components). The Toll-like receptors (TLRs) are a family of PRRs that are an important link between innate and adaptive immunity. Some studies have shown that TLR ligands have adjuvant activity and enhance antigen-specific antibody and cell-mediated immune responses, especially when they are combined with delivery systems that promote their uptake and delivery into antigen-presenting cells [22–24]. For clinical studies, TLR9 is generally stimulated with synthetic oligodeoxynucleotides containing one or more unmethylated CpG dinucleotides. In humans, CpG has been used as an adjuvant for infectious disease vaccination [25,26] and in the development of cancer therapy [27]. In a mouse model, CpG has also been shown to induce T helper 1 (Th1) immune responses, which are characterized by the production of IFN-γ and the generation of IgG2a [28,29]. Moreover, a previous study had demonstrated that different liposomes with CpG ODN significantly increased Th1-biased cytokines and augmented cell mediated immune response [30].", "ground_truth": ["immunopotentiators", "activate innate immunity", "pattern-recognition receptors", "prrs", "toll-like receptors", "tlrs", "prrs", "tlr ligands", "antigen-presenting cells", "tlr9", "clinical studies", "stimulated", "synthetic oligodeoxynucleotides", "unmethylated cpg dinucleotides", "cpg", "adjuvant", "infectious disease vaccination", "e development of cancer therapy", "mouse model", "cpg", "induce t helper 1", "production of ifn-γ and the generation of igg2a", "liposomes", "th1-biased cytokines", "augmented cell mediated immune response"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "immunopotentiators", "innate immunity", "cytokines", "pattern-recognition receptors (prrs)", "toll-like receptors (tlrs)", "adaptive immunity", "tlr ligands", "adjuvant activity", "antigen-specific antibody", "cell-mediated immune responses", "delivery systems", "antigen-presenting cells", "clinical studies", "tlr9", "synthetic oligodeoxynucleotides", "unmethylated cpg dinucleotides", "infectious disease vaccination", "cancer therapy", "mouse model", "t helper 1 (th1"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Two methods of formulating anionic nanocomplexes were evaluated. In both, nanocomplexes were prepared in water at a range of molar charge ratios of L to D while the peptide P to D molar charge ratio was maintained constant at 3:1. Method 1 (L:D:P): DNA was first added to an anionic liposome (LA, LAP1 or LAP2) and incubated for 15min at room temperature and then the peptide was added with rapid mixing and incubated at room temperature for a further 20min; Method 2 (P:D:L): the peptide was added to the DNA and incubated for 15min at room temperature and then liposome was added with rapid mixing and incubated at room temperature for a further 20min. Irrespective of the method of order of mixing, all molar charge ratios in this study refer to L:P:D. Cationic formulations LPD and LCPRGPD were prepared in the order L:P:D as described previously; first, the peptide was added to the liposome DOTMA/DOPE or LCPRG followed by addition of the DNA with rapid mixing and incubated for 30min at room temperature to allow for complex formation [30]. The nanocomplexes prepared were termed LPD (liposome DOTMA/DOPE), LADP and PDLA (liposome LA), PDLAP1 (liposome LAP1), PDLAP2 (liposome LAP2), PDLAPRG (liposome LAPRG) and LCPRGPD (liposome LCPRG).", "ground_truth": ["formulating anionic nanocomplexes", "nanocomplexes", "prepared", "an anionic liposome", "added", "incubated", "la", "lap1 or lap2", "added", "incubated", "dna", "peptide", "added", "incubated", "added", "liposome", "incubated", "lpd", "lcprgpd", "added", "dotma", "dope", "lcprg", "dna", "rapid mixing and incubated", "lpd", "liposome dotma/dope", "ladp", "pdla", "liposome la", "pdlap1", "liposome lap1", "pdlap2", "liposome lap2", "pdlaprg", "liposome laprg", "lcprgpd", "liposome lcprg"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "anionic nanocomplexes", "formulating methods", "molar charge ratios", "l to d", "peptide p to d", "3:1 ratio", "method 1 (l:d:p)", "dna", "anionic liposome", "la", "lap1", "lap2", "incubation", "room temperature", "rapid mixing", "method 2 (p:d:l)", "order of mixing", "cation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The crack band approach for producing mesh independent load–displacement curves for fracture in plain concrete is based on the idea that the crack opening is transformed into inelastic strain by distributing it over an element length dependent zone [5]. This approach will only produce mesh independent load–displacement curves, if the inelastic strain profiles in the finite element analysis are mesh size dependent. This requirement is an important difference to the nonlocal model which is designed to produce both mesh size independent load–displacement curves and strain profiles. In CDPM2, the crack band approach is applied only to the tensile part of the damage algorithm by replacing the stress–inelastic strain law shown in Fig. 2(b) by a stress–inelastic displacement law of the form(13)σ=ftexp(−ϵinhwft)if(ϵin>0)Here, wft is a crack opening threshold used to control the slope of the softening curve and h is the width of the crack-band, which in the present study is equal to the maximum dimension of the element along the principal direction of the strain tensor corresponding to the maximum tensile principal strain at the onset of damage. For the compressive part, a stress–inelastic strain law was used to determine the compressive damage parameter, since it was reported in [14] for columns subjected to eccentric compression that inelastic strain profiles in compression do not exhibit a mesh dependence which would satisfy the assumptions of the crack-band approach. This approach of applying the crack-band approach only to the tensile part has already been successfully used in Grassl et al. [16].", "ground_truth": ["crack band approach", "crack band approach", "stress–inelastic strain law", "inelastic strain profiles", "crack-band approach", "tensile part", "tensile part", "stress–inelastic strain law", "stress–inelastic displacement law", "(13)σ=ftexp(−ϵinhwft)if(ϵin>0)"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "crack band approach", "mesh independent", "load–displacement curves", "fracture", "plain concrete", "crack opening", "inelastic strain", "element length dependent zone", "finite element analysis", "mesh size dependent", "nonlocal model", "cdpm2", "tensile damage algorithm", "stress–inelastic strain law", "stress–inelastic displacement law"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "HOMO–LUMO energy band gaps between ylides and their pyrene adducts propose that the 1,3-DC of second pyridinium ylides to ylidepyrene adducts are HOMOylide–LUMOylide–pyrene controlled since the energy band gap is smaller than HOMOylide–pyrene–LUMOylide. Regioselectivity of second cycloaddition was predicted using the atomic orbital coefficients corresponding to HOMOylide–LUMOylide–pyrene. According to Fukui [33], reactions can be favorable in the direction of maximal HOMO–LUMO overlapping of larger coefficients at the reactive sites. The most favorable interactions between corresponding ylides and ylidepyrene adducts to form the most favorable regioisomer conformation are given in Fig. 3. Second ylide addition to ylidepyrene structure is therefore anticipated to proceed via ylideC2/C6–ylidepyrene-C3 and ylideC7–ylidepyrene/C2 interactions to produce the same regioisomer conformations. Considering the theoretical calculations performed for pyrrolidine attached pyrene structure, it is also expected that formation of the same type of regioisomers are favorable for SWNTs after the 1,3-DC of pyridinium ylides, Fig. 3.", "ground_truth": ["ylides", "pyrene adducts", "1", "3-dc", "second pyridinium ylides", "ylidepyrene adducts", "homoylide–pyrene–lumoylide", "regioselectivity of second cycloaddition", "atomic orbital coefficients", "homoylide–lumoylide–pyrene", "maximal homo–lumo overlapping", "ylides and ylidepyrene adducts", "most favorable regioisomer conformation", "ylide", "ylidepyrene", "ylidec2/c6–ylidepyrene-c3", "ylidec7–ylidepyrene/c2", "regioisomer conformations", "theoretical calculations", "pyrrolidine attached pyrene", "1", "3-dc of pyridinium ylides", "swnts", "formation of the same type of regioisomers", "regioisomers", "homoylide–lumoylide–pyrene", "homo–lumo", "homo–lumo"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "homo–lumo energy band gaps", "ylides", "pyrene adducts", "1", "3-dc (1", "3-dipolar cycloaddition)", "homoylide–lumoylide–pyrene", "homoylide–pyrene–lumoylide", "regioselectivity", "cycloaddition", "atomic orbital coefficients", "fukui", "homo–lumo overlapping", "reactive sites", "regioisomer conformation", "ylidec2/c"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Gamma titanium aluminides are a family of low density, high performance alloys with the potential to replace current Ni-base superalloys used in the production of aero-engine components. Investment casting is one of the most economical methods to produce titanium and titanium aluminide alloy products, increasing the components' integrity and mechanical properties, whilst reducing material waste and machining cost [1]. Titanium aluminides are difficult to process mainly due to the low fluidity of the TiAl alloy around its melting temperature [2]. Due to the high affinity of elements such as oxygen, nitrogen etc., titanium and its alloys can easily interact with mould materials during the investment casting process, resulting in an interaction hardened layer being generated at the metal surface [3,4]. This hardened layer contains a large amount of dissolved oxygen, and it is very brittle and susceptible to crack generation and propagation [5].", "ground_truth": ["gamma titanium aluminides", "alloys", "ni-base superalloys", "aero-engine components", "production of aero-engine components", "investment casting", "produce titanium and titanium aluminide alloy products", "titanium", "titanium aluminide alloy", "the components", "titanium aluminides", "tial alloy", "oxygen", "nitrogen", "titanium", "alloys", "elements", "mould materials", "casting process", "metal surface", "interaction hardened layer", "hardened layer", "dissolved oxygen", "crack generation and propagation"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "gamma titanium aluminides", "low density", "high performance alloys", "ni-base superalloys", "aero-engine components", "investment casting", "economical methods", "titanium aluminide alloy products", "components' integrity", "mechanical properties", "material waste", "machining cost", "titanium aluminides", "difficult to process", "low fluidity", "tial alloy", "melting temperature", "high affinity", "oxygen", "nitrogen", "mould materials", "interaction hardened layer", "metal surface", "dissolved oxygen"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "From this study where a commercial Al–12Si alloy was inoculated with different level of Nb+B addition to assess the grain refining potency of Nb+B inoculation it can be concluded that in-situ formed Nb-based intermetallics compounds are potent heterogeneous nucleation substrates with high potency for the refinement of Al–Si cast alloys. The primary α-Al dendritic grain size varies with the addition level of Nb and B. Moreover, significant grain refinement over a wide range of cooling rates is obtained via enhanced heterogeneous nucleation making the grain size of the material less sensitive to the cooling rate. Nb+B inoculants are characterised by some fading which is still acceptable after 4 h of contact time. Moreover, alloys refined by means of Nb+B inoculants can be recycled obtaining a fine grain structure with small addition or no further addition of inoculants after the first initial addition. Concluding, Nb+B inoculation is a promising candidate for the refinement of cast Al alloy which could lead to their wider employment in the automotive industry with the resultant intrinsic advantages of lighter structural component from an environmental point of view.", "ground_truth": ["al–12si alloy", "inoculated with different level of nb+b", "nb+b", "grain refining potency of nb+b inoculation", "nb-based intermetallics compounds", "heterogeneous nucleation substrates", "al–si cast alloys", "refinement of al–si cast alloys", "α-al dendritic grain", "nb", "b", "grain refinement", "wide range of cooling rates", "enhanced heterogeneous nucleation", "nb+b inoculants", "alloys", "nb+b inoculants", "refinement of cast al alloy", "cast al alloy", "grain", "fading", "inoculants", "fine grain structure", "nb+b inoculation", "nb+b", "lighter structural component", "nb+b inoculation", "nb+b"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "al–12si alloy", "nb+b addition", "grain refining potency", "in-situ formed nb-based intermetallics", "heterogeneous nucleation substrates", "al–si cast alloys", "primary α-al dendritic grain size", "cooling rates", "grain refinement", "fading", "contact time", "recycling", "fine grain structure", "automotive industry", "lighter structural components"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The observed conductivity of A2FeMoO6–δ (A = Ca, Sr, Ba) [7] was linked to a potential double exchange mechanism, with conduction between Fe3+-O-Mo-O-Fe2+. Double-exchange mechanisms, as proposed by Zener [23], posit that electron transfer between ions in different oxidation states may be facilitated if the electron does not have to alter its spin state. Replacement of Mo with Fe in this mechanism would be expected to result in a reduction of the conductivity through reduction of the available percolation pathways, unless delocalisation of Fe electrons through Fe2+-O-Fe3+ exchange could also occur. Double exchange mechanisms have been observed previously for mixed valent iron in iron oxides [24], and, as iron is known to exist in a mixed valent state for Ca2–xSrxFeMoO6–δ [25], this provides a plausible explanation for the observed metallic conductivity. Band structure calculations and Mossbauer spectroscopy could be utilised to further elucidate the conduction mechanism for these compounds, however this is outside the scope of this enquiry.", "ground_truth": ["observed conductivity", "a2femoo6–δ", "fe3+-o-mo-o-fe2+", "double exchange mechanism", "double-exchange mechanisms", "electron transfer", "ions", "electron", "mo", "fe", "replacement", "reduction of the conductivity", "percolation pathways", "delocalisation of fe electrons", "fe electrons", "fe2+-o-fe3+", "fe2+-o-fe3+ exchange", "double exchange mechanisms", "iron in iron oxides", "iron", "ca2–xsrxfemoo6–δ", "metallic conductivity", "band structure calculations", "mossbauer spectroscopy", "conduction mechanism", "oxidation", "compounds", "ca", "sr", "ba", "electron", "reduction of the available percolation pathways"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "conductivity", "a2femoo6–δ", "ca", "sr", "ba", "double exchange mechanism", "fe3+-o-mo-o-fe2+", "zener", "electron transfer", "oxidation states", "spin state", "mo", "fe", "percolation pathways", "delocalisation", "fe2+-o-fe3+ exchange", "mixed valent iron", "iron oxides", "ca2–xsrxfemoo6–δ", "metallic conductivity", "band structure calculations"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "To address the vertical displacement estimation of conventional pile groups subjected to mechanical loads, various numerical and analytical methods have been proposed. These methods include the finite element method [e.g., 2,3], the boundary element method [e.g., 4,5], the finite difference method [e.g., 6], the interaction factor method [e.g., 7,8–11], the equivalent pier and raft methods [e.g., 12–14], and the settlement ratio method [e.g., 15]. The finite element method, while providing the most rigorous and exhaustive representation of the pile group-related problem, is generally computationally expensive and considered mainly a research tool rather than a design tool. Conversely, the versatility of simplified (approximate) methods, such as the interaction factor approach that allows capturing the (e.g., vertical) displacements of any general pile group by the analysis of the displacement interaction between two identical piles and by the use of the elastic principle of superposition of effects, makes them attractive as design tools because they allow for the use of expedient parametric studies under various design conditions.", "ground_truth": ["vertical displacement estimation of conventional pile groups subjected to mechanical loads", "numerical and analytical methods", "finite element method", "boundary element method", "finite difference method", "interaction factor method", "equivalent pier and raft methods", "settlement ratio method", "finite element method", "pile group-related problem", "research tool", "design tool", "versatility of simplified (approximate) methods", "interaction factor approach", "capturing the (e.g.", "vertical) displacements of any general pile group", "analysis of the displacement interaction between two identical piles", "elastic principle of superposition of effects", "design tools", "expedient parametric studies"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "vertical displacement estimation", "conventional pile groups", "mechanical loads", "numerical methods", "analytical methods", "finite element method (fem)", "boundary element method (bem)", "finite difference method (fdm)", "interaction factor method", "equivalent pier method", "equivalent raft method", "settlement ratio method", "computationally expensive", "research tool", "design tool"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The optimised structure at the B3LYP/aug-cc-pVTZ level was then used to perform calculations of the lowest electronic singlet excited states with the coupled cluster linear response (LR) coupled cluster hierarchy CCS, CC2, CCSD and CC3, along with perturbative corrected methods CIS(D) and CCSDR(3). The correlated response methods were performed with an all-electron atomic natural orbital (ANO) basis set contracted to 6s5p4d3f1g on manganese, [47] together with the cc-pVTZ basis set on the oxygen atoms. The all-electron correlated calculations invoked a 13 orbital frozen core (O 1s, Mn 1s2s2p3s3p). Trial calculations correlating these orbitals only had a minor effect on excitation energies. For comparison the EOM-CCSD method with the cc-pVTZ basis on all atoms was tested to compare with LR-CCSD. These formally give exactly the same excitation energies, although the transition moments are more accurate for LR-CCSD. Abelian symmetry (D2) was used in all correlated excited state calculations.", "ground_truth": ["b3lyp/aug-cc-pvtz", "lowest electronic singlet", "linear response", "lr", "coupled cluster hierarchy", "perturbative corrected methods", "correlated response methods", "atomic natural orbital", "ano", "manganese", "oxygen", "6s5p4d3f1g", "cc-pvtz", "all-electron correlated calculations", "o", "mn", "excitation energies", "eom-ccsd method", "cc-pvtz", "lr-ccsd", "excitation energies", "transition moments", "lr-ccsd", "abelian symmetry", "excited state calculations", "optimised structure", "d2", "ccs", "cc2", "ccsd", "cc3", "cis(d)", "ccsdr(3)"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "optimised structure", "b3lyp", "aug-cc-pvtz", "electronic singlet excited states", "coupled cluster linear response (lr)", "coupled cluster hierarchy", "ccs", "cc2", "ccsd", "cc3", "perturbative corrected methods", "cis(d)", "ccsdr(3)", "correlated response methods", "all-electron atomic natural orbital (ano) basis set", "6s5p4d3f1g", "manganese", "cc-pvtz basis set", "oxygen atoms"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Arrays of TFTs and circuits were fabricated on precleaned, 5cm×5cm, 125μm thick polyethylene naphthalate (PEN) substrates (Dupont-Teijin). Full details of our vacuum-fabrication procedures have been given in previous publications [17–19,23]. Briefly, aluminium gate electrodes and associated tracks were vacuum evaporated onto the substrates through shadow masks. Subsequently, the substrates were attached to a cooled web-coater drum (Aerre Machines). With the drum rotating at a linear speed of 25m/min under vacuum, flash-evaporated TPGDA monomer vapour which condensed onto the substrates was cross-linked by exposure, in situ, to a plasma. The resulting smooth, pinhole-free films were typically 500nm to 1μm thick with a measured dielectric constant varying in the range 4–5. For circuit fabrication, the insulator was patterned using shadow masks to define rectangular areas separated by 1mm gaps to act as vias for inter-layer metallic connections. The substrates were then transferred into an evaporator (Minispectros, Kurt Lesker) integrated into a nitrogen glovebox for the vacuum-deposition (2.4nm/min) of DNTT onto the insulator. Without exposing the substrates to ambient air, the gold source/drain metallisation layer was deposited through a shadow mask in the same evaporator.", "ground_truth": ["tfts", "circuits", "polyethylene naphthalate", "pen", "polyethylene naphthalate (pen) substrates", "vacuum-fabrication procedures", "aluminium gate electrodes", "tracks", "vacuum evaporated", "substrates", "shadow masks", "substrates", "web-coater drum", "drum", "flash-evaporated tpgda monomer vapour", "substrates", "plasma", "cross-linked", "exposure", "films", "circuit fabrication", "insulator", "shadow masks", "inter-layer metallic connections", "substrates", "evaporator", "nitrogen glovebox", "vacuum", "vacuum-deposition", "insulator", "substrates", "ambient air", "gold source/drain metallisation layer", "shadow mask", "evaporator"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "arrays", "tfts (thin-film transistors)", "circuits", "fabricated", "precleaned", "polyethylene naphthalate (pen)", "substrates", "dupont-teijin", "vacuum-fabrication procedures", "aluminium gate electrodes", "tracks", "vacuum evaporated", "shadow masks", "cooled web-coater drum", "aerre machines", "linear speed", "flash-evaporated", "tpgda monomer vapour", "cross-linked", "plasma", "p"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "For decades, vibronic coupling models [1–4] have served as bridges connecting nuclear dynamics studies with the static studies of electronic structure calculations [5]. The vibronic coupling model is a simple polynomial expansion of diabatic potential energy surfaces and couplings. The expansion coefficients are chosen so that the eigenvalues of the potential operator map on to the adiabatic potential surfaces. This diabatisation by ansatz circumvents many of the problems of describing non-adiabatic systems. It is also the inspiration for a diabatisation scheme that is used in modern, direct-dynamic methods that include non-adiabatic effects [6]. For a model Hamiltonian to correctly approximate the eigenvectors of the true Hamiltonian it has to span the totally symmetric irreducible representation (IrRep) of the point groups the molecule belongs to, at the appropriate symmetric geometries [7]. In recent times, many articles have demonstrated the advantages of using symmetry when constructing analytic model potentials [8–12], most often in the context of permutation-inversion groups [13].", "ground_truth": ["vibronic coupling models", "nuclear dynamics studies", "electronic structure calculations", "vibronic coupling model", "polynomial expansion", "diabatic potential energy surfaces and couplings", "expansion coefficients", "diabatisation", "non-adiabatic systems", "diabatisation scheme", "direct-dynamic methods", "irreducible representation", "irrep", "symmetric geometries", "analytic model potentials", "model hamiltonian", "true hamiltonian", "permutation-inversion groups"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "vibronic coupling models", "nuclear dynamics", "electronic structure calculations", "diabatic potential energy surfaces", "couplings", "polynomial expansion", "expansion coefficients", "adiabatic potential surfaces", "diabatisation by ansatz", "non-adiabatic systems", "direct-dynamic methods", "model hamiltonian", "true hamiltonian", "totally symmetric irreducible representation (irrep)", "point groups", "symmetric geometries", "symmetry", "analytic model potentials", "permutation-inversion groups"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the present work, a LIF technique is applied for investigation of gas-sheared film flow in horizontal rectangular duct. The technique makes it possible to perform field measurements of local film thickness, resolved in both space and time, similar to the work of Alekseenko et al. (2009). The flat shape and large transverse size of the duct allow us to resolve the film thickness in transverse coordinate as well. Alekseenko et al. (2012) attempted to do this in annular downward flow, but, for technical reasons, the sampling frequency was not high enough in their experiments. More recently Alekseenko et al. (2014a) showed that the LIF technique can also detect entrained droplets. The technique allows the simultaneous study of three-dimensional wavy structures and liquid entrainment, and can improve understanding of the entrainment phenomenon.", "ground_truth": ["lif technique", "gas-sheared film flow", "field measurements of local film thickness", "duct", "duct", "resolve the film thickness", "annular downward flow", "sampling frequency", "lif technique", "detect entrained droplets", "three-dimensional wavy structures", "liquid entrainment", "improve understanding of the entrainment phenomenon", "film flow", "film", "film", "film"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "lif technique", "gas-sheared film flow", "horizontal rectangular duct", "field measurements", "local film thickness", "space and time resolution", "transverse coordinate", "annular downward flow", "sampling frequency", "entrained droplets", "three-dimensional wavy structures", "liquid entrainment", "entrainment phenomenon", "alekseenko et al. (2009", "2012"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In general, liquid film flows of practical relevance are turbulent and, hence, are associated with the presence of broadband interfacial waves on the film surface. A thorough understanding of the characteristic profiles, scales and dynamics of these interfacial waves is of essential importance in making accurate and reliable predictions of heat and mass transfer rates (Mathie and Markides, 2013a; Mathie et al., 2013). Previous efforts in downwards annular flow have focused on the spatio/temporal measurement of liquid film thickness, followed by in-depth statistical analyses of this film thickness (Webb and Hewitt, 1975; Belt et al., 2010; Alekseenko et al., 2012; Zhao et al., 2013). These efforts have contributed to a much improved understanding of the interfacial topology observed in downwards annular flows and also to the subsequent proposal of a series of correlations for the quantification of the mean film thickness, wave amplitudes and liquid entrainment rates into the gas phase (Ambrosini et al., 1991; Karapantsios and Karabelas, 1995; Azzopardi, 1997). On the other hand, less has been published on the velocity distribution and the flow structure within the liquid films, underneath the film surface. This can be related to the relative difficulty of these measurements caused by: (i) the extremely restricted measurement space, due to the small thickness of the liquid films (in the order of and often sub-mm), (ii) the highly disturbed and intermittent nature of the gas–liquid interface, (iii) the entrainment of gas inside the liquid film and of liquid into the gas core, and (iv) the relatively high velocities of both the gas and liquid phases.", "ground_truth": ["liquid film flows", "liquid film", "broadband interfacial waves", "interfacial waves", "downwards annular flow", "spatio/temporal measurement", "liquid film thickness", "liquid film", "statistical analyses", "interfacial topology", "downwards annular flows", "mean film thickness", "wave amplitudes", "velocity distribution", "flow structure within the liquid films", "liquid films", "thickness of the liquid films", "gas–liquid interface", "entrainment of gas", "high velocities", "liquid film", "film surface", "understanding of the characteristic profiles", "scales and dynamics of these interfacial waves", "film", "statistical analyses of this film thickness", "film", "wave", "liquid", "gas phase", "gas", "film surface", "quantification of the mean film thickness", "wave amplitudes and liquid entrainment rates", "liquid films", "gas", "liquid", "gas core", "gas", "liquid"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "liquid film flows", "turbulent", "broadband interfacial waves", "film surface", "characteristic profiles", "scales", "dynamics", "heat transfer rates", "mass transfer rates", "downwards annular flow", "spatio/temporal measurement", "liquid film thickness", "statistical analyses", "interfacial topology", "mean film thickness", "wave amplitudes", "liquid entrainment rates", "gas phase", "velocity distribution", "flow structure", "measurement space", "gas–liquid interface", "entrainment", "gas core", "high velocities"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "There is also a lack of agreement as to what constitutes churn flow. It is fairly certainly a gas continuous flow. There is growing agreement that there are huge waves present and some of the liquid is carried as drops. Sekoguchi and Mori (1997) and Sawai et al. (2004) using measurements from their multiple probes (92 over an axial length of 2.325 m) obtained time/axial position/void fraction information. From this they were able to identify huge wave from amongst disturbance waves and slugs. They classified individual structures as huge waves from their size together with the fact that their velocities depended significantly on the corresponding axial length. This was in contrast to disturbance waves where the velocity of individual waves only increased slightly with the axial extent of these waves. They also found that the frequency of huge waves first increased and then decrease with increasing gas superficial velocity. Similarly, their velocities were found to deviate from the line for slug flow velocities and pass through a maximum and then a minimum.", "ground_truth": ["churn flow", "probes", "time/axial position/void fraction information", "identify huge wave", "disturbance waves", "disturbance waves", "velocity of individual waves", "frequency of huge waves", "gas superficial velocity", "slug flow velocities", "gas continuous flow", "gas", "churn", "huge waves", "liquid", "huge wave", "slugs", "huge waves", "waves", "waves", "huge waves", "gas", "slug"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "churn flow", "gas continuous flow", "huge waves", "liquid drops", "sekoguchi and mori (1997)", "sawai et al. (2004)", "multiple probes", "axial length", "time/axial position/void fraction", "disturbance waves", "slugs", "wave size", "wave velocities", "axial length dependence", "frequency of huge waves", "gas superficial velocity", "slug flow velocities", "velocity maximum", "velocity minimum"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The scheduling process we adopt matches a multiple stage stochastic programming approach. Standard two-stage stochastic programs with linear or convex functions are often solved using the L-shaped method or Bender's decomposition [44,6,7]. However, our recourse decision (scheduled cancellations) is still anticipative to further uncertainty, namely the second shift surgery durations, unavailability and cancellations. As such, the decision problem can be viewed as a three-stage recourse model [5,6]. Solving the scheduling problem is further complicated because the recourse function is integer. Laporte and Louveaux [26] propose modified L-shaped decomposition with adjusted optimal cuts for two stage stochastic program with integer recourse. Angulo et al. [1] alternately generate optimal cuts of the linear sub-problem and the integer sub-problem, which improves the practical convergence (see also [15,8]). We follow a sample average approximation approach (SAA) which uses this framework. Moreover, we prove and exploit a specific relationship between the first-stage realization and the optimal number of scheduled cancellations to speed up the computation of integer cuts. We use Jensen's inequality [17] to upper bound the minus second (and third) stage cost, a technique that was proposed by Batun et al. [3].", "ground_truth": ["scheduling process", "multiple stage stochastic programming approach", "two-stage stochastic programs", "linear or convex functions", "l-shaped method", "bender's decomposition", "recourse decision", "scheduled cancellations", "decision problem", "three-stage recourse model", "scheduling problem", "recourse function", "modified l-shaped decomposition", "two stage stochastic program", "integer recourse", "generate optimal cuts of the linear sub-problem and the integer sub-problem", "linear sub-problem", "integer sub-problem", "sample average approximation approach", "saa", "first-stage realization", "integer cuts", "jensen's inequality", "upper bound the minus second (and third) stage cost"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "scheduling process", "multiple stage stochastic programming", "two-stage stochastic programs", "linear functions", "convex functions", "l-shaped method", "bender's decomposition", "recourse decision", "scheduled cancellations", "anticipative uncertainty", "second shift surgery durations", "unavailability", "cancellations", "three-stage recourse model", "decision problem", "integer recourse", "laporte and louveaux", "modified l-shaped decomposition", "optimal cuts", "angulo et al.", "linear sub-problem", "integer sub-problem", "practical convergence"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Modeling collaboration processes is a challenging task. Existing modeling approaches are not capable of expressing the unpredictable, non-routine nature of human collaboration, which is influenced by the social context of involved collaborators. We propose a modeling approach which considers collaboration processes as the evolution of a network of collaborative documents along with a social network of collaborators. Our modeling approach, accompanied by a graphical notation and formalization, allows to capture the influence of complex social structures formed by collaborators, and therefore facilitates such activities as the discovery of socially coherent teams, social hubs, or unbiased experts. We demonstrate the applicability and expressiveness of our approach and notation, and discuss their strengths and weaknesses.", "ground_truth": ["modeling collaboration processes", "modeling approaches", "human collaboration", "evolution of a network of collaborative documents along with a social network of collaborators", "modeling approach", "graphical notation and formalization", "influence of complex social structures", "discovery of socially coherent teams", "modeling approach", "collaboration processes"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "modeling collaboration processes", "challenging task", "existing modeling approaches", "unpredictable nature", "non-routine nature", "human collaboration", "social context", "collaborators", "proposed modeling approach", "evolution of a network", "collaborative documents", "social network", "graphical notation", "formalization", "complex social structures", "socially coherent teams"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We start by outlining the motivation, structure and content of the review. It has long been known that cardiovascular signals contain a number of oscillatory components that are not exactly periodic. To put it differently, their periods (frequencies) fluctuate with time. For example, heart rate variability (HRV) has in itself provided a major topic of discussion. We introduce one of the statistical approaches to HRV in Section 3. However, in order to understand the variability of the cardiovascular system, discussion of a single source is insufficient because the cardiovascular system is composed of many different physiological components (subsystems) and it is the effects of their mutual interaction that combine to produce HRV. This is demonstrated in Section 4, revealed by results obtained using the wavelet transform. In Section 5, we discuss the cardio-respiratory interaction in terms of phase synchronization. To set the scene for these later discussions, we summarize the basic principles of phase dynamics in Section 2. For readers who are unfamiliar with the physiological aspects of the research, we provide Appendices A on the cardiovascular system and B on how measurements of cardiovascular signals are conducted. Appendix C provides details of the statistical methods used in the group data analyses.", "ground_truth": ["cardiovascular signals", "oscillatory components", "periods", "frequencies", "heart rate variability", "hrv", "statistical approaches", "introduce one of the statistical approaches to hrv", "understand the variability of the cardiovascular system", "physiological components", "subsystems", "cardiovascular system", "cardiovascular system", "hrv", "wavelet transform", "cardio-respiratory interaction", "phase synchronization", "phase dynamics", "cardiovascular system", "measurements of cardiovascular signals", "statistical methods", "group data analyses", "hrv", "discuss the cardio-respiratory interaction", "summarize the basic principles of phase dynamics", "cardiovascular signals"], "prediction_list": ["here are the extracted keywords from the paragraph:", "motivation", "structure", "content", "review", "cardiovascular signals", "oscillatory components", "periodic", "periods", "frequencies", "fluctuate", "heart rate variability (hrv)", "statistical approaches", "variability", "cardiovascular system", "physiological components", "subsystems", "mutual interaction", "wavelet transform", "cardio-respiratory interaction", "phase synchronization", "phase dynamics", "physiological aspects", "cardiovascular system (appendix a)", "measurements (appendix b)", "statistical methods (appendix c)", "group data analyses"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "By the early 1970s, and following the ‘golden age’ of general relativity that took place in the 1960s, there was a wide array of candidate theories of gravity in existence that could rival Einstein’s. A formalism was needed to deal with this great abundance of possibilities, and this was provided in the form of the Parameterised Post-Newtonian (PPN) formalism by Kenneth Nordtvedt, Kip Thorne and Clifford Will. The PPN formalism was built on the earlier work of Eddington and Dicke, and allowed for the numerous theories available at the time to be compared to cutting edge astrophysical observations such as lunar laser ranging, radio echo, and, in 1974, the Hulse–Taylor binary pulsar. The PPN formalism provided a clear structure within which one could compare and assess various theories, and has been the benchmark for how theories of gravity should be evaluated ever since. We will give an outline of the PPN formalism, and the constraints available within it today, in Section 2.", "ground_truth": ["general relativity", "theories of gravity", "parameterised post-newtonian", "ppn", "ppn formalism", "lunar laser ranging", "radio echo", "hulse–taylor binary pulsar", "ppn formalism", "compare and assess", "theories of gravity", "ppn formalism", "cutting edge astrophysical observations", "benchmark for how theories of gravity should be evaluated"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "early 1970s", "golden age", "general relativity", "1960s", "candidate theories of gravity", "einstein", "formalism", "parameterised post-newtonian (ppn) formalism", "kenneth nordtvedt", "kip thorne", "clifford will", "eddington", "dicke", "astrophysical observations", "lunar laser ranging", "radio echo", "hulse–taylor binary pulsar", "1974", "benchmark", "theories of gravity", "section 2"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Despite the ubiquity of time-dependent dynamical systems in nature, there has been relatively little work done on the analysis of time series from such systems. Mathematically they are known as non-autonomous systems, which are named as such because, unlike autonomous systems, in addition to the points in space over which they are observed they are also influenced by the points in time. Recently there has been much work on the direct ‘bottom-up’ approach to these systems, which includes the introduction of a subclass known as chronotaxic systems that are able to model the stable but time-varying frequencies of oscillations in living systems  [8,9]. In contrast, the time series analysis of these systems, referred to as the inverse or ‘top-down’ approach, has not been studied in detail before. This is partly because non-autonomous systems can still be analysed in the same way as other types of systems in both the deterministic  [10] and the stochastic  [11] regime. However, it is now argued that this type of analysis is insufficient and that an entirely new analytical framework is required to provide a more useful picture of such systems. In the case of chronotaxic systems some methods have already been developed for the inverse approach and they have shown to be useful in analysing heart rate variability  [12]. A general and dedicated procedure for analysing non-autonomous systems has still not been tackled though.", "ground_truth": ["time-dependent dynamical systems", "analysis of time series", "non-autonomous systems", "‘bottom-up’ approach", "chronotaxic systems", "model the stable but time-varying frequencies", "time series analysis", "‘top-down’", "inverse", "non-autonomous systems", "deterministic", "stochastic", "chronotaxic systems", "inverse approach", "heart rate variability", "non-autonomous systems", "autonomous systems", "living systems", "analytical framework"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "time-dependent dynamical systems", "time series analysis", "non-autonomous systems", "autonomous systems", "points in space", "points in time", "bottom-up approach", "chronotaxic systems", "stable time-varying frequencies", "oscillations", "living systems", "inverse approach", "top-down approach", "deterministic regime", "stochastic regime", "analytical framework", "heart rate variability", "general procedure", "dedicated procedure"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The purpose of this Letter is to answer the above question and to confront those six-zero textures of lepton mass matrices with the latest experimental data. First, we shall present a concise analysis of the lepton mass matrices in Table 1 and reveal their isomeric features, namely, they have the same phenomenological consequences, although their structures are apparently different. Second, we shall examine the predictions of these lepton mass matrices by comparing them with the 2σ and 3σ intervals of two neutrino mass-squared differences and three lepton flavor mixing angles,22To be specific, we make use of the 2σ and 3σ intervals of two neutrino mass-squared differences and three lepton flavor mixing angles given by M. Maltoni et al. in Ref. [5]. which are obtained from a global analysis of the latest solar, atmospheric, reactor (KamLAND and CHOOZ [10]) and accelerator (K2K) neutrino data. We find no parameter space allowed for six isomeric lepton mass matrices at the 2σ level. At the 3σ level, however, their results for neutrino masses and lepton flavor mixing angles can be compatible with current data. Third, we incorporate the seesaw mechanism and the Fukugita–Tanimoto–Yanagida hypothesis [9] in the charged lepton and Dirac neutrino mass matrices with six texture zeros. It turns out that their predictions, including θ23≈45°, are in good agreement with the present experimental data even at the 2σ level.", "ground_truth": ["confront those six-zero textures of lepton mass matrices", "lepton mass matrices", "reveal their isomeric features", "examine the predictions of these lepton mass matrices", "fukugita–tanimoto–yanagida hypothesis", "neutrino mass-squared differences", "lepton flavor mixing angles", "solar", "atmospheric", "reactor (kamland and chooz [10]) and accelerator (k2k) neutrino data", "experimental data", "lepton mass matrices", "lepton mass matrices", "neutrino mass-squared differences", "lepton flavor mixing angles", "global analysis", "isomeric lepton mass matrices", "lepton flavor mixing angles", "current data", "charged lepton", "neutrino", "dirac neutrino mass matrices", "seesaw mechanism", "experimental data"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "letter", "six-zero textures", "lepton mass matrices", "experimental data", "isomeric features", "phenomenological consequences", "neutrino mass-squared differences", "lepton flavor mixing angles", "2σ intervals", "3σ intervals", "global analysis", "solar neutrino data", "atmospheric neutrino data", "reactor neutrino data", "kamland", "chooz"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The aim of this note is nothing more than to bring both approaches on equal footing and to relax the assumptions under which the results of [11,13] have been derived using the first approach. More concretely, we generalize the one-loop partition functions, as derived in [11,13] for levels being odd, to the case of even levels. Moreover, on the level of partition functions we implement additional dressings of the world-sheet parity symmetry and identify them with the dressings introduced in [12] in the crosscap state approach. As expected, all the physical information can be read off entirely from the various amplitudes. We will end up with a collection of very explicit and general one-loop partition functions and tadpole cancellation conditions covering simple current extensions of all 168 Gepner models with additional dressings of the parity symmetry. In fact providing a compact collection of the main relevant formulas for constructing supersymmetric Gepner model orientifolds was one of the motivations for writing this Letter. We hope that these expressions turn out to be useful for a systematic search for Standard-like models respectively for providing a statistical ensemble in the spirit of [29].", "ground_truth": ["generalize the one-loop partition functions", "", "dressings", "parity symmetry", "one-loop partition functions", "constructing supersymmetric gepner model orientifolds", "bring both approaches on equal footing", "relax the assumptions", "additional dressings of the world-sheet parity symmetry", "crosscap state approach", "tadpole cancellation conditions", "current extensions of all 168 gepner models", "dressings", "systematic search for standard-like models", "providing a statistical ensemble in the spirit of [29]"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "approaches", "assumptions", "results", "one-loop partition functions", "levels", "odd", "even levels", "partition functions", "dressings", "world-sheet parity symmetry", "crosscap state approach", "physical information", "amplitudes", "tadpole cancellation conditions", "simple current extensions", "gepner models", "parity symmetry", "supersymmetric gepner model orientifolds", "standard-like models", "statistical ensemble"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Absorption events through the charged current reactions (2)νe+40Ar→e−+40K∗andν̄e+40Ar→e++40Cl∗. There is some uncertainty in predicting e−(e+) event rates for these processes which arise due to the nuclear model dependencies of the absorption cross section and the treatment of the Coulomb distortion of electron (positron) in the field of the residual nucleus. The nuclear absorption cross section for the charged current neutrino reactions in 40Ar relevant to supernova neutrino energies was first calculated by Raghavan [10] and Bahcall et al. [11] for Fermi transitions leading to isobaric analogue state (IAS) at 4.38 MeV in 40K∗. Later Ormand et al. [12] used a shell model to calculate the Fermi and Gamow–Teller transitions. In these calculations Fermi function F(Z,Ee) was used to take into account the Coulomb effects. In a recent paper Bueno et al. [13] make use of a calculation by Martinez-Pinedo et al. [14] who use a shell model for Fermi and Gamow–Teller transitions and a continuum random phase approximation (CRPA) for forbidden transitions to calculate the absorption cross sections. In this calculation the Coulomb distortion of the produced electron is treated with a hybrid model where a Fermi function is used for lower electron energies and modified effective momentum approximation (MEMA) for higher electron energies [14–17]. In a recent work Bhattacharya et al. [18] have measured the Fermi and Gamow–Teller transition strengths leading to excited states up to 6 MeV in 40K∗ and obtained the neutrino absorption cross section for supernova neutrinos in 40Ar.", "ground_truth": ["absorption events through the charged current reactions", "treatment of the coulomb distortion", "isobaric analogue state", "ias", "4.38 mev in 40k", "shell model", "the fermi and gamow–teller transitions", "predicting e−(e+) event rates", "coulomb effects", "calculation by martinez-pinedo et al.", "shell model for fermi and gamow–teller transitions", "continuum random phase approximation", "crpa", "the absorption cross sections", "fermi and gamow–teller transition strengths", "neutrino absorption cross section", "modified effective momentum approximation", "mema", "nuclear absorption cross section for the charged current neutrino reactions", "nuclear model dependencies", "supernova neutrino energies", "electron", "positron", "residual nucleus"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "absorption events", "charged current reactions", "νe + 40ar → e− + 40k∗", "ν̄e + 40ar → e+ + 40cl∗", "event rates", "nuclear model dependencies", "absorption cross section", "coulomb distortion", "electron (positron)", "residual nucleus", "supernova neutrino energies", "raghavan", "bahcall et al.", "fermi transitions", "isobaric analogue state (ias)", "40k∗", "ormand et al."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Classical, two-dimensional sigma models on compact symmetric spaces G/H are integrable by virtue of conserved quantities which can arise as integrals of local or non-local functions of the underlying fields (the accounts in [1–5] contain references to the extensive literature). Since these models are asymptotically free and strongly coupled in the infrared, their quantum properties are not straightforward to determine. Nevertheless, following Lüscher [6], Abdalla, Forger and Gomes showed [7] that, in a G/H sigma model with H simple,11Here, and throughout this Letter, we shall use ‘simple’ to mean that the corresponding Lie algebra has no non-trivial ideals. Hence U(1) is simple in our terminology, in addition to the usual non-Abelian simple groups of the Cartan–Killing classification [13]. the first conserved non-local charge survives quantization (after an appropriate renormalization [6–8]), which suffices to ensure quantum integrability of the theory. By contrast, calculations using the 1/N expansion reveal anomalies that spoil the conservation of the quantum non-local charges in the CPN−1=SU(N)/SU(N−1)×U(1) models for N>2, and in the wider class of theories based on the complex Grassmannians SU(N)/SU(n)×SU(N−n)×U(1) for N>n>1 [9].", "ground_truth": ["two-dimensional sigma models on compact symmetric spaces g/h", "integrals of local or non-local functions", "g/h sigma model with h simple", "simple", "corresponding lie algebra has no non-trivial ideals", "quantization", "renormalization", "quantum integrability", "calculations using the 1/n expansion", "conservation of the quantum non-local charges", "cpn−1=su(n)/su(n−1)×u(1) models for n>2", "complex grassmannians su(n)/su(n)×su(n−n)×u(1) for n>n>1"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "classical", "two-dimensional", "sigma models", "compact symmetric spaces", "g/h", "integrable", "conserved quantities", "local", "non-local", "fields", "asymptotically free", "strongly coupled", "infrared", "quantum properties", "lüscher", "abdalla", "forger", "gomes", "simple", "lie algebra", "ideals", "u(1)", "non-abelian", "cartan–killing classification", "conserved non-local charge", "quantization", "renormalization", "quantum integrability"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We propose a method for the lattice QCD computation of nucleon–nucleon low-energy interactions. It consists in simulating QCD in the background of a “electromagnetic” field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of mπ/2.", "ground_truth": ["lattice qcd computation of nucleon–nucleon low-energy interactions", "electromagnetic” field", "field strength is zero", "tuning the background field", "phase-shifts at any (but small) momenta", "measuring the shift of the ground state energy", "lattice sizes", "calculation of phase shifts", "lattice qcd computation", "nucleon–nucleon low-energy interactions", "simulating qcd"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "lattice qcd", "nucleon–nucleon interactions", "low-energy interactions", "electromagnetic field", "field potential", "field strength", "background field", "phase-shifts", "momenta", "ground state energy", "lattice sizes", "fermi", "calculation", "mπ/2"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In our study we illustrate the properties of gauge invariant extensions of local functionals. We aim at clarifying, via specific examples, the relation between a functional which is local in a particular gauge (but not necessarily gauge invariant), and its gauge invariant extension (which is not necessarily local). We show that the non-localities found are not perturbatively local because they cannot be expressed in terms of an infinite derivative expansion. We believe that the implications of this observation have not been clearly emphasised in the literature, as attested by the absence of any debate about it in recent works. It is precisely these dangerous infrared modes that make it hard to define a gauge independent renormalisation for the gauge invariant extensions of local functionals. This observation supports the remark in [2] that the expectation value receives important contributions from both large and small distances. Our arguments on renormalisability are based on the notion of renormalisation in the modern sense [8] which relies on BRST cohomology theorems. The BRST terminology will therefore be frequently used here, even though it is not always necessary.", "ground_truth": ["invariant extensions of local functionals", "functional which is local in a particular gauge", "its gauge invariant extension", "nfinite derivative expansion", "infrared modes", "gauge independent renormalisation", "gauge invariant extensions of local functionals", "non-localities found", "expectation value", "notion of renormalisation", "renormalisability", "brst cohomology theorems", "brst terminology"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "gauge invariant extensions", "local functionals", "specific examples", "gauge invariance", "non-localities", "perturbatively local", "infinite derivative expansion", "infrared modes", "gauge independent renormalisation", "expectation value", "large distances", "small distances", "renormalisability", "modern renormalisation", "brst cohomology theorems", "brst terminology"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Certainly therefore the see-saw mechanism is an attractive explanation of why the light neutrino masses are so small. However, it is not without its faults. In particular, there is a tension between the strongly hierarchical nature of the observed Yukawa couplings in the quark and charged lepton sectors, and the essentially hierarchy-free masses implied by the Δm2's. Moreover, both the θ12 and θ23 mixing angles are large while the angle θ13 is small which is in sharp contrast with the corresponding mixings in the quark sector which are all small. These problems can be solved in specific models, for example, the Δm2 values can be fitted by taking the spectrum of rhd neutrino masses to be hierarchical in such a way as to almost compensate for the hierarchical neutrino Yukawa couplings. But this has the price of introducing a wide range of rhd neutrino masses MR∼1010–1015 which then require explanation.", "ground_truth": ["light neutrino masses", "see-saw mechanism", "yukawa couplings in the quark and charged lepton sectors", "hierarchy-free masses implied by the δm2's", "θ12 and θ23 mixing angles", "the angle θ13", "quark sector", "rhd neutrino masses", "δm2 values can be fitted by taking the spectrum of rhd neutrino masses", "hierarchical neutrino yukawa couplings", "rhd neutrino masses", "mr∼1010–1015", "quark", "charged lepton sectors", "yukawa couplings"], "prediction_list": ["- see-saw mechanism", "light neutrino masses", "yukawa couplings", "quark sector", "charged lepton sectors", "hierarchical nature", "δm²", "θ12 mixing angle", "θ23 mixing angle", "θ13 mixing angle", "quark sector mixings", "specific models", "rhd neutrino masses", "hierarchical spectrum", "neutrino yukawa couplings", "mass range", "mr ∼ 10¹⁰–10¹⁵"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We analyze the diagonal and transition magnetic and electric dipole moments of charged leptons in extended technicolor (ETC) models, taking account of the multiscale nature of the ETC gauge symmetry breaking, conformal (walking) behavior of the technicolor theory, and mixing in the charged-lepton mass matrix. We show that mixing effects dominate the ETC contributions to charged lepton electric dipole moments and that these can yield a value of |de| comparable to the current limit. The rate for μ→eγ can also be close to its limit. From these and other processes we derive constraints on the charged lepton mixing angles. The constraints are such that the ETC contribution to the muon anomalous magnetic moment, which includes a significant lepton mixing term, can approach, but does not exceed, the current sensitivity level.", "ground_truth": ["diagonal and transition magnetic and electric dipole moments of charged leptons", "extended technicolor", "etc", "gauge symmetry breaking", "conformal (walking) behavior of the technicolor theory", "mixing in the charged-lepton mass matrix", "mixing effects", "charged lepton electric dipole moments", "value of |de|", "rate for μ→eγ", "charged lepton mixing angles", "muon anomalous magnetic moment", "lepton mixing term", "etc", "charged-lepton mass matrix", "etc", "etc"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "diagonal magnetic dipole moments", "transition magnetic dipole moments", "electric dipole moments", "charged leptons", "extended technicolor (etc) models", "multiscale etc gauge symmetry breaking", "conformal behavior", "walking technicolor theory", "charged-lepton mass matrix", "mixing effects", "lepton electric dipole moments", "|dₑ| (electron electric dipole moment)", "μ→"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "It is well known that one of the long standing problems in physics is understanding the confinement physics from first principles. Hence the challenge is to develop analytical approaches which provide valuable insight and theoretical guidance. According to this viewpoint, an effective theory in which confining potentials are obtained as a consequence of spontaneous symmetry breaking of scale invariance has been developed [1]. In particular, it was shown that a such theory relies on a scale-invariant Lagrangian of the type [2] (1)L=14w2−12w−FμνaFaμν, where Fμνa=∂μAνa−∂νAμa+gfabcAμbAνc, and w is not a fundamental field but rather is a function of 4-index field strength, that is, (2)w=εμναβ∂μAναβ. The Aναβ equation of motion leads to (3)εμναβ∂βw−−FγδaFaγδ=0, which is then integrated to (4)w=−FμνaFaμν+M. It is easy to verify that the Aaμ equation of motion leads us to (5)∇μFaμν+MFaμν−FαβbFbαβ=0. It is worth stressing at this stage that the above equation can be obtained from the effective Lagrangian (6)Leff=−14FμνaFaμν+M2−FμνaFaμν. Spherically symmetric solutions of Eq. (5) display, even in the Abelian case, a Coulomb piece and a confining part. Also, the quantum theory calculation of the static energy between two charges displays the same behavior [1]. It is well known that the square root part describes string like solutions [3,4].", "ground_truth": ["understanding the confinement physics from first principles", "develop analytical approaches", "spontaneous symmetry breaking of scale invariance", "scale-invariant lagrangian", "function of 4-index field strength", "(2)w=εμναβ∂μaναβ", "aναβ equation of motion leads", "εμναβ∂βw−−fγδafaγδ=0", "integrated to (4)w=−fμνafaμν+m", "aaμ equation of motion", "the effective lagrangian", "leff=−14fμνafaμν+m2−fμνafaμν", "spherically symmetric solutions", "quantum theory calculation of the static energy between two charges", "coulomb piece", "confining part", "string like solutions"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "physics", "confinement physics", "first principles", "analytical approaches", "theoretical guidance", "effective theory", "confining potentials", "spontaneous symmetry breaking", "scale invariance", "scale-invariant lagrangian", "field strength", "equation of motion", "effective lagrangian", "spherically symmetric solutions", "abelian case", "coulomb piece", "confining part"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the supersymmetric case, such a small coupling for quartic interaction cannot be realized if the potential is lifted by the gauge D-term interactions, since, if so, the coupling constant λ becomes of the order O(g2) where g is the gauge coupling constant in the standard model. Therefore, we focus our attention on the D-flat directions. For D-flat directions, we have to be more careful since behaviors of the potential depend on which flat direction we consider. In the MSSM, Yukawa interactions exist in the superpotential to generate the fermion masses. Such Yukawa interactions lift some of the D-flat directions. In addition, we can also find several D-flat directions which are not affected by the Yukawa interactions associated with the fermion masses; without R-parity violation, such D-flat directions are only lifted by the effects of supersymmetry breaking.33Here, we assume that coefficients of non-renormalizable terms are suppressed enough to be neglected. This may be explained by the R-symmetry, assigning R-charge 23 to each MSSM chiral superfields. (See Ref. [6] for the details.)", "ground_truth": ["d-flat directions", "d-flat directions", "behaviors of the potential", "gauge d-term interactions", "the coupling constant λ", "yukawa interactions", "yukawa interactions", "coefficients of non-renormalizable terms", "r-symmetry", "gauge coupling constant", "standard model", "small coupling", "small coupling for quartic interaction", "d-flat directions", "mssm chiral superfields", "r-charge 23", "d-flat directions", "d-flat directions"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "supersymmetric case", "small coupling", "quartic interaction", "potential", "gauge d-term interactions", "coupling constant (λ)", "order o(g²)", "gauge coupling constant (g)", "standard model", "d-flat directions", "mssm (minimal supersymmetric standard model)", "yukawa interactions", "superpotential", "fermion masses", "r-par"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Some non-standard couplings, which should be determined here, could also be studied in the standard e+e− option of a linear collider. Therefore, it is worth while to compare the potential power of the two options. As far as the parameter αγ1 is concerned, the γγ collider does not allow for its determination, while it could be determined at e+e−. The second tt̄γ coupling αγ2, which is proportional to the real part of the top-quark electric dipole moment,44See [23] taking into account that the operators OuB, OqB and OqW are redundant. can be measured here. It should be recalled that energy and polar-angle distributions of leptons and b-quarks in e+e− colliders are sensitive only to the imaginary part of the electric dipole moment,55However, it should be emphasized that there exist observables sensitive also to the real part of the top-quark electric dipole moment, see [24]. while here the real part could be determined. For the measurement of γγH couplings, e+e− colliders are, of course, useless, while here, for the bX final state both αh1 and αh2 could be measured. In the case of the decay form factor αd measurement, the e+e− option seems to be a little more advantageous, especially if e+e− polarization can be tuned appropriately [25].", "ground_truth": ["non-standard couplings", "standard e+e− option", "linear collider", "potential power", "parameter αγ1", "γγ collider", "e+e−", "coupling αγ2", "e+e− colliders", "polar-angle distributions of leptons and b-quarks", "imaginary part of the electric dipole moment", "he real part of the top-quark electric dipole moment", "measurement of γγh couplings", "e+e− colliders", "the decay form factor αd measurement", "e+e− option", "e+e− polarization", "the bx final state", "αh1 and αh2 could be measured"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "non-standard couplings", "linear collider", "e+e− option", "γγ collider", "parameter αγ1", "tt̄γ coupling αγ2", "top-quark electric dipole moment", "operators oub", "oqb", "oqw", "energy distributions", "polar-angle distributions", "leptons", "b-quarks", "imaginary part", "real part", "γγh couplings", "bx final state", "αh1", "αh2", "decay form factor αd", "e+e−"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In contrast to the H particle, the situation for the Θ+ baryon is very promising. Thus, in this Letter we explore the formation of the Θ+-baryon within a new approach called parton-based Gribov–Regge theory. It is realized in the Monte Carlo program NEXUS 3.97 [22,23]. In this model high energy hadronic and nuclear collisions are treated within a self-consistent quantum mechanical multiple scattering formalism. Elementary interactions, happening in parallel, correspond to underlying microscopic (predominantly soft) parton cascades and are described effectively as phenomenological soft pomeron exchanges. A pomeron can be seen as layers of a (soft) parton ladder, which is attached to projectile and target nucleons via leg partons. At high energies one accounts also for the contribution of perturbative (high pt) partons described by a so-called “semihard pomeron”—a piece of the QCD parton ladder sandwiched between two soft pomerons which are connected to the projectile and to the target in the usual way. The spectator partons of both projectile and target nucleons, left after pomeron emissions, form nucleon remnants. The legs of the pomerons form color singlets, such as q–q̄, q–qq or q̄–q̄q̄. The probability of q–qq and q̄–q̄q̄ is controlled by the parameter Pqq and is fixed by the experimental yields on (multi-)strange baryons [23].", "ground_truth": ["θ+ baryon", "the formation of the θ+-baryon", "parton-based gribov–regge theory", "monte carlo program", "self-consistent quantum mechanical multiple scattering formalism", "fixed by the experimental yields on (multi-)strange baryons", "nexus 3.97", "high energy hadronic and nuclear collisions", "elementary interactions", "microscopic (predominantly soft) parton cascades", "phenomenological soft pomeron exchanges", "pomeron", "(soft) parton ladder", "projectile and target nucleons", "leg partons", "perturbative (high pt) partons", "“semihard pomeron”", "piece of the qcd parton ladder sandwiched between two soft pomerons which are connected to the projectile and to the target in the usual way", "spectator partons", "projectile and target nucleons", "pomeron emissions", "nucleon remnants", "pomerons", "color singlets", "q–q̄", "q–qq", "q̄–q̄q̄", "q–qq", "q̄–q̄q̄", "(multi-)strange baryons"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "h particle", "θ+ baryon", "parton-based gribov–regge theory", "monte carlo program", "nexus 3.97", "high energy hadronic collisions", "nuclear collisions", "self-consistent quantum mechanical multiple scattering formalism", "elementary interactions", "microscopic parton cascades", "soft pomeron exchanges", "parton ladder", "projectile nucleons", "target nucleons", "leg partons", "perturbative partons", "high pt partons", "semihard pomeron", "q"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "I also could not resist mentioning another wild speculation [10]. Many years ago, inspired by the almost exact correspondence between Einstein's post-Newtonian equations of gravity and Maxwell's equations of motion I proposed the gravitipole in analogy with Dirac's magnetic monopole. After Dirac there was considerable debate on how a field theory of magnetic monopoles may be formulated. Eventually, 't Hooft and Polyakov showed that the magnetic monopole exists as an extended solution in certain non-abelian gauge theories. Most theorists now believe that electromagnetism is merely a piece of a grand unified theory and that magnetic monopoles exist. Might it not turn out that Einstein's theory is but a piece of a bigger theory and that gravitipoles exist? In grand unified theory the electromagnetic field is a component of a multiplet. Could it be that the gravitational field also somehow carries an internal index and that the field we observe is just a component of a multiplet? Throwing caution to the wind, I also asked in [10] if the gravitipole and the graviton might not form a representation under some dual group just as the magnetic monopole and the photon form a triplet under the dual group of Montonen and Olive [11].", "ground_truth": ["einstein's post-newtonian equations of gravity", "gravitipole", "field theory of magnetic monopoles", "magnetic monopole", "gravitipoles", "gravitational field", "gravitipole", "maxwell's equations of motion", "dirac's magnetic monopole", "non-abelian gauge theories", "electromagnetism", "magnetic monopoles", "electromagnetic field", "graviton", "magnetic monopole", "photon"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "wild speculation", "einstein's post-newtonian equations", "maxwell's equations", "gravitipole", "dirac's magnetic monopole", "field theory", "magnetic monopoles", "'t hooft", "polyakov", "extended solution", "non-abelian gauge theories", "electromagnetism", "grand unified theory", "einstein's theory", "gravitational field", "internal index", "multiplet", "graviton", "dual group", "montonen and olive", "photon", "triplet"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In summary, we have shown that one can describe the experimental data of the HERMES Collaboration for hadron attenuation on nuclei without invoking any changes in the fragmentation function due to gluon radiation. In our dynamical studies, that include the most relevant FSI, we employ only the ‘free’ fragmentation function on a nucleon and attribute the hadron attenuation to the deceleration of the produced (pre-)hadrons due to FSI in the surrounding medium. We find that in particular the z-dependence of RMh is very sensitive to the interaction cross section of leading prehadrons and can be used to determine σlead. The interaction of the leading prehadrons during the formation time could be interpreted as an in-medium change of the fragmentation function, which however could not be given in a closed form. The extracted average hadron formation times of τf≳0.3 fm/c are compatible with the analysis of antiproton attenuation in p+A reactions at AGS energies [17]. In an upcoming work we will investigate in detail the spectra for different particle species (π±,K±,p,p̄) to examine, if the formation times of mesons and antibaryons are about equal. In addition we will improve our model to describe the primary photon–nucleon reaction below the PYTHIA threshold of W⩾4 GeV.", "ground_truth": ["describe the experimental data of the hermes collaboration for hadron attenuation on nuclei", "fragmentation function", "fsi", "‘free’ fragmentation function", "the z-dependence of rmh", "determine σlead", "detail the spectra for different particle species (π±", "k±", "p", "p̄)", "examine", "if the formation times of mesons and antibaryons are about equal", "interaction of the leading prehadrons during the formation time", "improve our model to describe the primary photon–nucleon reaction below the pythia threshold of w⩾4 gev", "hadron attenuation", "nuclei", "gluon radiation", "dynamical studies", "nucleon", "hadron attenuation", "hadron", "produced (pre-)hadrons", "fsi", "surrounding medium", "rmh", "leading prehadrons", "leading prehadrons", "in-medium change of the fragmentation function", "hadron formation", "hadron", "analysis of antiproton attenuation", "antiproton", "p+a reactions", "mesons", "antibaryons", "particle species", "π±", "k±", "p", "p̄", "primary photon–nucleon reaction", "photon", "nucleon"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "experimental data", "hermes collaboration", "hadron attenuation", "nuclei", "fragmentation function", "gluon radiation", "dynamical studies", "fsi (final state interactions)", "free fragmentation function", "nucleon", "deceleration", "pre-hadrons", "medium", "z-dependence", "rmh", "interaction cross section"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments [1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in [3], in cosmological context in [4,5] and more recently in [6–8]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.", "ground_truth": ["semi-classical arguments in weak coupling limit", "quantal states", "associated topological conservation laws", "fermionic zero-energy modes trapped on such solutions", "assignment of half-integer fermion number to the solitonic states", "the back reaction of the fermion zero-modes on the soliton", "solitons", "fractional values of the fermionic charge", "the fate of the soliton", "vacuum", "carrying the fractional value of the fermionic charge", "dynamical stability of such objects", "condensed matter systems", "systematic understanding of the phenomenon"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "solitons", "quantum field theory", "semi-classical arguments", "weak coupling limit", "quantal states", "topological conservation laws", "fermionic zero-energy modes", "fermion number", "half-integer fermion number", "solitonic states", "back reaction", "fermion zero-modes", "fractional fermionic charge", "dynamical stability", "cosmological context", "condensed matter systems", "fractional fermion number phenomenon", "systematic understanding", "these keywords capture the main concepts and topics discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The charmonium production has long been considered as a good process for investigating both perturbative and nonperturbative properties of quantum chromodynamics (QCD), because of the relatively large difference between the scale at which the charm–quark pair is produced at the parton level and the scale at which it evolves into a quarkonium. In particular, comparing to hadron colliders, e+e− colliders, provide a cleaner environment to study the charmonium productions and decays. However, some puzzles arise from the recent measurements on the prompt J/ψ productions at BaBar and Belle [1–3]. For the inclusive J/ψ productions, the cross section is much larger than the predictions of nonrelativistic quantum chromodynamics (NRQCD) [4]; there is also an over-abundance of the four-charm–quark processes including the exclusive J/ψ and charmonium productions; there is no apparent signal in the hard J/ψ spectrum which has been predicted by the J/ψgg production mode as well as the color-octet mechanism in NRQCD. To provide plausible solutions and explanations for these conflicts, theorists have studied the possibilities of the contribution from two-virtual-photon mediate processes [5], large higher-order QCD corrections [6,7], collinear suppression at the end-point region of the J/ψ momentum [7,8], contribution from the J/ψ-glueball associated production [9] and contribution from a very light scalar boson [10].", "ground_truth": ["charmonium production", "investigating both perturbative and nonperturbative properties of quantum chromodynamics (qcd", "quantum chromodynamics", "qcd", "e+e− colliders", "study the charmonium productions and decays", "nonrelativistic quantum chromodynamics", "nrqcd", "babar", "belle", "inclusive j/ψ productions", "recent measurements on the prompt j/ψ", "four-charm–quark processes", "exclusive j/ψ and charmonium productions", "j/ψgg production mode", "color-octet mechanism in nrqcd", "possibilities of the contribution from two-virtual-photon mediate processes", "large higher-order qcd corrections", "collinear suppression at the end-point region of the j/ψ momentum", "contribution from the j/ψ-glueball associated production", "contribution from a very light scalar boson"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "charmonium production", "perturbative properties", "nonperturbative properties", "quantum chromodynamics (qcd)", "charm–quark pair", "parton level", "quarkonium", "hadron colliders", "e+e− colliders", "charmonium decays", "prompt j/ψ productions", "babar", "belle", "inclusive j/ψ productions", "cross section", "nonrelativistic quantum chromodynamics (nrqcd)", "four-charm–quark processes", "exclusive j/ψ"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "States outside the constituent quark model have been hypothesized to exist almost since the introduction of color [1–4]. Hybrid mesons, qq̄ states with an admixture of gluons, and glueballs, states with no quark content, rely on the self interaction property of gluons due to their color charge. Looking for glueballs would be the most obvious way to find evidence for states with constituent gluons; however, the search is hindered by the fact that these states may significantly mix with regular qq̄-mesons in the region where the lightest are predicted to occur. As such, they may not be observable as pure states and disentangling the observed spectra may be a very difficult task. Instead, hybrid mesons (qq̄gn) may be a better place to search for evidence of resonances outside the constituent quark model, especially since the lightest of theses states are predicted to have exotic quantum numbers of spin, parity, and charge conjugation, JPC, that is, combinations that are unattainable by regular qq̄-mesons.", "ground_truth": ["looking for glueballs", "find evidence for states with constituent gluons", "disentangling the observed spectra", "hybrid mesons", "resonances outside the constituent quark model", "qq̄-mesons", "hybrid mesons", "gluons", "glueballs", "quark", "gluons", "color charge", "glueballs", "gluons", "qq̄-mesons", "qq̄gn", "constituent quark model", "jpc"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "states", "constituent quark model", "hybrid mesons", "qq̄ states", "gluons", "glueballs", "quark content", "color charge", "self interaction", "evidence", "constituent gluons", "qq̄-mesons", "pure states", "observed spectra", "resonances", "exotic quantum numbers", "spin", "parity", "charge conjugation", "jpc", "these keywords capture the main"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Correlation of charm-quark–charm-antiquark in γp scattering are calculated in the kt-factorization approach. We apply different unintegrated gluon distributions (uGDF) used in the literature. The results of our calculations are compared with very recent experimental results from the FOCUS Collaboration. The CCFM uGDF developed recently by Kwieciński et al. gives a good description of the data. New observables are suggested for future studies. Predictions and perspectives for the HERA energies are presented.", "ground_truth": ["kt-factorization approach", "correlation of charm-quark–charm-antiquark in γp scattering", "unintegrated gluon distributions", "ugdf", "ccfm ugdf", "hera energies", "charm", "quark", "charm", "antiquark", "γp scattering"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "charm-quark", "charm-antiquark", "γp scattering", "kt-factorization approach", "unintegrated gluon distributions (ugdf)", "focus collaboration", "ccfm ugdf", "kwieciński et al.", "experimental results", "hera energies", "predictions", "perspectives", "new observables"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The agreement between the new data and the calculations with the relativistic deuteron wave function should not be considered as accidental one; in this connection other results should be mentioned. Previously it was shown [15] that calculations within the framework of light-front dynamics with Karmanov's deuteron wave function are in reasonably good agreement with the experimental data on the T20 parameter of deuteron breakup on H and C targets with the emission of protons at 0° in the k region from 0.4 to 0.8 GeV/c. Furthermore, within the same approach a qualitative description of the momentum behaviour of the Ayy parameter of the 9Be(d,p)X reaction at a deuteron momentum of 4.5 GeV/c and a detected proton angle of 80 mr and a rather good description of the Ayy data for the 12C(d,p)X reaction at 9 GeV/c and 85 mr were obtained [16].", "ground_truth": ["agreement between the new data and the calculations with the relativistic deuteron wave function", "calculations within the framework of light-front dynamics with karmanov's deuteron wave function", "t20 parameter of deuteron breakup on h and c targets", "emission of protons at 0° in the k region from 0.4 to 0.8 gev/c", "deuteron momentum", "ayy data", "12c(d", "p)x reaction", "relativistic deuteron wave function", "light-front dynamics", "karmanov's deuteron wave function", "qualitative description of the momentum behaviour of the ayy parameter of the 9be(d", "p)x reaction"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "agreement", "new data", "calculations", "relativistic deuteron wave function", "accidental", "results", "light-front dynamics", "karmanov's deuteron wave function", "experimental data", "t20 parameter", "deuteron breakup", "h target", "c target", "proton emission", "0°", "k region", "0.4 to 0.8 gev/c", "approach", "qualitative description", "momentum behaviour", "ayy parameter", "9be(d", "p)x reaction", "deuteron momentum"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A scenario is proposed for bi-large lepton mixing in the framework of nearly threefold degenerate Majorana neutrinos. In our proposal, we impose Z3 symmetry in the neutrino sector at a high energy scale to account for the threefold degenerate neutrinos and the maximal mixing between νμ and ντ. In order to obtain the atmospheric neutrino mass splitting while keeping the maximal mixing between νμ and ντ, we introduce a small perturbation to the neutrino mass matrix without breaking Z3 symmetry. On the other hand, the solar neutrino mixing arises due to the non-diagonal charged lepton mass matrix, and the desirable large mixing and mass splitting for the solar neutrino oscillation can be obtained by radiative corrections.", "ground_truth": ["bi-large lepton mixing", "atmospheric neutrino mass splitting while keeping the maximal mixing between νμ and ντ", "introduce a small perturbation to the neutrino mass matrix without breaking z3 symmetry", "non-diagonal charged lepton mass matrix", "radiative corrections", "large mixing and mass splitting for the solar neutrino oscillation", "degenerate majorana neutrinos", "neutrino sector", "degenerate neutrinos", "neutrino", "neutrino mass matrix", "solar neutrino mixing", "solar neutrino"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "scenario", "bi-large lepton mixing", "nearly threefold degenerate majorana neutrinos", "z3 symmetry", "neutrino sector", "high energy scale", "threefold degenerate neutrinos", "maximal mixing", "νμ (muon neutrino)", "ντ (tau neutrino)", "atmospheric neutrino mass splitting", "small perturbation", "neutrino mass matrix", "solar neutrino mixing", "non-diagonal charged lepton mass matrix", "large mixing", "mass splitting", "solar neutrino oscillation", "radiative corrections"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the NJL model studied here, we find no stable stars with either CFL or normal quark matter cores. This is the opposite of the prediction of Ref. [15] where it was argued that there is no 2SC phase in compact stars. Let us be more precise: performing a Taylor expansion in the strange quark mass, the authors of Ref. [15] found that in beta-equilibrated electrically and color neutral quark matter the 2SC phase is always less favored than the CFL phase or normal quark matter. From this observation they concluded that the 2SC phase is absent in compact stars. In contrast to this result, it was shown in Ref. [16] in the framework of the NJL model that neutral 2SC matter could be the most favored quark phase in a certain regime. However, the authors argued that this interval might disappear if the hadronic phase is included more properly. This is indeed what we found for parameter set RKH, while for parameter set HK the 2SC phase survives only in a tiny window. Nevertheless, if Nature chooses to be similar to this equation of state, it will be this tiny window which gives rise to hybrid stars, whereas the CFL phase would be never present in compact stars.", "ground_truth": ["njl model studied", "taylor expansion", "neutral 2sc matter", "compact stars", "beta-equilibrated electrically", "2sc phase", "2sc phase", "njl model", "hadronic phase", "hybrid stars", "compact stars", "njl model", "cfl or normal quark matter cores", "stable stars", "strange quark mass", "color neutral quark matter", "cfl phase", "normal quark matter", "compact stars", "quark phase", "cfl phase"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "njl model", "stable stars", "cfl (color-flavor-locked) quark matter", "normal quark matter", "ref. [15]", "2sc (2-flavor superconducting) phase", "compact stars", "taylor expansion", "strange quark mass", "beta-equilibrated", "electrically neutral", "color neutral quark matter", "ref. [16]", "neutral 2sc matter", "quark phase", "hadronic phase", "parameter set rkh", "parameter set hk", "equation of state", "hybrid"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We investigate the density behavior of the symmetry energy with respect to isospin equilibration in the combined systems Ru(Zr)+Zr(Ru) at relativistic energies of 0.4 and 1.528A GeV. The study is performed within a relativistic framework and the contribution of the iso-vector, scalar δ field to the symmetry energy and the isospin dynamics is particularly explored. We find that the isospin mixing depends on the symmetry energy and a stiffer behavior leads to more transparency. The results are also nicely sensitive to the “fine structure” of the symmetry energy, i.e., to the covariant properties of the isovector meson fields.The isospin tracing appears much less dependent on the in medium neutron–proton cross sections (σnp) and this makes such observable very peculiar for the study of the isovector part of the nuclear equation of state.Within such a framework, comparisons with experiments support the introduction of the δ meson in the description of the iso-vector equation of state.", "ground_truth": ["combined systems", "relativistic energies", "investigate the density behavior of the symmetry energy with respect to isospin equilibration", "relativistic framework", "contribution of the iso-vector", "scalar δ field to the symmetry energy and the isospin dynamics is particularly explored", "isospin mixing", "study of the isovector part of the nuclear equation of state", "comparisons with experiments", "introduction of the δ meson in the description of the iso-vector equation of state", "ru(zr)+zr(ru)", "study", "σnp", "medium neutron–proton cross sections", "δ meson", "isovector meson fields"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "density behavior", "symmetry energy", "isospin equilibration", "combined systems", "ru(zr)+zr(ru)", "relativistic energies", "0.4a gev", "1.528a gev", "relativistic framework", "iso-vector", "scalar δ field", "isospin dynamics", "isospin mixing", "stiffer behavior"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The most ambitious goal may be stated as the one of detecting the location of, say, one missing level on an otherwise complete sequence. Dyson, in a recent review [7], uses information theory concepts and argues that correlations in a sequence may provide the necessary redundancy from which error correcting codes can be constructed. At one extreme where no correlations and therefore no redundancy are present (Poissonian sequence), there is no possibility of detecting one missing level. At the other extreme, a sequence of equally spaced levels (picket fence), there is a maximum redundancy and a missed level can be obviously detected as a hole in the spectrum. Eigenvalues of random matrices, which exhibit characteristic correlations, correspond to an intermediate situation between these two extremes. The attempts to locate in the last case a single missed level have remained unsuccessful so far. However, it should be mentioned that for two-dimensional chaotic systems where, besides correlations of the order of one mean spacing as described by random matrices, the presence and the role of long range correlations governed by the shortest periodic orbits and reflected in Weyl's law describing the average spectral density, is well understood. It is then possible to approximately locate, from the study of spectral fluctuations, a single missed level [8].", "ground_truth": ["detecting the location of", "say", "one missing level on an otherwise complete sequence", "information theory concepts", "correlations in a sequence", "error correcting codes can be constructed", "detecting one missing level", "no correlations and therefore no redundancy are present", "poissonian sequence", "sequence of equally spaced levels", "picket fence", "missed level can be obviously detected as a hole in the spectrum", "locate in the last case a single missed level", "approximately locate", "a single missed level", "study of spectral fluctuations", "two-dimensional chaotic systems", "weyl's law describing the average spectral density"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "ambitious goal", "detecting", "missing level", "complete sequence", "dyson", "information theory", "correlations", "redundancy", "error correcting codes", "poissonian sequence", "no redundancy", "equally spaced levels", "picket fence", "maximum redundancy", "hole in the spectrum", "eigenvalues", "random matrices", "characteristic correlations", "intermediate situation", "locate", "single missed level", "two-dimensional chaotic systems", "long range correlations", "shortest periodic orbits", "weyl's law", "average spectral density", "spectral fluctuations"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA) [10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions γuni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance [10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to Ψ(j−1)−Ψ(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc→∞ are equivalent to the Schrödinger equation for an integrable Heisenberg spin model [11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators [13]. In the case of N=4 SYM the equations for other sets of operators are also integrable [14–16]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref. [17].", "ground_truth": ["investigate the bfkl and dglap equations in the case of supersymmetric theories", "high symmetry may significantly simplify the structure of these equations", "leading logarithmic approximation", "lla", "anomalous dimensions obtained from universal anomalous dimensions γuni(j) by shifting its arguments by an integer number", "fixed by the superconformal invariance", "anomalous dimension matrices for twist-2 operators", "calculations in the maximally extended n=4 sym", "where the coupling constant is not renormalized", "twist-2 operators", "anomalous dimension matrix", "super-conformal invariance", "n=4 sym", "evolution equations for quasi-partonic operators", "evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit nc→∞"], "prediction_list": ["here is the list of keywords extracted from the given scientific paragraph:", "bfkl equations", "dglap equations", "supersymmetric theories", "high symmetry", "leading logarithmic approximation (lla)", "quasi-partonic operators", "n=1 sym", "supermultiplets", "anomalous dimensions", "universal anomalous dimensions γuni(j)", "superconformal invariance", "n=4 sym", "coupling constant", "twist-2 operators", "schrödinger equation", "heisenberg spin model", "multicolor limit nc→∞", "integrability", "qcd"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Thus, the extension to the charmed analogue Θc(3099) provides an interesting test for the SDO sum rule and lattice calculations [17]. Here, the charm quark is quite heavy so that the constituent-quark picture may fit well and the JW prediction for the parity is expected to be reproduced from QCD. In fact, quenched lattice calculation finds the parity of Θc(3099) to be positive [28]. In the extension to the Θc(3099) sum rules, there are two important aspects, which make this sum rule different from the SDO sum rule. First of all, since the charm quark is too heavy to form quark condensate, it gives non-perturbative effects only by radiating gluons. The quark–gluon mixed condensate 〈s̄gsσ·Gs〉, which was the important contribution in the Θ+ sum rule, is replaced by gluonic operators in the heavy quark expansion that are normally suppressed. Secondly, the charm quark mass has to be kept finite in the OPE, which can be done by using the momentum space expression for the charm-quark propagator. This is different from the light-quark sum rule where the calculation is performed in the coordinate space and all the quark propagators are obtained based on the expansion with the small quark mass. Keeping these two aspects in mind, we construct QCD sum rules for Θc(3099) and see how they are different from the Θ+(1540) sum rule.", "ground_truth": ["extension to the charmed analogue θc(3099)", "sdo sum rule and lattice calculations", "charm quark", "constituent-quark picture", "jw prediction", "quenched lattice calculation", "qcd sum rules", "see how they are different from the θ+(1540) sum rule", "sum rule", "radiating gluons", "replaced by gluonic operators in the heavy quark expansion", "charm quark mass has to be kept finite in the ope", "momentum space expression", "quark–gluon mixed condensate", "qcd", "sdo sum rule", "charm quark", "quark condensate", "s̄gsσ·gs", "θ+ sum rule", "gluonic operators", "heavy quark", "charm quark", "ope", "charm-quark propagator", "the light-quark sum rule", "quark propagators", "small quark"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "charmed analogue", "θc(3099)", "sdo sum rule", "lattice calculations", "charm quark", "constituent-quark picture", "jw prediction", "parity", "qcd", "quenched lattice calculation", "positive parity", "sum rules", "non-perturbative effects", "quark condensate", "gluons", "quark–gluon mixed condensate", "〈s̄gsσ·gs〉", "θ+ sum rule", "gluonic operators", "heavy quark expansion", "charm quark mass"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Including the O(αs) corrections, all the operators listed in (9) and (10) have to be included. A convenient framework to carry out these calculations is the QCD factorization framework [14] which allows to express the hadronic matrix elements in the schematic form: (11)〈Vγ|Oi|B〉=FB→VTiI+∫dk+2π∫01duφB,+(k+)TiII(k+,u)φV⊥(u), where FB→V are the transition form factors defined through the matrix elements of the operator O7, φB,+(k+) is the leading-twist B-meson wave-function with k+ being a light-cone component of the spectator quark momentum, φ⊥V(u) is the leading-twist light-cone distribution amplitude (LCDA) of the transversely-polarized vector meson V, and u is the fractional momentum of the vector meson carried by one of the two partons. The quantities TiI and TiII are the hard-perturbative kernels calculated to order αs, with the latter containing the so-called hard-spectator contributions. The factorization formula (11) holds in the heavy quark limit, i.e., to order ΛQCD/MB. This factorization framework has been used to calculate the branching fractions and related quantities for the decays B→K∗γ [15–17] and B→ργ [15,17]. The isospin violation in the B→K∗γ decays in this framework have also been studied [18]. (For applications to B→K∗γ∗, see Refs. [16,19,20].) Very recently, the hard-spectator contribution arising from the chromomagnetic operator O8 have also been calculated in next-to-next-to-leading order (NNLO) in αs showing that the spectator interactions factorize in the heavy quark limit [21]. However, the numerical effect of the resummed NNLO contributions is marginal and we shall not include this in our update.", "ground_truth": ["operators listed in (9) and (10) have to be included", "qcd factorization framework", "express the hadronic matrix elements in the schematic form:", "schematic form", "〈vγ|oi|b〉=fb→vtii+∫dk+2π∫01duφb", "+(k+)tiii(k+", "u)φv⊥(u)", "leading-twist light-cone distribution amplitude", "lcda", "isospin violation in the b→k∗γ decays", "next-to-next-to-leading orde", "nnlo", "fb→v", "transition form factors defined through the matrix elements of the operator o7", "φb", "+(k+)", "leading-twist b-meson wave-function with k+ being a light-cone component of the spectator quark momentum", "φ⊥v(u", "u", "fractional momentum of the vector meson carried by one of the two partons", "the factorization formula", "this factorization framework", "hard-spectator contribution arising from the chromomagnetic operator o8", "spectator interactions", "heavy quark"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "o(αs) corrections", "operators", "qcd factorization framework", "hadronic matrix elements", "transition form factors (fb→v)", "b-meson wave-function (φb", "+)", "light-cone component (k+)", "spectator quark momentum", "light-cone distribution amplitude (lcda", "φ⊥v)", "transversely-polarized vector meson (v)", "fractional momentum (u)", "hard-"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Several methods based on dynamical assumptions were suggested for determination of the P-parity of the Θ+ [13]. According to a general theorem [14], in order to determine the parity of one particle in a binary reaction one has to know polarizations at least of two fermions participating in this reaction. Model independent methods for determination of the P-parity of the Θ+ were suggested recently in Refs. [15,16] for pp-collision, and in Ref. [17] for photoproduction of the Θ+. The method of Refs. [15,16], based on the assumption that the spin of the Θ+ equals 12, suggests to measure the spin–spin correlation parameter in the reaction p→p→→Σ+Θ+ near the threshold. We generalize here this method for an arbitrary spin of the Θ+ and both isospins T=0 and T=1 of the NN channel of the NN→YΘ+ reaction. Furthermore, we consider a polarization transfer from a nucleon to the hyperon Y in this reaction. Our consideration is model independent, since it is based only on conservation of the P-parity, total angular momentum and isospin in the reaction and the generalized Pauli principle for nucleons.", "ground_truth": ["determination of the p-parity", "general theorem", "determine the parity", "binary reaction", "model independent methods", "determination of the p-parity", "measure the spin–spin correlation parameter", "polarization transfer", "fermions", "this reaction", "particle", "isospin", "nucleons", "conservation of the p-parity", "total angular momentum", "nucleon", "the hyperon y", "pp-collision", "photoproduction", "reaction"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "methods", "dynamical assumptions", "p-parity", "θ+ (theta plus)", "binary reaction", "polarizations", "fermions", "model independent methods", "pp-collision", "photoproduction", "spin–spin correlation parameter", "reaction p→p→→σ+θ+", "threshold", "arbitrary spin", "isospin (t=0"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A central question from the point of view of nuclear physics involves the changes to the quark and antiquark distributions of a bound proton. Since one must develop a reliable model of both the free proton and the binding of nucleons starting from the quark level [8], this problem is rather complicated. We intend to report on our investigation of that problem in future work. For the present, we have chosen to illustrate the formal ideas developed here by applying them to a toy model, namely the quark distributions of isospin symmetric quark matter in which each quark feels a scalar potential, −Vsq, and a vector potential, Vvq. This is the premise of the Quark–Meson Coupling (QMC) model [9] which has been used successfully to calculate the properties of nuclear matter as well as finite nuclei [10,11]. Most recently it has also been used to derive an effective nuclear force which is very close to the widely used Skyrme III force [12]. (Except that in QMC the quarks are confined by the MIT bag, as well as feeling the mean-field scalar and vector potentials generated by the surrounding nucleons.) In the mean field approximation, the Dirac equation for the quark in infinite quark matter is written as: (30)iγ·∂−m−Vqs−γ0VqvψQMq(x)=0.", "ground_truth": ["changes to the quark and antiquark distributions of a bound proton", "develop a reliable model of both the free proton and the binding of nucleons starting from the quark level", "illustrate the formal ideas developed here by applying them to a toy model", "quark distributions of isospin symmetric quark matter", "each quark feels a scalar potential", "−vsq", "and a vector potential", "vvq", "quark–meson coupling", "qmc", "calculate the properties of nuclear matter as well as finite nuclei", "effective nuclear force", "mean field approximation", "dirac equation for the quark in infinite quark matter", "(30)iγ·∂−m−vqs−γ0vqvψqmq(x)=0"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "nuclear physics", "quark", "antiquark distributions", "bound proton", "free proton", "nucleons", "quark level", "toy model", "isospin symmetric quark matter", "scalar potential", "vector potential", "quark–meson coupling (qmc) model", "nuclear matter", "finite nuclei", "effective nuclear force", "skyrme iii force", "mit bag", "mean-field approximation", "dirac equation", "infinite quark matter"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Within a coalescence approach as successfully applied earlier in the light-quark sector, we have evaluated transverse-momentum dependencies of charmed hadrons in central heavy-ion reactions at RHIC. For the charm-quark distributions at hadronization we have considered two limiting scenarios, i.e., no reinteractions (using spectra from PYTHIA) and complete thermalization with transverse flow of the bulk matter. The resulting J/ψ (mT-)spectra differ in slope by up to a factor of 2 (harder for pQCD c-quarks), and the integrated yield is about a factor of 3 larger in the thermal case. For D-mesons, we found that the difference in the slope parameters of the pT-spectra in the two scenarios is less pronounced, but their elliptic flow is about a factor of 2 larger for pT⩾1.5 GeV in the thermalized case. The elliptic flow pattern of D-mesons was found to be essentially preserved in the single-electron decay spectra, rendering the latter a very promising observable to address the strength of charm reinteractions in the QGP. The present study can be straightforwardly generalized to charmed baryons (Λc), which may serve as a complimentary probe for charm-quark reinteractions in the QGP.", "ground_truth": ["coalescence approach", "evaluated transverse-momentum dependencies", "central heavy-ion reactions", "charm-quark distributions", "complete thermalization", "transverse flow", "integrated yield", "difference in the slope parameters", "elliptic flow", "pt-spectra", "elliptic flow pattern", "single-electron decay spectra", "charm reinteractions", "charm-quark reinteractions", "complimentary probe", "spectra", "pqcd c-quarks", "d-mesons", "charmed hadrons", "hadronization", "d-mesons", "baryons", "qgp", "light-quark sector", "quark", "qgp"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "coalescence approach", "light-quark sector", "transverse-momentum dependencies", "charmed hadrons", "heavy-ion reactions", "rhic", "charm-quark distributions", "hadronization", "reinteractions", "pythia", "thermalization", "transverse flow", "bulk matter", "j/ψ spectra", "mt-spectra"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "One of the challenges in quantum chromodynamics (QCD) is the relativistic bound state problem. In the light-cone Hamiltonian approach [1] light-cone wave functions can be constructed in a boost invariant way. It is necessary to have reliable light-cone wave functions if one wants to calculate high energy scattering, especially exclusive reactions. Many parametrizations assume separability of the dependence on the longitudinal momentum fraction and transverse momentum which is very unlikely since the two momenta are coupled in the kinetic energy operator. Various approaches have been tried to compute such wave functions. One can use the usual equal time Hamiltonian [2] and transform the resulting wave functions into light-cone form with the help of kinematical on-shell equations. The light-cone Hamiltonian in a string picture is formulated in Ref. [3]. More ambitious is the construction of an effective Hamiltonian including the gauge degrees of freedom explicitly and then solving the bound state problem. For mesons this approach [4,5] still needs many parameters to be fixed. Attempts have been made to solve the valence quark wave function for mesons in a simple Hamiltonian with a two-body potential [6].", "ground_truth": ["quantum chromodynamics", "qcd", "relativistic bound state problem", "light-cone hamiltonian approach", "light-cone wave functions", "reliable light-cone wave functions", "calculate high energy scattering", "equal time hamiltonian", "transform the resulting wave functions into light-cone form", "construction of an effective hamiltonian including the gauge degrees of freedom explicitly", "solving the bound state problem", "solve the valence quark wave function for mesons", "simple hamiltonian with a two-body potential", "kinematical on-shell equations", "mesons", "valence quark wave function", "hamiltonian", "mesons", "reactions"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "quantum chromodynamics (qcd)", "relativistic bound state problem", "light-cone hamiltonian approach", "light-cone wave functions", "boost invariant", "high energy scattering", "exclusive reactions", "longitudinal momentum fraction", "transverse momentum", "kinetic energy operator", "equal time hamiltonian", "kinematical on-shell equations", "string picture", "effective hamiltonian", "gauge degrees of freedom", "mesons", "valence quark wave function", "two-body potential", "let me know if you'd like any modifications or additional details!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The microwave background is not the only universal photon field that has to be taken in consideration. Especially interesting is the isotropic infrared and optical background (IRB). The number density of IRB is smaller than that of MBR by more that two orders of magnitude. On the other hand, protons of lower energy can interact on the IRB, and the smaller number density has to be weighted with the higher flux of interacting protons. The present Universe is optically thin to 1019 eV and lower energy protons, but even at low redshifts the proton interaction rate quickly increases. This is different from the interactions on MBR, where the interacting protons quickly lose their energy even at z=0. The cosmological evolution of UHECR injection is thus of major importance for the contribution of such interactions to the flux of cosmogenic neutrinos.", "ground_truth": ["microwave background", "isotropic infrared and optical background", "irb", "the smaller number density has to be weighted with the higher flux of interacting protons", "present universe", "proton interaction rate", "cosmological evolution of uhecr injection", "flux of cosmogenic neutrinos", "taken in consideration", "interactions on mbr", "interacting protons", "quickly lose their energy even at z=0", "irb", "mbr", "irb", "interacting protons", "mbr", "proton", "protons", "universal photon field", "lower energy protons"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "microwave background", "universal photon field", "isotropic infrared and optical background (irb)", "number density", "mbr (microwave background radiation)", "protons", "energy", "flux", "interaction rate", "redshifts", "proton interaction", "z=0", "cosmological evolution", "uhecr (ultra-high-energy cosmic rays)", "injection", "cosmogenic neutrinos"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this Letter, we extend the McVittie's solution into charged black holes. We first deduce the metric for a Reissner–Nordström black hole in the expanding universe; several special cases of our solution are exactly the same as some solutions discovered previously. In the previous work [6] we have applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe, which is exactly the same as that derived by McVittie by solving the full Einstein equations. That demonstrates the power of this simple and straight-forward approach. In this Letter we follow the same procedure to derive the metric for the Reissner–Nordström black holes in Friedman–Robertson–Walker universe. We then study the influences of the evolution of the universe on the size of the black hole. Finally, in order to study the motion of the planet, we rewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system.", "ground_truth": ["study the influences of the evolution of the universe on the size of the black hole", "study the motion of the planet", "rewrite the metric from the cosmic coordinates system to the schwarzschild coordinates system", "mcvittie's solution", "extend the mcvittie's solution into charged black holes", "deduce the metric for a reissner–nordström black hole in the expanding universe", "asymptotic conditions to derive the schwarzschild metric", "applied the asymptotic conditions to derive the schwarzschild metric in the expanding universe", "solving the full einstein equations", "follow the same procedure to derive the metric", "reissner–nordström black holes in friedman–robertson–walker universe", "charged black holes", "reissner–nordström black hole", "black hole"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "letter", "mcvittie's solution", "charged black holes", "metric", "reissner–nordström black hole", "expanding universe", "special cases", "solutions", "previous work", "asymptotic conditions", "schwarzschild metric", "einstein equations", "friedman–robertson–walker universe", "evolution of the universe", "size of the black hole", "motion of the planet", "cosmic coordinates system", "schwarzschild coordinates system"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The ART model is a hadronic transport model that includes baryons such as N, Δ(1232), N∗(1440), N∗(1535), Λ, Σ, and mesons such as π, ρ, ω, η, K, K∗. Both elastic and inelastic collisions among most of these particles are included by using the experimental data from hadron–hadron collisions. The ART model has been quite successful in explaining many experimental observations, including the surprisingly large kaon antiflow [11,12] in heavy ion collisions at AGS energies. The ART model also allows us to understand whether or not the strongly interacting matter formed in these collisions reaches chemical and/or thermal equilibrium. In the present study, we extend the ART model to include perturbatively the Ξ particle as in the studies for other rare particles using the transport model [6,13,14].", "ground_truth": ["art model", "hadronic transport model", "experimental data from hadron–hadron collisions", "elastic and inelastic collisions", "art model", "art model", "understand whether or not the strongly interacting matter formed in these collisions reaches chemical and/or thermal equilibrium", "extend the art model to include perturbatively the ξ particle", "transport model", "baryons", "n", "δ(1232)", "n∗(1440)", "n∗(1535)", "λ", "σ", "mesons", "π", "ρ", "ω", "η", "k", "k∗", "these particles", "hadron–hadron collisions", "hadron", "hadron", "explaining many experimental observations", "kaon antiflow", "heavy ion collisions", "kaon", "heavy ion", "ags energies", "strongly interacting matter", "collisions", "art model", "ξ particle", "rare particles"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "art model", "hadronic transport model", "baryons", "n", "δ(1232)", "n∗(1440)", "n∗(1535)", "λ", "σ", "mesons", "π", "ρ", "ω", "η", "k", "k∗", "elastic collisions", "inelastic collisions", "hadron–hadron collisions", "experimental data", "kaon antiflow", "heavy ion collisions", "ags energies", "strongly interacting matter", "chemical equilibrium", "thermal"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Though, in this Letter we have constructed the Born–Infeld black holes in the presence of a cosmological constant and discussed their thermodynamical properties, many issues however still remain to be investigated. We know that Reissner–Nordström AdS black holes undergo Hawking–Page phase transition. This transition gets modified as we include Born–Infeld corrections into account. We hope to carry out a detail study on this issue in the future. Furthermore, in the context of brane world cosmology, it was found that a brane moving in a Reissner–Nordström AdS background generates non-singular cosmology [14]. However, as shown in [15], the brane always crosses the inner horizon of the bulk geometry, creating an instability. It would be interesting to study cosmology on the brane when it is moving in the charged black hole backgrounds that we have constructed. Note that since these charged holes does not have inner horizon for certain range of parameters, we may generate non-singular cosmology without creating the instabilities that we have just mentioned.", "ground_truth": ["reissner–nordström ads black holes", "hawking–page phase transition", "born–infeld corrections", "constructed the born–infeld black holes in the presence of a cosmological constant", "reissner–nordström ads background", "brane", "crosses the inner horizon of the bulk geometry", "study cosmology on the brane when it is moving in the charged black hole backgrounds", "charged holes", "generate non-singular cosmology without creating the instabilities", "born–infeld black holes", "discussed their thermodynamical properties", "brane world cosmology", "non-singular cosmology", "brane", "instability", "cosmology", "brane", "charged black hole"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "born–infeld black holes", "cosmological constant", "thermodynamical properties", "reissner–nordström ads black holes", "hawking–page phase transition", "brane world cosmology", "non-singular cosmology", "inner horizon", "instability", "charged black hole backgrounds", "parameters", "non-singular cosmology", "instabilities"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We prove the uniqueness of the supersymmetric Salam–Sezgin (Minkowski)4×S2 ground state among all non-singular solutions with a four-dimensional Poincaré, de Sitter or anti-de Sitter symmetry. We construct the most general solutions with an axial symmetry in the two-dimensional internal space, and show that included amongst these is a family that is non-singular away from a conical defect at one pole of a distorted 2-sphere. These solutions admit the interpretation of 3-branes with negative tension.", "ground_truth": ["prove the uniqueness", "non-singular solutions with a four-dimensional poincaré", "de sitter or anti-de sitter symmetry", "construct the most general solutions", "axial symmetry in the two-dimensional internal space", "", "non-singular away from a conical defect", "interpretation of 3-branes with negative tension", "3-branes with negative tension"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "supersymmetric", "salam–sezgin", "minkowski", "ground state", "non-singular solutions", "four-dimensional", "poincaré", "de sitter", "anti-de sitter", "symmetry", "axial symmetry", "two-dimensional", "internal space", "conical defect", "distorted 2-sphere", "3-branes"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Longitudinal beam and target single-spin asymmetries have been at the center of the attention lately, since they have been measured by the HERMES and CLAS experimental Collaborations [1–4] and more measurements are planned. They were originally believed to be signals of the so-called T-odd fragmentation functions [5], in particular, of the Collins function [6–12]. However, both types of asymmetry can receive contributions also from T-odd distribution functions [13–16], a fact that has often been neglected in analyses. An exhaustive treatment of the contributions of T-odd distribution functions has not been carried out completely so far, especially up to subleading order in an expansion in 1/Q, Q2 being the virtuality of the incident photon and the only hard scale of the process, and including quark mass corrections. It is the purpose of the present work to describe the longitudinal beam and target spin asymmetries in a complete way in terms of leading and subleading twist distribution and fragmentation functions. We consider both single-particle inclusive DIS, e+p→e′+h+X, and single-jet inclusive DIS, e+p→e′+jet+X. We assume factorization holds for these processes, even though at present there is no factorization proof for observables containing subleading-twist transverse-momentum dependent functions (only recently proofs for the leading-twist case have been presented in Refs. [17,18]).", "ground_truth": ["longitudinal beam and target single-spin asymmetries", "hermes and clas experimental collaborations", "t-odd fragmentation functions", "receive contributions also from t-odd distribution functions", "quark mass corrections", "exhaustive treatment of the contributions of t-odd distribution functions", "subleading order in an expansion in 1/q", "describe the longitudinal beam and target spin asymmetries", "factorization", "subleading-twist transverse-momentum dependent functions", "proofs for the leading-twist case", "collins function", "t-odd distribution functions", "t-odd distribution functions", "incident photon", "quark", "twist distribution", "fragmentation functions", "single-particle inclusive dis", "e+p→e′+h+x", "single-jet inclusive dis", "e+p→e′+jet+x", "factorization", "factorization proof for observables containing subleading-twist transverse-momentum dependent functions"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "longitudinal beam", "target single-spin asymmetries", "hermes", "clas", "experimental collaborations", "t-odd fragmentation functions", "collins function", "t-odd distribution functions", "subleading order", "1/q expansion", "virtuality", "incident photon", "hard scale", "quark mass corrections", "leading twist", "subleading twist", "distribution functions", "fragmentation functions", "single-particle inclusive dis", "e+p→e′+h+x", "single-jet inclusive dis", "e+p→"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "There are many possible applications for this mechanism. In this Letter, we have concentrated on its contribution to leptogenesis and baryogenesis. Our calculation is applicable in the phase when the fields are rolling. This rolling phase will start when the Hubble constant drops to a value comparable to the mass of the scalar fields. It is at this time in the cosmological evolution that CP violation is most efficient. After the fields have relaxed to their vacuum values, our CP violation mechanism turns off. We plan to discuss more details, in particular applications to concrete baryogenesis models, in a future publication [20]. Note that string cosmology and brane world scenarios may provide natural settings for the origin of the scalar fields required for our mechanism (e.g. see Ref. [30] for a recent paper on how scalar fields from brane world scenarios can play a new role in spontaneous baryogenesis).", "ground_truth": ["its contribution to leptogenesis and baryogenesis", "applications for this mechanism", "in the phase when the fields are rolling", "applications to concrete baryogenesis models", "fields have relaxed to their vacuum values", "cp violation mechanism", "string cosmology", "how scalar fields from brane world scenarios can play a new role in spontaneous baryogenesis", "rolling phase", "hubble constant drops to a value comparable to the mass of the scalar fields", "scalar fields", "cosmological evolution", "cp violation", "concrete baryogenesis models", "brane world scenarios", "scalar fields", "our mechanism", "scalar fields", "brane", "spontaneous baryogenesis"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "applications", "mechanism", "letter", "leptogenesis", "baryogenesis", "calculation", "phase", "fields", "rolling", "hubble constant", "mass", "scalar fields", "cosmological evolution", "cp violation", "vacuum values", "future publication", "string cosmology", "brane world scenarios", "origin"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the brane system appearing in string/D-brane theory, the stableness is the most important requirement. We find some stable brane configurations in the SUSY bulk-boundary theory. We systematically solve the singular field equation using a general mathematical result about the free-wave solution in S1/Z2-space. The two scalars, the extra-component of the bulk-vector (A5) and the bulk-scalar (Φ), constitute the solutions. Their different roles are clarified. The importance of the “parallel” configuration is disclosed. The boundary condition (of A5) and the boundary matter fields are two important elements for making the localized configuration. Among all solutions, the solution (c1=−1, c2=−1) is expected to be the thin-wall limit of a kink solution. We present a bulk Higgs model corresponding to the non-singular solution. The model is expected to give a non-singular and stable brane solution in the SUSY bulk-boundary theory.", "ground_truth": ["string/d-brane theory", "susy bulk-boundary theory", "solve the singular field equation", "general mathematical result about the free-wave solution", "extra-component of the bulk-vector", "a5", "bulk-scalar", "φ", "different roles are clarified", "boundary condition", "thin-wall limit", "c1=−1", "c2=−1", "solution", "bulk higgs model", "non-singular and stable brane solution", "susy bulk-boundary theory", "brane system", "stableness", "stable brane configurations", "kink solution", "non-singular solution"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "brane system", "string/d-brane theory", "stableness", "stable brane configurations", "susy bulk-boundary theory", "singular field equation", "free-wave solution", "s1/z2-space", "scalars", "extra-component", "bulk-vector (a5)", "bulk-scalar (φ)", "solutions", "parallel configuration", "boundary condition (a5)", "boundary matter fields", "localized configuration", "thin-wall limit", "kink solution", "bulk higgs model", "non-singular solution"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The spins and parities of Θ+ and Ξ−− are not yet known experimentally. In this new wave of pentaquark research, most theoretical papers take the spin equal to 1/2. The parity is more controversial. In chiral soliton or Skyrme models the parity is positive [4]. In constituent quark models it is usually positive. In the present approach, the parity of the pentaquark is given by P=(−)ℓ+1, where ℓ is the angular momentum associated with the relative coordinates of the q4 subsystem. We analyze the case where the subsystem of four light quarks is in a state of orbital symmetry [31]O and carries an angular momentum ℓ=1. Although the kinetic energy of such a state is higher than that of the totally symmetric [4]O state, the [31]O symmetry is the most favourable both for the flavour–spin interaction [12] and the colour–spin interaction [13]. In the first case the statement is confirmed by the comparison between the realistic calculations for positive parity [12] and negative parity [14], based on the same quark model [15]. In Ref. [12] the antiquark was heavy, c or b, and accordingly the interaction between light quarks and the heavy antiquark was neglected, consistent with the heavy quark limit. In Ref. [16] an attractive spin–spin interaction between s̄ and the light quarks was incorporated and shown that a stable or narrow positive parity uudds̄ pentaquark can be accommodated within such a model. This interaction has a form that corresponds to η meson exchange [17] and its role is to lower the energy of the whole system.", "ground_truth": ["spins and parities of θ+ and ξ−−", "new wave of pentaquark research", "take the spin equal to 1/2", "chiral soliton", "constituent quark models", "parity of the pentaquark", "p=(−)ℓ+1", "ℓ", "angular momentum associated with the relative coordinates of the q4 subsystem", "subsystem of four light quarks is in a state of orbital symmetry", "comparison between the realistic calculations for positive parity [12] and negative parity [14]", "based on the same quark model [15]", "lower the energy of the whole system", "kinetic energy", "totally symmetric [4]o state", "flavour–spin interaction", "θ+", "ξ−−", "pentaquark", "skyrme models", "pentaquark", "light quarks", "o symmetry", "colour–spin interaction", "quark model", "quark", "antiquark", "interaction between light quarks and the heavy antiquark was neglected", "consistent with the heavy quark limit", "heavy quark", "heavy antiquark", "light quarks", "spin–spin interaction", "s̄", "light quarks", "uudds̄ pentaquark", "η meson exchange", "η meson"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "spins", "parities", "θ+", "ξ−−", "pentaquark", "theoretical papers", "spin 1/2", "chiral soliton models", "skyrme models", "positive parity", "constituent quark models", "angular momentum (ℓ)", "relative coordinates", "q4 subsystem", "orbital symmetry [31]o", "tot"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "There exist some interesting cases where the deformation structure becomes simple. One is the limit to the N=1/2 superspace [5], where the action should reduce to N=1/2 super-Yang–Mills theory with adjoint matter. Another interesting case is the singlet deformation [10,11], where the deformation parameters belongs to the singlet representation of the R-symmetry group SU(2)R. In this Letter, we will study N=2 supersymmetric U(1) gauge theory in the harmonic superspace with singlet deformation. In this case, the gauge and supersymmetry transformations get correction linear in the deformation parameter. Therefore we can easily perform the field redefinition such that the component fields transform canonically under the gauge transformation. In the case of N=1/2 super-Yang–Mills theory, such field redefinition is also possible [5]. But in this case the component fields do not transform canonically under the deformed supersymmtery transformation. In the singlet case, we will show that there is a field redefinition such that the redefined fields also transform canonically under the deformed supersymmetry. We will construct a deformed Lagrangian which is invariant under both the gauge and supersymmetry transformations. We find that the deformed Lagrangian is characterized by a single function of an antiholomorphic scalar field.", "ground_truth": ["deformation structure", "singlet deformation", "deformation parameters", "n=2 supersymmetric u(1) gauge theory", "the gauge and supersymmetry transformations", "correction linear", "field redefinition", "n=1/2 super-yang–mills theory", "field redefinition", "component fields", "deformed supersymmtery transformation", "singlet case", "field redefinition", "deformed lagrangian", "deformed lagrangian", "n=1/2 superspace", "n=1/2 super-yang–mills theory with adjoint matter", "r-symmetry group", "su(2)r", "harmonic superspace", "singlet representation", "singlet deformation", "deformation parameter", "gauge transformation", "transform canonically under the deformed supersymmetry", "deformed supersymmetry", "gauge and supersymmetry transformations", "single function", "antiholomorphic scalar field"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "deformation structure", "n=1/2 superspace", "n=1/2 super-yang–mills theory", "adjoint matter", "singlet deformation", "deformation parameters", "singlet representation", "r-symmetry group", "su(2)r", "n=2 supersymmetric", "u(1) gauge theory", "harmonic superspace", "gauge transformation", "supersymmetry transformations", "field redefinition", "component fields", "deformed supersymmetry transformation", "deformed lagrangian", "antiholomorphic scalar field"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We consider finite-time, future (sudden or Big Rip type) singularities which may occur even when strong energy condition is not violated but equation of state parameter is time-dependent. Recently, example of such singularity has been presented by Barrow, we found another example of it. Taking into account back reaction of conformal quantum fields near singularity, it is shown explicitly that quantum effects may delay (or make milder) the singularity. It is argued that if the evolution to singularity is realistic, due to quantum effects the universe may end up in de Sitter phase before scale factor blows up. This picture is generalized for braneworld where sudden singularity may occur on the brane with qualitatively similar conclusions.", "ground_truth": ["back reaction of conformal quantum fields near singularity", "quantum effects", "evolution to singularity", "the universe may end up in de sitter phase", "qualitatively similar conclusions", "example of such singularity", "finite-time", "future (sudden or big rip type) singularities", "strong energy condition", "we found another example of it", "singularity", "singularity", "quantum effects", "scale factor blows up", "sudden singularity", "brane", "braneworld"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "finite-time", "future singularities", "sudden singularity", "big rip", "strong energy condition", "equation of state parameter", "time-dependent", "barrow", "back reaction", "conformal quantum fields", "quantum effects", "delay singularity", "milder singularity", "evolution to singularity", "de sitter phase", "scale factor", "braneworld", "brane", "qualitative conclusions"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "One of the great successes of the experimental program carried out at LEP has been to put a firm lower bound on the Higgs mass, mH>114 GeV [1], and at the same time, together with the information coming from SLD, to give a strong indirect evidence that the Higgs boson, the still missing particle of the Standard Model (SM), should be relatively light with a high probability for its mass to be below 200 GeV. The search for the Higgs boson is one of the main objective of the Tevatron and the future Large Hadron Collider (LHC), that are supposed to span all the Higgs mass regions up to 1 TeV. At hadron colliders the main Higgs production mechanism is the gluon fusion [2], a process whose knowledge is fundamental in order to put limits on the Higgs mass or, in case the Higgs is discovered, to compare the measured cross section with the SM result. Concerning the Higgs decay channels, it is quite difficult for an hadron collider to access part of the mass range favored by the LEP results, the so-called intermediate Higgs mass region 114≲mH≲160 GeV, because of the large QCD background to the dominant modes. In this region the rare decay H→γγ is the most interesting alternative to the usual decay channels.", "ground_truth": ["the search for the higgs boson", "tevatron", "large hadron collider", "(lhc)", "span all the higgs mass regions up to 1 tev", "hadron colliders", "main higgs production mechanism", "gluon fusion", "put limits on the higgs mass", "hadron collider", "part of the mass range favored by the lep results", "access part of the mass range favored by the lep results", "intermediate higgs mass region", "rare decay", "decay channels", "put a firm lower bound on the higgs mass", "mh>114 gev", "higgs boson", "standard model", "sm", "sm", "higgs", "higgs", "114≲mh≲160 gev", "sld", "particle", "higgs mass", "higgs production mechanism", "compare the measured cross section with the sm result", "higgs decay", "qcd background", "h→γγ", "higgs mass"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "lep", "higgs mass", "mh > 114 gev", "sld", "higgs boson", "standard model (sm)", "tevatron", "large hadron collider (lhc)", "1 tev", "hadron colliders", "higgs production mechanism", "gluon fusion", "cross section", "higgs decay channels", "intermediate higgs mass region", "114 ≲ mh ≲ 160 gev", "qcd background", "rare decay", "h → γγ"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Recent publications [31] employ a variety of methods for calculating upper limits and there is no universally accepted procedure [27,32,33]. We choose an approach similar to that first advocated by Feldman and Cousins [27]. This method has been since extended by Conrad et al. [34] to incorporate uncertainties in detector sensitivity and the background estimate based on an approach described by Cousins and Highland [35]. A further refinement of the Conrad et al. method by Hill [36] results in more appropriate behavior of the upper limit when the observed number of events is less than the estimated background, as is the case for the present measurement. We have adopted this method but note that Table 2 contains all of the numbers needed to calculate an upper limit using any of the methods in the papers cited above. We assume that the probability density functions of Fsens and background estimates are Gaussian-distributed.", "ground_truth": ["calculating upper limits", "incorporate uncertainties", "background estimate", "further refinement of the conrad et al. method", "more appropriate behavior", "calculate an upper limit", "probability density functions", "background estimates", "detector sensitivity", "measurement", "gaussian-distributed"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "recent publications", "methods", "calculating upper limits", "universally accepted procedure", "feldman and cousins", "conrad et al.", "uncertainties", "detector sensitivity", "background estimate", "cousins and highland", "refinement", "hill", "observed number of events", "estimated background", "present measurement", "table 2", "probability density"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In these frameworks, however, the physical spacetime dimension is an input rather than a prediction of the theory. In fact, in standard theories whose gravitational sector is described by the Einstein–Hilbert action, there is no obstruction to perform dimensional reductions to spacetimes of dimensions d≠4. Then the question arises, since eleven-dimensional Minkowski space is a maximally (super)symmetric state, and the theory is well-behaved around it, why the theory does not select this configuration as the vacuum, but instead, it chooses a particular compactified space with less symmetry. An ideal situation, instead, would be that the eleven-dimensional theory dynamically predicted a low energy regime which could only be a four-dimensional effective theory. In such a scenario, a background solution with an effective spacetime dimension d>4 should be expected to be a false vacuum where the propagators for the dynamical fields are ill-defined, lest a low energy effective theory could exist in dimensions higher than four.", "ground_truth": ["physical spacetime dimension", "prediction of the theory", "standard theories", "einstein–hilbert action", "dimensional reductions", "minkowski space", "(super)symmetric state", "select this configuration", "compactified space", "eleven-dimensional theory", "low energy regime", "four-dimensional effective theory", "background solution", "effective spacetime dimension", "d>4", "false vacuum", "propagators for the dynamical fields", "low energy effective theory"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "frameworks", "physical spacetime dimension", "input", "prediction", "theory", "gravitational sector", "einstein–hilbert action", "dimensional reductions", "spacetimes", "dimensions", "d≠4", "eleven-dimensional minkowski space", "maximally (super)symmetric state", "well-behaved", "vacuum", "compactified space", "symmetry", "eleven-dimensional theory", "dynamically predicted", "low energy regime", "four-dimensional effective theory", "background solution", "effective spacetime dimension", "d>4", "false vacuum"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The presence of chaotic motion in nuclear systems has been firmly related with the statistics of high-lying energy levels [8,9]. Poisson distributions of normalized spacings of successive nuclear or atomic excited levels with the same spin and parity correspond to integrable classical dynamics, while Wigner's statistics signal chaotic motion in the corresponding classical regime [10]. Intermediate situations are more difficult to assess. Very recently a proposal has been made to treat the spectral fluctuations δn as discrete time series [11]. Defining (1)δn=∫−∞En+1ρ˜(E)dE−n, with ρ˜(E) the mean level density which allows the mapping to dimensionless levels with unitary average level density, and analyzing the energy fluctuations as a discrete time series, they found that nuclear power spectra behave like 1f noise, postulating that this might be a characteristic signature of generic quantum chaotic systems. In the present work we implement this idea, using the 1f spectral behavior as a test for the presence of chaos in nuclear mass errors.", "ground_truth": ["chaotic motion in nuclear systems", "statistics of high-lying energy levels", "poisson distributions", "excited levels", "spin", "parity", "integrable classical dynamics", "wigner's statistics", "chaotic motion", "classical regime", "spectral fluctuations", "δn", "discrete time series", "mapping to dimensionless levels", "unitary average level density", "analyzing the energy fluctuations", "discrete time series", "nuclear power spectra", "quantum chaotic systems", "1f spectral behavior", "chaos in nuclear mass errors"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "chaotic motion", "nuclear systems", "statistics", "high-lying energy levels", "poisson distributions", "normalized spacings", "excited levels", "spin", "parity", "integrable classical dynamics", "wigner's statistics", "chaotic motion", "classical regime", "intermediate situations", "spectral fluctuations", "discrete time series", "mean level density", "dimensionless levels", "unitary average level density", "energy fluctuations", "nuclear power spectra", "1/f noise", "quantum chaotic systems", "spectral behavior", "chaos", "nuclear mass errors"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Table 1 lists 8 pairs of B decays. In fact, there are more decay pairs, since many of the particles in the final states can be observed as either pseudoscalar (P) or vector (V) mesons. Note that certain decays are written in terms of VV final states, while others are have PP states. There are three reasons for this. First, some decays involve a final-state π0. However, experimentally it will be necessary to find the decay vertices of the final particles. This is virtually impossible for a π0, and so we always use a ρ0. Second, some pairs of decays are related by SU(3) in the SM only if an (ss¯) quark pair is used. However, there are no P's which are pure (ss¯). The mesons η and η′ have an (ss¯) component, but they also have significant (uu¯) and (dd¯) pieces. As a result the b¯→s¯ and b¯→d¯ decays are not really related by SU(3) in the SM if the final state involves an η or η′. We therefore consider instead the vector meson ϕ which is essentially a pure (ss¯) quark state. Finally, we require that both B0 and B¯0 be able to decay to the final state. This cannot happen if the final state contains a single K0 (or K¯0) meson. However, it can occur if this final-state particle is an excited neutral kaon. In this case one decay involves K*0, while the other has K¯*0. Assuming that the vector meson is detected via its decay to ψKsπ0 (as in the measurement of sin2β via Bd0(t)→J/ψK*), then both B0 and B¯0 can decay to the same final state.", "ground_truth": ["b decays", "decay pairs", "mesons", "find the decay vertices", "quark pair", "η", "η′", "mesons", "decays", "sm", "η", "η′", "vector meson", "ϕ", "quark state", "b0", "b¯0", "decay to the final state", "meson", "final-state particle", "excited neutral kaon", "k*0", "k¯*0", "vector meson", "decay to ψksπ0", "measurement of sin2β", "b0", "b¯0", "decay", "decays", "decays", "final particles", "pairs of decays", "sm", "quark", "ψksπ0"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "table 1", "b decays", "decay pairs", "final states", "pseudoscalar (p)", "vector (v) mesons", "vv final states", "pp states", "π0", "decay vertices", "ρ0", "su(3)", "sm (standard model)", "(ss¯) quark pair", "η", "η′", "(uu¯)", "(dd¯)", "b¯→s¯", "b¯→d¯", "vector meson ϕ"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "If signals suggesting supersymmetry (SUSY) are discovered at the LHC then it will be vital to measure the spins of the new particles to demonstrate that they are indeed the predicted super-partners. A method is discussed by which the spins of some of the SUSY particles can be determined. Angular distributions in sparticle decays lead to charge asymmetry in lepton-jet invariant mass distributions. The size of the asymmetry is proportional to the primary production asymmetry between squarks and anti-squarks. Monte Carlo simulations are performed for a particular mSUGRA model point at the LHC. The resultant asymmetry distributions are consistent with a spin-0 slepton and a spin-12χ˜20, but are not consistent with both particles being scalars.", "ground_truth": ["supersymmetry", "susy", "super-partners", "susy particles", "angular distributions", "charge asymmetry", "lepton-jet invariant mass distributions", "primary production asymmetry", "msugra model point", "asymmetry distributions", "measure the spins of the new particles", "monte carlo simulations", "particles", "particles", "squarks", "anti-squarks", "sparticle decays", "demonstrate that they are indeed the predicted super-partners", "spin-0 slepton"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "signals", "supersymmetry (susy)", "lhc", "spins", "new particles", "super-partners", "method", "sparticle decays", "angular distributions", "charge asymmetry", "lepton-jet invariant mass distributions", "primary production asymmetry", "squarks", "anti-squarks", "monte carlo simulations", "msugra model", "asymmetry distributions", "spin-0 slepton", "spin-12χ˜20", "scalars"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "It should be noted that BEBC [21] and NOMAD [20] observed a discrepancy between experimental ρ rates and those estimated with JETSET [16]. NOMAD [20] proposed to retune some of the parameters used within JETSET to obtain better agreement. Therefore, for the purpose of this analysis events were simulated both with the default setting and with the setting proposed by NOMAD of key JETSET parameters, taking an average between them as a result and half a difference as a systematic error. We used experimental rates of light neutral mesons and resonances where available (Table 1) for normalization purposes. The uncertainty introduced by the JETSET parameter settings (which amounts to 20% at most) affects only the production of the η′ and ϕ for which no experimental data are available. This uncertainty is reflected in the error quoted in the table. However, since the contribution from η′ and ϕ is small, the overall effect is less important.", "ground_truth": ["bebc", "nomad", "jetset", "nomad", "observed a discrepancy", "jetset", "retune some of the parameters", "events were simulated", "setting proposed by nomad", "jetset", "taking an average between them", "half a difference as a systematic error", "resonances", "jetset", "parameter settings", "light neutral mesons", "nomad", "obtain better agreement", "analysis", "default setting", "normalization", "experimental data"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "bebc", "nomad", "discrepancy", "experimental ρ rates", "jetset", "retune", "parameters", "default setting", "systematic error", "light neutral mesons", "resonances", "normalization", "uncertainty", "η′ (eta prime)", "ϕ (phi)", "experimental data", "error", "contribution", "overall effect"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this Letter, we present results of a relativistic calculation of decay constants in the framework of full Salpeter equation. The full Salpeter equation is a relativistic equation describing a bound state. Since this method has a very solid basis in quantum field theory, it is very good in describing a bound state which is a relativistic system. In a previous paper [16], we solved the instantaneous Bethe–Salpeter equation [17], which is also called full Salpeter equation [18]. After we solved the full Salpeter equation, we obtained the relativistic wave function of the bound state. We used this wave function to calculate the average kinetic energy of the heavy quark inside a heavy meson in 0− state, and obtained values which agree very well with recent experiments. We also found there that the relativistic corrections are quite large and cannot be ignored [16]. In this Letter we use this method to predict the values of decay constants of heavy mesons in 0− state.", "ground_truth": ["relativistic calculation of decay constants", "framework of full salpeter equation", "salpeter equation", "relativistic equation", "quantum field theory", "describing a bound state", "relativistic system", "solved the instantaneous bethe–salpeter equation", "full salpeter equation", "bethe–salpeter equation", "solved the full salpeter equation", "relativistic wave function of the bound state", "calculate the average kinetic energy", "obtained values", "relativistic corrections are quite large", "use this method", "predict the values of decay constants", "heavy quark", "heavy meson", "heavy mesons", "wave function"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "relativistic calculation", "decay constants", "salpeter equation", "full salpeter equation", "bound state", "quantum field theory", "relativistic system", "instantaneous bethe-salpeter equation", "relativistic wave function", "average kinetic energy", "heavy quark", "heavy meson", "0− state", "relativistic corrections", "prediction"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Our aim is to introduce vector mesons in terms of a Lagrangian which satisfies the low energy current algebra. One consistent method is in terms of a non-linear chiral Lagrangian with a hidden local symmetry [6]. In this theory the vector mesons emerge as dynamical vector mesons. The three point vector-pseudo scalar interaction is given by (11)ih4〈Vμ(P∂μP−∂μPP)〉, where h stands for the vector-pseudoscalar coupling. Some typical vertices of ρ's to pseudoscalar mesons are (12)π+(p1)π−(p2)ρ0:h(p1−p2)μεμ,π+(p1)π0(p2)ρ−:h(p1−p2)μεμ,K+(p1)K¯0(p2)ρ−:h2(p1−p2)μεμ,etc., which is directly related to the ρ decay width: Γ(ρ)=h2(|pπ|)3/(6πmρ2), where pπ is the momentum of final state pions in the ρ rest frame. With Γ(ρ)=149.2MeV, we find h=5.95. We note in passing that the Kawarabayashi–Suzuki–Riazuddin–Fayyazuddin relation gives the value h=mρ/(2fπ)[12]. Thus the value of h in Eq. (4) and the two values in this paragraph differ by small amounts (∼19%). The strong four-point vertices involving pions are obtained from the first two terms of Eq. (5). The weak vertices are obtained from the definitions of Q6 and Q8. In the numerical work we shall use the value of h from Eq. (4) and also h=5.95 obtained from the decay width.", "ground_truth": ["introduce vector mesons", "lagrangian", "low energy current algebra", "non-linear chiral lagrangian", "hidden local symmetry", "dynamical vector mesons", "vector-pseudo scalar interaction", "h", "vector-pseudoscalar coupling", "ih4〈vμ(p∂μp−∂μpp)〉", "vertices of ρ's to pseudoscalar mesons", "(12)π+(p1)π−(p2)ρ0:h(p1−p2)μεμ", "π+(p1)π0(p2)ρ−:h(p1−p2)μεμ", "k+(p1)k¯0(p2)ρ−:h2(p1−p2)μεμ", "ρ decay width", "γ(ρ)=h2(|pπ|)3/(6πmρ2)", "pπ", "momentum of final state pions in the ρ rest frame", "kawarabayashi–suzuki–riazuddin–fayyazuddin relation", "differ by small amounts", "strong four-point vertices involving pions are obtained", "weak vertices are obtained", "vector mesons", "vector mesons"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "vector mesons", "lagrangian", "low energy current algebra", "non-linear chiral lagrangian", "hidden local symmetry", "dynamical vector mesons", "vector-pseudoscalar interaction", "coupling (h)", "three-point vertex", "ρ meson (rho)", "pseudoscalar mesons", "π (pion)", "k (kaon)", "decay width (γ)"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "On the other hand, the other local fields except the gravitational field are not always localized on the brane even in the warped geometry. Indeed, in the Randall–Sundrum model in five dimensions [2], the following facts are well known: spin 0 field is localized on a brane with positive tension which also localizes the graviton while the spin 1/2 and 3/2 fields are localized not on a brane with positive tension but on a brane with negative tension [6]. Spin 1 field is not localized neither on a brane with positive tension nor on a brane with negative tension [7]. In six space–time dimensions, the spin 1 gauge field is also localized on the brane [8]. Thus, in order to fulfill the localization of Standard Model particles on a brane with positive tension, it seems that some additional interactions except the gravitational interaction must be also introduced in the bulk. There is a lot of papers devoted to the different localization mechanisms of the bulk fields in various brane world models.", "ground_truth": ["localized on the brane", "warped geometry", "randall–sundrum model", "five dimensions", "spin 0 field is localized on a brane", "positive tension", "localizes the graviton", "positive tension", "negative tension", "positive tension", "negative tension", "six space–time dimensions", "localized on the brane", "standard model", "localization", "positive tension", "introduced in the bulk", "localization mechanisms", "brane world models"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "local fields", "gravitational field", "brane", "warped geometry", "randall–sundrum model", "five dimensions", "spin 0 field", "positive tension", "graviton", "spin 1/2 fields", "spin 3/2 fields", "negative tension", "spin 1 field", "six space–time dimensions", "gauge field", "standard model particles", "additional interactions", "gravitational interaction", "bulk", "localization mechanisms", "bulk fields", "brane world models"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "First-principles calculations have clarified the electronic structure and stability of the W@Si12 cluster under O2 molecule adsorption and reaction. Our results show that the W-encapsulated Si12 hexagonal prism cage is very inert to oxidation. The O2 molecule only weakly adsorbs onto the cluster at relatively low temperatures, in the range of several tens meV. However, significant reaction barriers (0.593–1.118 eV) for the O2 molecule on the cluster are identified on different adsorption sites, nevertheless, these reaction paths are spin forbidden reactions according to Wingerʼs spin selection rule. These results imply that O2 readily desorb from the cluster surface rather than dissociate and oxide the W@Si12 cluster upon excitations. In high temperature and high pressure conditions, the O2 molecules may dissociate on the preferential edge site by overcoming a significantly large energy barrier.", "ground_truth": ["first-principles calculations", "electronic structure and stability", "w@si12 cluster", "o2", "o2 molecule adsorption and reaction", "oxidation", "w-encapsulated si12 hexagonal prism cage", "o2", "low temperatures", "significant reaction barriers", "0.593–1.118 ev", "o2", "cluster", "cluster", "spin forbidden reactions", "o2", "cluster", "w@si12 cluster", "high temperature and high pressure conditions", "o2", "overcoming a significantly large energy barrier", "adsorption", "reaction", "adsorption", "reaction", "wingerʼs spin selection rule"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "first-principles calculations", "electronic structure", "stability", "w@si12 cluster", "o2 molecule adsorption", "o2 molecule reaction", "w-encapsulated si12 hexagonal prism cage", "inert to oxidation", "weak adsorption", "low temperatures", "reaction barriers (0.593–1.118 ev)", "adsorption sites", "spin forbidden reactions", "winger’s spin selection rule", "o2 desorption", "cluster surface", "17"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The length effect is always important in nanodevices. So we investigate the length dependence of electronic transport properties in M3 by increasing the number of carbon unit cells in the scattering region. Here we present the transport results when the numbers of carbon unit cells in the scattering region are 10 and 12, which are called M4 and M5, respectively. The current–voltage characteristics shown in Fig. 8. We can see that the large rectifying ratio still can be observed irrespective of the length of heterojunctions. This is due to the fact that the electronic transport properties for M3 are mainly determined by the parity of the π and π⁎ subbands of left and right electrodes. Thees results indicate that the lengths of the two parts in the scattering regions have no affects on the qualitative charge transport in M3.", "ground_truth": ["nanodevices", "length effect", "length dependence of electronic transport properties", "increasing the number of carbon unit cells", "carbon unit cells", "carbon unit cells", "current–voltage characteristics", "large rectifying ratio", "electronic transport properties", "qualitative charge transport", "π and π⁎ subbands"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "length effect", "nanodevices", "length dependence", "electronic transport properties", "m3", "carbon unit cells", "scattering region", "transport results", "m4", "m5", "current–voltage characteristics", "rectifying ratio", "heterojunctions", "π subbands", "π⁎ subbands", "left and right electrodes", "qualitative charge transport", "scattering regions"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Topological insulators (TIs) are promising candidates of spintronics materials because of their robust helical surface states and the extremely strong spin–orbit interaction [1–3]. Initially, binary chalcogenides Bi2Te3, Sb2Te3 and Bi2Se3 have been identified as three-dimensional TIs by surface sensitive probes such as angle resolved photoemission spectroscopy and scanning tunneling microscopy/spectroscopy. Later, ternary chalcogenide (BixSb1−x)2Te3 [4,5], which has similar tetradymite structure to the parent compounds Bi2Te3 and Sb2Te3, was predicted by ab initio calculations and confirmed by ARPES measurements as a tunable topological insulator whose Fermi energy and carrier density can be adjusted via changing the Bi/Sb composition ratio with stable topological surface state for the entire composition range. Combined with magnetism or superconductivity, TIs have attracted great attention due to the rich variety of new physics and applications. The ferromagnetism in several transition metal (TM) doped TIs, which breaks the time-reversal symmetry, has been reported [6–13]. Ferromagnetism in TIs is important because the combination of magnetism with TIs makes a good platform to study fundamental physical phenomena, such as the quantum anomalous Hall effect [14–17], Majorana fermions [18], image magnetic monopole effect [19], and topological contributions to the Faraday and Kerr magneto-optical effect [20].", "ground_truth": ["topological insulators", "tis", "spintronics materials", "spin–orbit interaction", "binary chalcogenides", "bi2te3", "sb2te3", "bi2se3", "tis", "surface sensitive probes", "angle resolved photoemission spectroscopy", "scanning tunneling microscopy/spectroscopy", "ternary chalcogenide", "(bixsb1−x)2te3", "bi2te3", "sb2te3", "ab initio calculations", "arpes measurements", "topological insulator", "changing the bi/sb composition ratio", "magnetism", "superconductivity", "tis", "ferromagnetism", "transition metal", "tm", "tis", "ferromagnetism", "magnetism", "tis", "tis", "hall effect", "majorana fermions", "image magnetic monopole effect", "faraday and kerr magneto-optical effect", "fundamental physical phenomena"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "topological insulators (tis)", "spintronics materials", "robust helical surface states", "spin–orbit interaction", "binary chalcogenides", "bi2te3", "sb2te3", "bi2se3", "three-dimensional tis", "angle resolved photoemission spectroscopy (arpes)", "scanning tunneling microscopy/spectroscopy", "ternary chalcogenide", "(bixsb1−x)2te3", "tetradymite structure", "ab initio calculations", "fermi energy"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Observations show that in the same area with dimensions of a few tenths of a parsec could be many sources, some of which only emits OH lines, and some – only lines H2O. The only known in physics the emission mechanism that can give tremendous power within a narrow range of the spectrum, is coherent (i.e. the same phase and direction) light lasers, which are called optical lasers, and radio-masers. Cosmic maser radio sources emitting in the lines of the molecules have an extremely high brightness temperature radiation Tb. In the molecules of methanol masers (CH3OH) Tb value can reach 109 K, with masers hydroxyl molecules (OH) 6×1012 K. The typical size of the maser clusters is about 1014–1015 m and the neutron star radius is of the order of 10 km. Thus, the radiation dilution coefficient is equaled approximately (2.5×10−23)–(2.5×10−21) and, therefore, μB2B2/4(hν)2∼(2.4×10−5)–(2.4×10−7) for the hydrogen line 21 cm and of the order 10−5–10−7 for the OH 18 cm line or the same order as Eq. (1).", "ground_truth": ["light lasers", "optical lasers", "radio-masers", "methanol masers", "ch3oh", "masers hydroxyl molecules", "oh", "cosmic maser radio sources", "maser clusters", "neutron star", "radiation dilution coefficient", "observations", "oh lines", "h2o", "hydrogen line", "emission mechanism"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "observations", "area", "dimensions", "parsec", "sources", "oh lines", "h2o lines", "emission mechanism", "power", "narrow range", "spectrum", "coherent light", "lasers", "optical lasers", "radio-masers", "cosmic maser", "radio sources", "molecules", "brightness temperature", "radiation", "tb", "methanol masers", "ch3oh", "hydroxyl molecules", "oh", "maser clusters", "size", "neutron star", "radius"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In exploring the WKB limit of quantum theory, Bohm [2] was the first to notice that although one starts with all the ambiguities about the nature of a quantum system, the first order approximation fits the ordinary classical ontology. By that we mean that the real part of the Schrödinger equation under polar decomposition of the wave function becomes the classical Hamilton–Jacobi equation in the limit where terms involving ℏ are neglected. In contrast to this approach, in this Letter we show that the classical trajectories arise from a short-time quantum propagator when terms of O(Δt2) can be neglected. This fact was actually already observed by Holland some twenty years ago: In page 269 of his book [6] infinitesimal time intervals are considered whose sequence constructs a finite path. It is shown that along each segment the motion is classical (negligible quantum potential), and that it follows that the quantum path may be decomposed into a sequence of segments along each of which the classical action is a minimum. The novel contribution of the present Letter is an improved proof of Hollandʼs result using an improved version of the propagator due to Makri and Miller [9,10]. (See also de Gosson [3] for a further discussion.)", "ground_truth": ["short-time quantum propagator", "propagator", "wkb limit of quantum theory", "schrödinger equation under polar decomposition of the wave function", "hamilton–jacobi equation", "infinitesimal time intervals"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "wkb limit", "quantum theory", "bohm", "first order approximation", "classical ontology", "real part", "schrödinger equation", "polar decomposition", "wave function", "classical hamilton–jacobi equation", "ℏ (planck's constant)", "classical trajectories", "short-time quantum propagator", "o(δt²)", "holland", "infinitesimal time intervals", "quantum potential", "classical action", "makri and miller", "de gosson", "improved proof", "propagator"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The goal of the glued trees (GT) algorithm for quantum search is the following: beginning from the left-most vertex of a given GT graph, traverse the graph and reach the right-most vertex, referred to as the target vertex. Childs et al. [1] use this algorithm to show quantum walk search to be fundamentally more effective than classical random walk search by presenting a class of graphs (the GT graphs) that force classical random walks to make exponentially many queries to an oracle encoding the structure of the graph, but that are traversable by quantum walks with a polynomial number of queries to such an oracle. In order to study the robustness of the algorithm to the detrimental effects of decoherence, we shall determine how effectively it achieves its goal when subjected to an increasing degree of phase damping noise. For this reason, we will focus on the probability that the walker is on the target vertex at the end of the walk. We thus consider GT graphs such as the one illustrated in Fig. 1(b), i.e. consisting of n layers before the gluing stage, and thus labelled as G′n.", "ground_truth": ["glued trees", "gt", "quantum search", "gt graph", "quantum walk search", "classical random walk search", "gt graphs", "class of graphs", "classical random walks", "gt graphs", "gluing stage", "graph", "increasing degree of phase damping noise"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "glued trees (gt) algorithm", "quantum search", "left-most vertex", "gt graph", "traverse", "target vertex", "childs et al.", "quantum walk search", "classical random walk search", "exponentially many queries", "oracle", "polynomial number of queries", "robustness", "decoherence", "phase damping noise", "probability", "walker", "n layers", "gluing stage", "g′n", "fig. 1(b)", "these keywords capture the main concepts", "entities", "and technical terms discussed in the"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Another remarkable feature of the quantum field treatment can be revealed from the investigation of the vacuum state. For a classical field, vacuum is realized by simply setting the potential to zero resulting in an unaltered, free evolution of the particle's plane wave (|ψI〉=|ψIII〉=|k0〉). In the quantized treatment, vacuum is represented by an initial Fock state |n0=0〉 which still interacts with the particle and yields as final state |ΨIII〉 behind the field region(19)|ΨI〉=|k0〉⊗|0〉⇒|ΨIII〉=∑n=0∞t0n|k−n〉⊗|n〉 with a photon exchange probability(20)P0,n=|t0n|2=1n!e−Λ2Λ2n The particle thus transfers energy to the vacuum field leading to a Poissonian distributed final photon number. Let's consider, for example, a superconducting resonant circuit as source of the field. The magnetic field along the axis of a properly shaped coil is well approximated by the rectangular form. A particle with a magnetic dipole moment passing through the coil then interacts with the circuit and excites it with a measurable loss of kinetic energy even if the circuit is initially uncharged and there is classically no field it can couple to. The phenomenon that vacuum in quantum field theory does not mean to “no influence” as known from Casimir forces or Lamb shift is clearly visible here as well.", "ground_truth": ["quantum field treatment", "quantized treatment", "superconducting resonant circuit", "vacuum state", "vacuum", "vacuum", "magnetic field", "properly shaped coil", "vacuum", "casimir forces or lamb shift", "particle's plane wave", "particle"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "quantum field treatment", "vacuum state", "classical field", "potential", "free evolution", "plane wave", "quantized treatment", "fock state", "photon exchange probability", "poissonian distribution", "final photon number", "superconducting resonant circuit", "magnetic field", "rectangular form", "magnetic dipole moment", "kinetic energy loss"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The systems in which the Stern–Gerlach force is most prominent are those with a high electromagnetic field gradient. Section 2 considers the implications of the coupling between the spin of a classical electron and the rapidly varying electromagnetic field produced by a laser-driven plasma wave. Sufficiently short, high-intensity laser pulses can form longitudinal waves within the electron density of a plasma. These density waves propagate with speed comparable to the group speed of the laser pulse. Not all plasma electrons form this wave, however; some of the electrons are caught up in the wave and accelerated by its high fields. The wave eventually collapses as these electrons damp the wave (the wave ‘breaks’). The extremely high electric field gradient of a plasma wave near wavebreaking provides an excellent theoretical testing ground for the effects of Stern–Gerlach-type contributions to the trajectory of a test electron.", "ground_truth": ["laser-driven plasma wave", "high-intensity laser pulses", "stern–gerlach-type contributions", "electron", "electromagnetic field", "electrons", "high electric field gradient of a plasma wave", "electron", "stern–gerlach force", "electromagnetic field gradient", "electrons", "plasma electrons", "density waves", "longitudinal waves", "plasma", "laser pulse", "laser", "wave ‘breaks’", "electrons damp the wave"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "stern–gerlach force", "electromagnetic field gradient", "coupling", "spin", "classical electron", "rapidly varying electromagnetic field", "laser-driven plasma wave", "high-intensity laser pulses", "longitudinal waves", "electron density", "plasma", "density waves", "group speed", "laser pulse", "plasma electrons", "accelerated", "high fields", "wave collapses", "wavebreaking", "electric field gradient", "theoretical testing ground", "trajectory", "test electron"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Flow-induced deformations can lead to irreversible changes in the structure of a polymeric fluid; if the rate of extension far exceeds the rate of relaxation, then the polymer chain can be broken. Mechanical degradation of polymers in extensional flow has long been recognised [30] and leads to a reduction in the average molecular weight. A-Alamry et al. [1] have recently reported evidence of flow-induced polymer degradation in DoD jetting. Central scission is observed for polystyrene in a number of good solvents under certain jetting conditions for a bounded range of molecular weights. Since only those molecules that are fully extended can be fractured at the centre of the polymer chain [29], in this paper we investigate whether flow-induced central scission is possible under the conditions of DoD jetting.", "ground_truth": ["polymeric fluid", "flow-induced central scission", "dod jetting", "dod jetting", "polymers", "polystyrene", "central scission", "reduction in the average molecular weight"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "flow-induced deformations", "irreversible changes", "polymeric fluid", "rate of extension", "rate of relaxation", "polymer chain", "mechanical degradation", "extensional flow", "average molecular weight", "a-alamry et al.", "flow-induced polymer degradation", "dod jetting", "central scission", "polystyrene", "good solvents", "jetting conditions", "bounded range", "molecular weights", "fully extended", "fractured", "flow-induced central scission"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This conclusion is a consequence of the high jet speeds and small nozzle diameters in combination with the relatively high viscosity solvent and modest molecular weights of the polystyrene, which results in high Weissenberg numbers and moderate values of the extensibility, L studied here. As discussed in earlier papers [3,6], other jetting fluid combinations, such as those of de Gans et al. [4], lie in a different jetting regime where full extension does not occur and relaxation time controls the viscoelastic behaviour. Consequently inkjet fluid assessment methods need to provide a full characterisation including both linear and nonlinear viscoelastic properties. This complexity suggests assessments of inkjet fluids might have to include jetting from sets of DoD print head devices with different sensitivities to all the various VE parameters [37], rather than reliance on testing without jetting. This was not the expected outcome from the present work but does echo the very pragmatic viewpoint expressed as a “map of misery” by Clasen et al. [38] and may provide a way forward for future R&D strategies towards ink testing.", "ground_truth": ["jetting fluid combinations", "inkjet fluid assessment methods", "inkjet fluids", "jetting", "ink testing", "dod print head devices", "polystyrene", "provide a full characterisation including both linear and nonlinear viscoelastic properties", "“map of misery”"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "jet speeds", "nozzle diameters", "high viscosity solvent", "modest molecular weights", "polystyrene", "weissenberg numbers", "extensibility (l)", "jetting fluid combinations", "de gans et al.", "jetting regime", "full extension", "relaxation time", "viscoelastic behaviour", "inkjet fluid assessment methods", "linear viscoelastic properties"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Denier and Hewitt [12] have shown that bounded solutions to 9a, 9b and 9c subject to (10a) and (10b) exist only in the shear-thinning case for n>12. In the shear-thickening case they have shown that solutions become non-differentiable at some critical location ηc, and although it transpires that this singularity can be regularised entirely within the context of the power-law model, we will not consider such flows here. Thus in this study we will consider flows with power-law index in the range 12<n⩽1. They have also shown that for 12<n<1 to ensure the correct algebraic decay in the numerical solutions one must apply the Robin condition(11)(u¯′,v¯′)=nη(n-1)(u¯,v¯)asη→∞,at some suitably large value of η=η∞≫1. In the Newtonian case this relationship becomes singular, this is due to the fact that when n=1 the functions u¯ and v¯ decay exponentially. Cochran [13] showed that in this case(12)(u¯′,v¯′)=w¯∞(u¯,v¯)asη→∞,where w∞=-2∫0∞u¯dη.", "ground_truth": ["flows", "solutions", "power-law model", "robin condition(11)", "shear-thickening", "singularity", "flows", "correct algebraic decay"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "denier and hewitt", "bounded solutions", "shear-thinning", "shear-thickening", "non-differentiable", "critical location (ηc)", "singularity", "regularised", "power-law model", "power-law index (n)", "range (12 < n ⩽ 1)", "algebraic decay", "numerical solutions", "robin condition", "newtonian case", "exponential decay", "cochran", "functions (u¯", "v¯)", "w¯∞", "integral (∫)"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In order for DLS based micro-rheology to be successful, there must be sufficient scattering contrast between the sample and the tracer particles. In order to achieve this, the maximum possible concentration of tracer particles was added such that single scattering events still dominated (as determined by measurements of diffusion coefficients in water at different concentrations). In order to determine whether or not the background scattering from the sample was sufficiently low compared to that of the tracer particles, we also compared the scattering intensities obtained from samples with and without tracer particles as a function of time. The results of this exercise are shown in Fig. 6. From this figure it can be seen that although initially the scattering from the sample without tracer particles is low compared to those containing tracer particles, as gelation proceeds this eventually ceases to be the case. This is presumably because of the development of supra-molecular structures, such as those seen previously (Fig. 2B). Based on the results in Fig. 6 it was decided to only use data collected in the first 240min of the experiment, after which point the scattering from the gel network became rather too large to ignore.", "ground_truth": ["dls based micro-rheology", "scattering intensities", "scattering from the gel network", "sample and the tracer particles", "tracer particles", "water", "tracer particles", "tracer particles", "scattering", "tracer particles", "tracer particles", "gelation", "supra-molecular structures"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "dls (dynamic light scattering)", "micro-rheology", "scattering contrast", "sample", "tracer particles", "concentration", "single scattering events", "diffusion coefficients", "water", "background scattering", "scattering intensities", "gelation", "supra-molecular structures", "gel network", "fig. 6", "fig. 2b", "240min"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Tack is an important property of a PSA as it quantifies its ability to form instantly a bond when brought into contact with a surface. The final adhesion and cohesive strength of the bond are influenced by numerous factors including the surface energies of the adhesive and substrate, dwell time, contact pressure, mechanical properties of the adhesive, as well as environmental conditions such as temperature and humidity [8]. Therefore, tack is important in many applications where an instant bond is required, however it is equally important when a ‘clean’ separation of the initially bonded surfaces is desirable. Many different methods for measuring the tack have been devised with the four main ones being the rolling ball, loop tack, quick stick and probe tack tests [9]. Each has its own advantages and disadvantages and the specific testing method should be selected based on the particular application.", "ground_truth": ["‘clean’ separation", "rolling ball", "loop tack", "quick stick", "probe tack", "tests", "tack", "tack", "psa", "adhesive", "adhesive", "substrate", "bonded surfaces"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "tack", "psa (pressure-sensitive adhesive)", "bond", "surface", "final adhesion", "cohesive strength", "surface energies", "adhesive", "substrate", "dwell time", "contact pressure", "mechanical properties", "environmental conditions", "temperature", "humidity", "instant bond", "clean separation", "rolling ball test", "loop tack test", "quick stick test", "probe tack test", "testing methods", "advantages", "disadvantages", "application", "let me know if you'd like any modifications or additional details!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The first of these systems, a biopolymer gel, involves the thermoreversible gelation of aqueous gelatin solutions to form a physical gel, whereas the other systems considered herein involve the formation of chemical gels featuring permanent cross-linked branching networks. The second system is a commercial silicone dielectric gel (SDG) which is used in the production of electronic products created by industrial printing processes. The third experimental system is a fibrin gel formed by the thrombin-induced polymerisation of fibrinogen molecules. The gel network product in the latter case forms the principal microstructural component of a blood clot [8]. The latter case is particularly interesting as the critical-gel which is established at the GP serves as a ‘template’ for the ensuing development of microstructure and associated rheological properties in the post-GP phase of fibrin clot evolution [9].", "ground_truth": ["thermoreversible gelation", "cross-linked branching networks", "commercial silicone dielectric gel", "sdg", "fibrin gel", "thrombin-induced polymerisation", "fibrinogen molecules", "biopolymer gel", "production of electronic products", "gel network product", "microstructural component", "blood clot", "critical-gel", "gp", "‘template’ for the ensuing development of microstructure and associated rheological properties", "fibrin clot", "gp"], "prediction_list": ["here are the extracted keywords from the paragraph:", "biopolymer gel", "thermoreversible gelation", "aqueous gelatin solutions", "physical gel", "chemical gels", "permanent cross-linked branching networks", "silicone dielectric gel (sdg)", "electronic products", "industrial printing processes", "fibrin gel", "thrombin-induced polymerisation", "fibrinogen molecules", "gel network", "blood clot", "critical-gel", "gp (gel point)", "microstructure", "rheological properties", "post-gp phase", "fibrin clot evolution", "let me know if you'd like any modifications or additional details!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A convenient and widely reported technique for detection of the GP involves measurements of the complex shear modulus, G∗, over a range of frequencies, ω, in oscillatory shear. At the GP the elastic and viscous components of the complex modulus, G′ and G″, respectively scale in oscillatory frequency, ω, as G′(ω)∼G″(ω)∼ωα where α is termed the stress relaxation exponent [15]. Thus, the GP may be identified as the instant where the G′ and G″ scale in frequency according to identical power laws [15], behaviour corresponding to attainment of a frequency independent phase angle, δ(=atan(G″/G′)). GP measurements may involve ‘frequency sweeps’ with repeated consecutive application of a set of small amplitude oscillatory shear, SAOS, waveforms [15,16], or by Fourier Transform Mechanical Spectroscopy, FTMS, in which G∗(ω) is found by simultaneous application of several harmonic frequencies in a composite waveform and its subsequent Fourier analysis [17,18]. Frequency sweeps are limited to relatively slow gelation processes due to sample mutation and interpolation errors [9,19,20]. FTMS may overcome these limitations, but is unsuitable for markedly strain sensitive materials, such as fibrin gels, due to the strain amplitude of the composite waveform exceeding the linear viscoelastic range (LVR) [9].", "ground_truth": ["gp measurements", "fourier transform mechanical spectroscopy", "ftms", "ftms", "markedly strain sensitive materials", "fibrin gels", "measurements of the complex shear modulus", "detection of the gp", "application of several harmonic frequencies", "fourier analysis", "small amplitude oscillatory shear", "saos", "waveforms", "composite waveform", "gelation processes", "composite waveform", "frequency sweeps"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "gp (gel point)", "complex shear modulus (g∗)", "frequencies (ω)", "oscillatory shear", "elastic component (g′)", "viscous component (g″)", "stress relaxation exponent (α)", "power laws", "phase angle (δ)", "frequency sweeps", "small amplitude oscillatory shear (saos)", "fourier transform mechanical spectroscopy (ftms)", "composite waveform", "fourier analysis", "gelation processes"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Equilibrium surface tension was measured at 21°C with a SITA pro line t-15 bubble tensiometer. Rheological measurements were performed with an ARES rheometer at shear rates up to 15s−1 and with a piezo axial vibrator [21] (PAV) at frequencies up to 6kHz. Table 1 shows the measured values of viscosity (the real component η′ of complex viscosity) at 1s−1 and 4000s−1 and of surface tension for the solutions with and without the surfactant mixture. For the most concentrated (1.1wt%) solution, viscosity fell from >60mPas at low shear rate to about 4mPas at the highest shear rates. The PEDOT:PSS fluids also exhibited elasticity that steadily reduced with increasing frequency [4]. All the aqueous PEDOT:PSS solutions shear-thinned significantly, but the presence of surfactants did not affect the trends in the rheological behaviour, particularly at the higher frequencies (10–4000s−1).", "ground_truth": ["sita pro line t-15 bubble tensiometer", "rheological measurements", "ares rheometer", "piezo axial vibrator", "pav", "surface tension", "pedot:pss fluids", "pedot:pss solutions", "values of viscosity", "equilibrium surface tension", "most concentrated (1.1wt%) solution"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "equilibrium surface tension", "21°c", "sita pro line t-15 bubble tensiometer", "rheological measurements", "ares rheometer", "shear rates", "15s−1", "piezo axial vibrator (pav)", "frequencies", "6khz", "viscosity", "real component η′", "complex viscosity", "1s−1", "4000s−1", "surface tension", "surfactant mixture", "concentrated solution", "1.1wt"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, we propose a general agent-based distributed framework where each agent is implementing a different metaheuristic/local search combination. Moreover, an agent continuously adapts itself during the search process using a direct cooperation protocol based on reinforcement learning and pattern matching. Good patterns that make up improving solutions are identified and shared by the agents. This agent-based system aims to provide a modular flexible framework to deal with a variety of different problem domains. We have evaluated the performance of this approach using the proposed framework which embodies a set of well known metaheuristics with different configurations as agents on two problem domains, Permutation Flow-shop Scheduling and Capacitated Vehicle Routing. The results show the success of the approach yielding three new best known results of the Capacitated Vehicle Routing benchmarks tested, whilst the results for Permutation Flow-shop Scheduling are commensurate with the best known values for all the benchmarks tested.", "ground_truth": ["framework", "each agent is implementing a different metaheuristic/local search combination", "adapts", "a direct cooperation protocol", "reinforcement learning", "pattern matching", "good patterns", "agents", "agent-based system", "provide a modular flexible framework", "deal with a variety of different problem domains", "this approach", "embodies a set of well known metaheuristics with different configurations", "two problem domains", "permutation flow-shop scheduling and capacitated vehicle routing", "approach", "capacitated vehicle routing benchmarks", "permutation flow-shop scheduling"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "agent-based", "distributed framework", "metaheuristic", "local search", "direct cooperation protocol", "reinforcement learning", "pattern matching", "improving solutions", "modular flexible framework", "problem domains", "permutation flow-shop scheduling", "capacitated vehicle routing", "best known results", "benchmarks"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "As mentioned earlier, this paper represents ongoing efforts to efficiently address the stochastic MPSP. Future work may consider investigating whether the algorithm would be as successful or not in solving variants of the MPSP that include more operational constraints, such as variable cut-off grade, grade blending, and stockpiling, as it is in solving the “classical” variant considered in this paper. Indeed, it is a general-purpose algorithm and should be applicable to any of these variants. Other research avenues include considering other strategies for updating the penalties within PH and other methods for solving the sub-problems. Finally, another important research direction is the development of other efficient solution approaches. Since it has been observed empirically that the problem formulation often achieves small integrality gaps, one approach could be to solve the linear relaxation of the problem using an efficient algorithm and then to use an LP-rounding procedure to get an integer solution.", "ground_truth": ["stochastic mpsp", "mpsp", "stockpiling", "grade blending", "variable cut-off grade", "lp-rounding procedure", "general-purpose algorithm", "solve the linear relaxation", "operational constraints", "ph"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "stochastic mpsp", "algorithm", "operational constraints", "variable cut-off grade", "grade blending", "stockpiling", "classical variant", "general-purpose algorithm", "research avenues", "updating penalties", "ph (progressive hedging)", "sub-problems", "solution approaches", "integrality gaps", "linear relaxation", "efficient algorithm", "lp"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We propose an equilibrium model that allows to analyze the long-run impact of the electricity market design on transmission line expansion by the regulator and investment in generation capacity by private firms in liberalized electricity markets. The model incorporates investment decisions of the transmission system operator and private firms in expectation of an energy-only market and cost-based redispatch. In different specifications we consider the cases of one vs. multiple price zones (market splitting) and analyze different approaches to recover network cost—in particular lump sum, generation capacity based, and energy based fees. In order to compare the outcomes of our multilevel market model with a first best benchmark, we also solve the corresponding integrated planner problem. Using two test networks we illustrate that energy-only markets can lead to suboptimal locational decisions for generation capacity and thus imply excessive network expansion. Market splitting heals these problems only partially. These results are valid for all considered types of network tariffs, although investment slightly differs across those regimes.", "ground_truth": ["equilibrium model", "analyze the long-run impact of the electricity market design on transmission line expansion", "market splitting", "analyze different approaches to recover network cost", "recover network cost", "lump sum", "generation capacity based", "energy based fees", "multilevel market model", "first best benchmark", "solve the corresponding integrated planner problem", "integrated planner problem", "excessive network expansion", "market splitting"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "equilibrium model", "long-run impact", "electricity market design", "transmission line expansion", "regulator", "investment", "generation capacity", "private firms", "liberalized electricity markets", "investment decisions", "transmission system operator", "energy-only market", "cost-based redispatch", "price zones", "market splitting", "network cost recovery", "lump sum fees", "generation capacity-based fees", "energy-based fees", "multilevel market model", "first best benchmark", "integrated planner problem", "test networks", "suboptimal locational decisions", "excessive"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Regarding the implications of the results of this paper, we note two points. From a practical point of view, we have endowed the weighted additive model with a distance function structure, which takes negative values for points located outside the technology and non-negative values for points into the production possibility set. In this respect, the weighted additive distance function methodologically supports the branch of the literature that resorts to the weighted additive model or some related approach to measure productivity over time (see, for example, Mahlberg & Sahoo, 2011 or Chang et al., 2012). From a theoretical point of view, we have provided a new distance function with some interesting properties in contrast to the usual ones, mainly (1) when technical inefficiency has to be estimated, the weighted additive distance function coincides with the weighted additive model, which means that technical inefficiency is measured following the Pareto-Koopmans notion of efficiency; and (2) when productivity has to be determined and decomposed over time the weighted additive distance function emerges as an attractive tool to be used for cross-period evaluation of returns to scale changes, since this distance function is always feasible, even under Variable Returns to Scale.", "ground_truth": ["implications of the results", "the weighted additive model with a distance function structure", "takes negative values for points located outside the technology and non-negative values for points into the production possibility set", "measure productivity over time", "weighted additive model", "some related approach", "the weighted additive distance function", "a new distance function", "means that technical inefficiency is measured following the pareto-koopmans notion of efficiency", "determined and decomposed over time", "the weighted additive distance function", "weighted additive distance function", "distance function"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "implications", "results", "practical point of view", "weighted additive model", "distance function structure", "negative values", "technology", "non-negative values", "production possibility set", "weighted additive distance function", "methodological support", "literature", "productivity measurement", "theoretical point of view", "new distance function", "properties", "technical inefficiency", "pareto-koopmans notion of efficiency", "productivity decomposition", "cross-period evaluation", "returns to scale changes", "variable returns to scale", "feasibility", "mahlberg & sahoo"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The iron ore may be extracted from blocks of 25×25×12meter3 located at three consecutive mining benches of 12meter height. For this case study, ten equally probable scenarios of iron content, phosphorous, silica, aluminum and LOI are used to quantify the joint uncertainty in the characteristics of the iron ore deposit considered and are the input to the SSTPS formulation proposed in the previous section. The simulated scenarios available were provided and generated using the stochastic simulated technique detailed in Boucher and Dimitrakopoulos (2012). The area considered is bounded by the limits of the given volume of production in the long-term first year production schedule provided. Fig. 4 shows 3 scenarios of iron ore content as well as the corresponding conventional and single estimated (average) representation of iron content (Fe2O3%) for the upper bench. In total, 734 blocks from 3525 to 21,150 tonnes, with Fe2O3 from 54.59% to 60.63%, P from 0.02% to 0.04%, SiO2 from 3.10% to 8.58%, Al2O3 from 0.53% to 1.88% and LOI from 8.75% to 11.75% are available.", "ground_truth": ["iron ore", "iron ore deposit", "sstps formulation", "probable scenarios", "stochastic simulated technique", "scenarios of iron ore content", "734 blocks", "iron content", "phosphorous", "silica", "aluminum", "loi", "quantify the joint uncertainty in the characteristics of the iron ore deposit", "simulated scenarios", "iron", "loi", "al2o3", "fe2o3", "sio2", "p"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "iron ore", "extracted", "blocks", "25×25×12meter3", "mining benches", "12meter height", "case study", "scenarios", "iron content", "phosphorous", "silica", "aluminum", "loi (loss on ignition)", "joint uncertainty", "iron ore deposit", "sstps formulation", "stochastic simulated technique", "boucher and dimitrakopoulos (2012)", "production schedule", "fe2o3%", "upper bench", "tonnes", "p (phosphorous)"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Two-state models are often insufficient to fit complex traces, therefore we also study the approximate fitting of large M3PPs. In the single class setting, a known limitation of MMPPs is the inability to simultaneously fit many statistical descriptors due to the non-linearity of their underlying equations (Bodrog, Heindl, Horváth, & Telek, 2008; Heindl, Horváth, & Gross, 2006; Horváth & Telek, 2009). This has led to the definition of several approaches to fit complex traces by composing multiple small-sized MMPPs or MAPs using Kronecker operators (Andersen & Nielsen, 1998; Casale, Zhang, & Smirni, 2010; Horváth & Telek, 2002). These methods employ composition operators for moment fitting, offering a different trade-off between computational cost and fitting accuracy compared to fitting methods based on the EM algorithm (Breuer, 2002; Horváth & Okamura, 2013; Klemm, Lindemann, & Lohmann, 2003). In particular, the superposition operator allows one to describe a trace by the statistical multiplexing of several MMPPs, at the expense of an exponential growth of the number of states in the resulting process (Sriram & Whitt, 1986). This state space explosion is an obstacle for the application of MMPPs and MAPs to modeling real systems; for example it considerably slows down, or even renders infeasible, the numerical evaluation of queueing models by matrix geometric methods (Bini, Meini, Steffé, Pérez, & Houdt, 2012; Pérez, Velthoven, & Houdt, 2008).", "ground_truth": ["m3pps", "mmpps", "composition operators", "em algorithm", "superposition operator", "matrix geometric methods", "two-state models", "composing multiple small-sized mmpps or maps", "kronecker operators", "mmpps", "mmpps and maps", "modeling real systems", "numerical evaluation of queueing models", "study the approximate fitting of large m3pps", "statistical descriptors", "underlying equations", "state space explosion", "fitting methods", "statistical multiplexing of several mmpps"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "two-state models", "complex traces", "large m3pps", "single class setting", "mmpps", "statistical descriptors", "non-linearity", "bodrog", "heindl", "horváth", "telek", "fitting methods", "kronecker operators", "andersen", "nielsen", "casale", "zhang", "smirni", "composition operators", "moment fitting", "computational cost", "fitting accuracy", "em algorithm", "breuer", "okamura", "klemm", "lindemann", "loh"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "For mixtures described by an equation of state, this calculation amounts to simultaneously solving the condition of thermal, mechanical and diffusive equilibria (equality of chemical potential) amongst two fluid phases for each component of the mixture. The analytical nature of this calculation lends itself to a reasonably rapid solution by numerical methods. In its most common form, the composition and temperature are fixed and the pressures at either the bubble or the dew point are recursively calculated. The reader is referred to the excellent textbooks that describe the common algorithms employed [80–82]. The quality of the result is obviously limited by the accuracy of the EoS to faithfully represent fluid mixtures. Furthermore, the fact that some of the more interesting features of the phase diagram are close to the critical points of the mixture, make these calculations particularly challenging for all but the most optimized and force-fitted of models.", "ground_truth": ["eos", "mixtures", "common algorithms employed", "fluid mixtures", "numerical methods", "pressures at either the bubble or the dew point"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "mixtures", "equation of state (eos)", "thermal equilibrium", "mechanical equilibrium", "diffusive equilibrium", "chemical potential", "fluid phases", "component", "analytical calculation", "numerical methods", "composition", "temperature", "pressure", "bubble point", "dew point", "recursive calculation", "textbooks", "algorithms", "accuracy", "phase diagram", "critical points", "optimized models", "force-fitted models"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The next phase of our current study is to use the parameters obtained from pure-component systems in a transferable manner to represent the corresponding mixtures. Mixtures of n-alkanes and H2O have been studied previously with SAFT-γ SW [82]. In general it is well known that the extreme nature of the phase separation [150] makes it challenging to model mixtures of H2O with non-polar compounds. Because of the large differences in the dielectric constant of the two phases as well as in the dipole moment of H2O and the hydrophobic molecules, it especially difficult to obtain phase-independent unlike interaction parameters [112] and thus to model simultaneously the equilibrium phases. In previous work [82], emphasis was placed on obtaining an accurate description of the alkane-rich phases (both liquid and vapour), while small absolute (but not relative) deviations for the aqueous phase composition were achieved. The systems of interest in our current work are typically aqueous mixtures containing a high proportion of H2O, alkylamine, and CO2. Consequently, in order to provide an improved overall description of the fluid-phase equilibria at the conditions of interest, refinements have been made to the unlike parameters presented in the previous study [129] relating to the interactions between H2O and the alkyl groups, CH3 and CH2, namely ϵCH3,H2O, ϵCH2,H2O and λCH3,H2O, λCH2,H2O.", "ground_truth": ["saft-γ sw", "non-polar compounds", "mixtures of n-alkanes and h2o", "pure-component systems", "mixtures of h2o", "description of the alkane-rich phases", "aqueous mixtures", "h2o", "alkyl groups", "ch3 and ch2", "interactions between h2o and the alkyl groups", "h2o", "liquid", "vapour", "h2o", "alkylamine", "co2", "ϵch3", "h2o", "ϵch2", "h2o", "λch3", "h2o", "λch2", "h2o", "hydrophobic molecules", "fluid-phase equilibria", "fluid"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "parameters", "pure-component systems", "transferable manner", "mixtures", "n-alkanes", "h2o", "saft-γ sw", "phase separation", "non-polar compounds", "dielectric constant", "dipole moment", "hydrophobic molecules", "unlike interaction parameters", "equilibrium phases", "alkane-rich phases", "liquid", "vapour", "aqueous phase composition", "aqueous mixtures", "alkylamine", "co2", "fluid-phase equilibria", "refinements", "unlike parameters", "interactions", "alkyl groups", "ch3"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Recently, fundamental (thermophysical property) research on ionic clathrate hydrates has experienced remarkable growth, particularly over the last ten years [21–30]. Previously, beginning with the first paper on unusual hydrates of tetrabutylammonium salts in 1940 [31], a number of studies could be found on ionic clathrate hydrates (hereafter, semiclathrate hydrates) [32–35] before the unified terminology semiclathrate hydrate was generally accepted. Semiclathrate hydrates have been attracting increased attention because of their promising applications as phase change materials for refrigeration systems and in gas capture and storage [36–41]. In addition, there is interesting speculation that semiclathrate hydrate may be regarded as a representative substance for the study of thermal conductivity in clathrate hydrate in general. This is because: (1) it can reduce characterization problems as a solid sample, since semiclathrate hydrate is formed around ambient temperature under atmospheric pressure and is easy to handle; (2) accurately measuring the thermal conductivity of semiclathrate hydrates, which have many similarities to clathrate hydrates, may make possible a deeper understanding of the unique (anomalous) behavior of the thermal conductivity of clathrate hydrates; and (3) currently, there are no experimental studies on the thermal conductivity of semiclathrate hydrates.", "ground_truth": ["ionic clathrate hydrates", "ionic clathrate hydrates", "unusual hydrates of tetrabutylammonium salts", "semiclathrate hydrates", "semiclathrate hydrates", "phase change materials for refrigeration systems", "gas capture and storage", "semiclathrate hydrate", "study of thermal conductivity", "semiclathrate hydrate", "measuring the thermal conductivity", "semiclathrate hydrates", "clathrate hydrates", "clathrate hydrates", "semiclathrate hydrates", "semiclathrate hydrate", "thermal conductivity"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fundamental research", "thermophysical property", "ionic clathrate hydrates", "semiclathrate hydrates", "tetrabutylammonium salts", "phase change materials", "refrigeration systems", "gas capture and storage", "thermal conductivity", "clathrate hydrate", "anomalous behavior", "experimental studies", "atmospheric pressure", "ambient temperature", "character"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "With development of performance-based design, some studies have been conducted on fire risk analysis in buildings from different perspectives and levels. Models such as FiRECAM [11,12] and FiERAsystem [13] were used to calculate the expected life risk. In other studies probabilistic methods have been used to assess levels of people safety in buildings [14]. Quantitative risk analysis approaches have also been used to quantify the risk to occupants using stochastic factors [15]. However, studies to date have largely been concerned with various aspects of fire risk analysis and there has been little in the way of development of systematic theoretical methods for analyzing fire risk in buildings in terms of fire risk management. Existing fire risk management involves the identification of alternative fire safety design options [16,17], the ongoing inspection, maintenance of fire protection systems [18] and evacuation training and drills [19]. In this study, basic process of fire risk analysis in building is described, and a fire risk analysis model based on scenario clusters is established with consideration of the characteristics of fire dynamics and occupants' behavior. The number of deaths and directive property loss are selected as fire risk indices and the average fire risk of residential buildings is quantitatively analyzed, so that appropriate fire risk management measures can be adopted.", "ground_truth": ["performance-based design", "firecam", "fierasystem", "models", "probabilistic methods", "quantitative risk analysis approaches", "stochastic factors", "aspects of fire risk analysis", "basic process of fire risk analysis", "number of deaths and directive property loss", "average fire risk", "systematic theoretical methods", "identification of alternative fire safety design options", "ongoing inspection", "maintenance of fire protection systems", "evacuation training and drills"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "performance-based design", "fire risk analysis", "buildings", "models", "firecam", "fierasystem", "expected life risk", "probabilistic methods", "people safety", "quantitative risk analysis", "stochastic factors", "systematic theoretical methods", "fire risk management", "fire safety design options", "inspection", "maintenance", "fire protection systems", "evacuation training", "drills", "scenario clusters", "fire dynamics", "occupants' behavior", "deaths", "property loss", "fire risk indices", "residential buildings", "risk management measures"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The mentioned difficulties associated with the calibration process inspired the concept of inverse modelling. In this case, the experimental data become entirely integrated in the calibration process and an optimization routine is used to quantify the best set of parameters which explain the observed pyrolysis behaviour (i.e. multivariable curve fitting). The most used experimental data for model calibration have been the mass loss rate and the surface temperature [10–12]. The optimization technique used is function of the number of variables and their interactions. In the past, only the few most uncertain parameters (i.e. the kinetics parameters) were generally used as potentiometers [13]. However, sophisticated mathematical procedures have been developed to increase the number of parameters optimized simultaneously (e.g. Genetic Algorithm (GA) [10,14] or Shuffled Complex Evolution (SCE) [11]). Lautenberger and Fernandez-Pello [12] have recently investigated the influence that the choice of algorithm can have on the optimized parameters. They generated using their code GPYRO a set of synthetic data (mass loss rate and surfaces temperature) and tried with different algorithms to find back the set of input parameters. The four optimization algorithms provided results with an absolute average error between 1% and 25%. SCE was the most suitable algorithm. The use of synthetic data conveniently avoids the problem of agreement between the actual physical phenomena and any modelling assumption.", "ground_truth": ["calibration process", "inverse modelling", "experimental data", "calibration process", "optimization routine", "pyrolysis behaviour", "multivariable curve fitting", "experimental data", "model calibration", "mass loss rate", "surface temperature [", "optimization technique", "potentiometers", "parameters", "kinetics parameters", "mathematical procedures", "parameters", "genetic algorithm", "ga", "shuffled complex evolution", "sce", "choice of algorithm", "gpyro", "synthetic data", "algorithms", "optimization algorithms", "sce", "algorithm", "synthetic data", "quantify the best set of parameters"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "calibration process", "inverse modelling", "experimental data", "optimization routine", "parameters", "pyrolysis behaviour", "multivariable curve fitting", "mass loss rate", "surface temperature", "optimization technique", "variables", "interactions", "uncertain parameters", "kinetics parameters", "potentiometers", "mathematical procedures", "genetic algorithm (ga)", "shuffled complex evolution (sce)", "algorithm", "optimized parameters", "gpyro", "synthetic data", "absolute average error", "modelling assumption"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "RemarkThe purely radiative spacetimes used as reference solutions in our analysis are not perturbations of the Minkowski spacetime. A way of seeing this is to consider the Newman–Penrose constants of the spacetime. The Newman–Penrose constants are a set of absolutely conserved quantities defined as integrals of certain components of the Weyl tensor and the Maxwell fields over cuts of null infinity—see [19–21] for the Einstein–Maxwell case. In [22] it has been shown that the value of the Newman–Penrose constants for a vacuum radiative spacetime coincides with the value of the rescaled Weyl spinor at i+—this result can be extended to the electrovacuum case using the methods of this article. For the radiative spacetimes arising from the construction of [17] it can be seen that the value of the Weyl spinor at i+ is essentially the mass quadrupole of the seed static spacetime. It follows, that the Newman–Penrose constants of the radiative spacetime can take arbitrary values. On the other hand, for the Minkowski spacetime, the Newman–Penrose constants are exactly zero, and those of perturbations thereof will be small. Thus, in this precise sense, our radiative spacetimes are, generically, not perturbations of the Minkowski spacetime, unless all the Newman–Penrose constants vanish.", "ground_truth": ["reference solutions", "minkowski spacetime", "radiative spacetimes", "newman–penrose constants", "newman–penrose constants", "newman–penrose constants", "vacuum radiative spacetime", "radiative spacetimes", "newman–penrose constants", "radiative spacetime", "minkowski spacetime", "newman–penrose constants", "radiative spacetimes", "minkowski spacetime", "newman–penrose constants", "weyl tensor", "maxwell fields"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "radiative spacetimes", "reference solutions", "perturbations", "minkowski spacetime", "newman–penrose constants", "conserved quantities", "weyl tensor", "maxwell fields", "null infinity", "einstein–maxwell case", "vacuum radiative spacetime", "rescaled weyl spinor", "electrovacuum case", "mass quadrupole", "seed static"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "To calculate hedonic price indices in the linear model, the initial or reference price has to be calculated (Triplett, 2006). The present study adopts the approach of de Haan and Diewert (2013): a price index is constructed using the price generated by the estimated coefficients of a base period regression model, and it is calculated based on the based period average values of a given cell phone plan characteristic z¯ for each operator (Supplementary Table S5). For continuous characteristics, direct averages are used; for binary characteristics, the proportions of cell phone plans containing the feature are used. The resulting prices for this average cell phone plan are converted to an index by applying previously calculated pure price changes (δs). Finally, the overall hedonic price index is calculated as the weighted average of firm-level indices. Weights correspond to the relative proportion of cell phone plans by operator in the sample (0.3534 for HT, 0.3212 for Vip, and 0.3254 for Tele2).", "ground_truth": ["linear model", "calculate hedonic price indices", "base period regression model", "applying previously calculated pure price changes"], "prediction_list": ["here are the keywords extracted from the given scientific paragraph:", "hedonic price indices", "linear model", "initial price", "reference price", "triplett (2006)", "de haan and diewert (2013)", "price index", "base period regression model", "estimated coefficients", "average values", "cell phone plan", "characteristic (z¯)", "operator", "supplementary table s5", "continuous characteristics", "binary characteristics", "proportions", "pure price changes (δs)", "weighted average", "firm-level indices", "relative proportion", "sample", "ht"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Social network gaming, which refers to playing games that are connected to social networking services (SNS) directly, or through mobile applications (apps), is a popular online activity. Social network games (SNG) are generally free-to-play and do not award monetary prizes, but users can make in-game purchases to advance within the game, customise the game, give gifts to friends, and access other exclusive benefits and features, leading to these games being referred to as ‘freemium’. Although SNG are connected to a SNS and encourage users to interact with their connections, most SNG can be played without any social interaction. SNG have grown rapidly in popularity and the global SNG market is predicted to grow annually at 16% from 2013 to 2019 to reach a total market value of US$17.4 billion (Transparency Market Research, 2015). A survey of Facebook users in Australia in November 2012 reported that there are over 3.5 million social gamers across Australia and almost 70% play SNG daily (Spiral Media, 2013), and it is highly likely that the use of SNG has increased since this time.", "ground_truth": ["sng market", "survey", "social networking services", "sns", "social network gaming", "mobile applications", "apps", "social network games", "sng", "sng", "sns", "sng", "sng", "sng", "sng"], "prediction_list": ["here are the extracted keywords from the paragraph:", "social network gaming", "social networking services (sns)", "mobile applications (apps)", "social network games (sng)", "free-to-play", "in-game purchases", "freemium", "social interaction", "market growth", "market value", "us$17.4 billion", "transparency market research", "facebook users", "australia", "3.5 million social gamers", "daily play"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "However, the measured reflectivity is less than the predicted value (∼96%), which is likely to relate to, amongst other factors, the roughness of the GaN/AlN interfaces particularly for the first layer in the DBR stack and the non-uniformity of the DBR layer thicknesses. Using STEM measurements of the thickness of each layer (on the a-plane) through the thickness of the stack, we calculate a new model (green curve) in which the overall reflectivity is reduced to 85%. This implies that variations in layer thickness through the stack are the main source of the reduced reflectivity in comparison to the model. In fact, a closer look at the cross-sectional STEM data and a careful extraction of layer thickness have revealed that whilst the layer thicknesses are fairly consistent through the DBR stack in the wing regions, there is a monotonic variation in the measured layer thicknesses in the window regions. (The GaN layer width smoothly increases, while the AlN layer thickness decreases through the DBR stack.). This observation could potentially be of practical importance, for samples grown on templates with a uniform defect density, as one could achieve much better reflectivities simply by altering the growth time to counteract the change in growth rate. This possibility is the subject of ongoing investigations. In addition, the presence of cracks and trenches in the top surface may also reduce the measured reflectivity further.", "ground_truth": ["stem measurements", "calculate a new model", "cross-sectional stem data", "extraction of layer thickness", "altering the growth time", "dbr stack", "dbr stack in the wing regions", "the window regions", "dbr stack", "gan/aln interfaces", "dbr layer", "gan layer", "aln layer"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "reflectivity", "predicted value", "roughness", "gan/aln interfaces", "dbr stack", "non-uniformity", "layer thicknesses", "stem measurements", "a-plane", "model", "reduced reflectivity", "cross-sectional stem data", "wing regions", "window regions", "monotonic variation", "gan layer width", "aln"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages. This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame. Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages. A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer. The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ.", "ground_truth": ["dnns", "other approaches", "i-vectors", "target languages", "evidence from past frames", "test utterance", "dnn", "combine the evidence from past frames", "assume that frames are independent and multiply the posterior estimates of the last layer", "multiplying the output probabilities pl obtained for all of its frames", "accumulating the logs as:(6)sl=1n∑t=1nlogp(ll|xt​", "θ)"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "architecture", "frame level", "single frame", "context", "fed-forward", "network", "class posterior probability", "target languages", "dnns (deep neural networks)", "real-time applications", "i-vectors", "decision", "new frame", "evidence", "past frames", "similarity score", "test utterance", "combination", "frames independent"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.", "ground_truth": ["anat procedure", "acmllr transforms", "gmm–hmm model", "factorisation approach using mllr speaker transforms", "speaker clustering", "speaker adaptation", "adaptive retraining of the gmm–hmm parameters", "training show-based acmllr transforms", "adaptive training", "anat model", "acmllr transforms"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "experiments", "adaptive retraining", "gmm–hmm parameters", "anat procedure", "model improvement", "acmllr transforms", "baseline gmm–hmm model", "show-based acmllr transforms", "adaptive training", "background conditions", "factorisation approach", "mllr speaker transforms", "anat model", "speaker clustering", "speaker adaptation", "absolute improvement", "relative improvement"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The research work in this paper elaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD. It also clearly indicates through numerical simulations and applications to bearing monitoring that the expansion from standard EMD to multivariate EMD is a successful exploration. Using multiple sensors to collect signal from different locations of the machine and using the multivariate EMD to analyze multivariate signal can contribute to comprehensively collect all the frequency components related to any bearing fault, and is beneficial to extract fault information, especially for early weak fault characteristics. Both the characteristic frequencies of simulated signal and the fault frequencies of practical rolling bearing signal can be extracted from the same order of IMF groups, thus showing that multivariate EMD is an effective signal decomposition algorithm and can be competently applied to fault diagnosis of rolling bearings when combined with a multiscale reduction method and fault correlation factor analysis. In signal acquisition and processing, given the circumstance that there is a trend toward the use of multiple sensors, multivariate EMD appears to be very useful and meaningful as a kind of multivariate data processing algorithm. By analyzing the simulated signal and two different practical multivariate signals, the results demonstrate the significance of the proposed method in the field of fault diagnosis of rolling bearing.", "ground_truth": ["multivariate emd", "elaborates on the theoretical effectiveness of the proposed method based on the multivariate emd", "numerical simulations", "applications to bearing monitoring", "bearing monitoring", "standard emd", "multivariate emd", "expansion from standard emd to multivariate emd", "sensors", "multivariate emd", "multivariate signal", "frequency components", "extract fault information", "practical rolling bearing signal", "multivariate emd", "signal decomposition algorithm", "fault diagnosis", "multiscale reduction method", "fault correlation factor analysis", "rolling bearings", "signal acquisition and processing", "sensors", "multivariate emd", "multivariate data processing algorithm", "simulated signal", "practical multivariate signals", "rolling bearing", "fault diagnosis of rolling bearing"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "research work", "theoretical effectiveness", "proposed method", "multivariate emd", "numerical simulations", "bearing monitoring", "standard emd", "multiple sensors", "signal collection", "frequency components", "bearing fault", "fault information", "early weak fault characteristics", "characteristic frequencies", "simulated signal", "practical rolling bearing signal", "imf groups", "signal decomposition algorithm", "fault diagnosis", "rolling bearings", "multiscale"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The GFRFs of nonlinear systems can be determined by either a parametric-model-based method or a nonparametric-model-based method [8]. In the parametric approach, a nonlinear parametric model is first identified from the input–output data. The GFRFs are then obtained by mapping the resultant model into the frequency domain using the probing method [9]. The nonparametric approach is often referred to as frequency-domain Volterra system identification and is based on the observation that the Volterra model of nonlinear systems is linear in terms of the unknown Volterra kernels, which, in the frequency domain, corresponds to a linear relation between the output frequency response and linear, quadratic, and higher order GFRFs. This linear relationship allows the use of a least squares (LS) approach to solve for the GFRFs. Several researchers [10–12] have used this method to estimate the GFRFs. But they usually made the assumption that it is known a priori that the system under study can be represented by just two or three terms. However, such information is rarely available a priori.", "ground_truth": ["a parametric-model-based method", "the gfrfs of nonlinear systems", "a nonparametric-model-based method", "a nonlinear parametric model", "gfrfs", "mapping the resultant model into the frequency domain", "probing method", "nonparametric approach", "frequency-domain volterra system identification", "volterra model of nonlinear systems", "corresponds to a linear relation", "linear relationship", "a least squares (ls) approach", "gfrfs", "gfrfs", "made the assumption", "the system", "two or three terms"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "gfrfs", "nonlinear systems", "parametric-model-based method", "nonparametric-model-based method", "input–output data", "probing method", "frequency-domain volterra system identification", "volterra model", "volterra kernels", "frequency domain", "output frequency response", "linear relationship", "least squares (ls) approach", "higher-order gfrfs", "system identification", "quadratic", "a priori assumption", "researchers", "estimation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "However this is not just a useful depiction of an apposite well-supported statistical model. If we are prepared to allow that the process is driven by a CRG and that the MAP model that we have discovered is indeed generating the idle process, then identifying the disconnected components of the system allows us to immediately make assertions about the impact of various controls we might apply to this regulatory process – just as we can were we to believe the model was a causal extension of a BN. In the context of microarrays, the objective of clustering is to identify patterns among the data and decide which genes to focus on in further, more gene-specific, experiments. It is therefore necessary for the scientist to make such causal conjectures about the effect of controls available to her on the expressions reflecting the underlying regulatory process she studies. These conjectures can be universal or nuanced by evoking ideas of parsimony.", "ground_truth": ["an apposite well-supported statistical model", "crg", "map model", "idle process", "identifying the disconnected components of the system", "the impact of various controls", "bn", "clustering", "identify patterns among the data", "the effect of controls", "evoking ideas of parsimony", "conjectures"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "statistical model", "crg (contextual regulatory graph)", "map model (maximum a posteriori)", "idle process", "disconnected components", "controls", "regulatory process", "causal extension", "bn (bayesian network)", "microarrays", "clustering", "genes", "gene-specific experiments", "causal conjectures", "expressions", "underlying regulatory process", "parsimony"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The first step of PB, the enumeration of the conditional sample space through abductive logic programming, could be compared to “logical inference” in ProbLog [9]. While both languages aim to generate a propositional formula and compile it into a decision diagram, “logical inference” in PB is based on abductive logic programming, while ProbLog grounds the relevant parts of the probabilistic program. Moreover, in PB compilation of the boolean formulas is performed using (RO)BDDs, while ProbLog can use a wider range of decision diagrams, e.g. sentential decision diagrams (SDD), deterministic, decomposable negation normal form (d-DNNF). These differences reflect the different aims of the two PPLs: ProbLog focuses on models where “logical inference” needs to be efficient, and the resulting representation, the decision diagrams, need to be compact, while PB focuses on models where “logical inference” is typically easy, however it must be applied repeatedly, according to the nature and the number of the observations. However, in future work, PB could benefit from the use of more compact decision diagrams.", "ground_truth": ["logical inference", "problog", "(ro)bdds", "decision diagrams", "sentential decision diagrams", "sdd", "deterministic", "decomposable negation normal form", "d-dnnf", "ppls", "problog", "logical inference", "pb", "pb", "enumeration of the conditional sample space", "“logical inference” in pb", "problog", "pb compilation", "problog", "“logical inference”", "pb", "use of more compact decision diagrams"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "pb", "enumeration", "conditional sample space", "abductive logic programming", "logical inference", "problog", "propositional formula", "decision diagram", "probabilistic program", "(ro)bdds", "sentential decision diagrams (sdd)", "deterministic", "decomposable negation normal form (d-dnnf)", "ppls", "efficient", "compact", "observations", "future work", "more compact decision diagrams"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the case of PSR applied to vessels, preservation of high curvature and branches (concavities) demands a high value of the d parameter, resulting in models with high number of polygons. To cope with this problem, Wu et al. (2013) evaluates a variant of PSR (in that work referred to as scale-adaptive [SA]), which includes curvature-dependent polygonization (e.g. increasing/decreasing the size of triangles according to the local curvature) (Wu et al., 2010). In Wu et al. (2013), other methods including MC (without smoothing and decimation) are evaluated with application to vessel modeling. The authors, point at SA as a suitable method for reconstruction of vessels with applications to surgery planning. The methods evaluated by Wu et al. (2013) could be also compared with another set of techniques (known as model-based methods) (Preim and Oeltze, 2008), widely used in the context of vessel modeling for surgery planning.", "ground_truth": ["mc", "psr", "vessel modeling", "sa", "surgery planning", "model-based methods", "psr applied to vessels", "scale-adaptive", "sa", "reconstruction of vessels", "vessel modeling for surgery planning", "curvature-dependent polygonization", "increasing/decreasing the size of triangles according to the local curvature"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "psr (poisson surface reconstruction)", "vessels", "high curvature", "branches", "concavities", "d parameter", "polygons", "wu et al. (2013)", "scale-adaptive (sa)", "curvature-dependent polygonization", "triangles", "local curvature", "mc (marching cubes)", "smoothing", "decimation", "vessel modeling", "surgery planning", "model-based methods", "preim and oeltze (2008)", "reconstruction", "techniques"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The need for power generation industry to improve the thermal efficiency of power plant has led to the development of 9–12% Cr martensitic steels. The development of and research on P91 steels started since late 1970s and early 1990s, respectively [1]. The work has focussed on their creep strengths due to its intended application at high temperature. Recently, the introduction of more cyclic operation of power plant has introduced the possibility of fatigue problems. Bore cracking due to the effects of varying steam warming has been reported [2]. The temperature cycling causes thermal gradients between the inside and outside of components and this can cause cyclic stress levels to be of concerns. Recently, research on thermal–mechanical analysis of P91 has been carried out including the characterisation of the cyclic behaviour of the material using the two-layer and unified visco-plasticity models [3,4].", "ground_truth": ["power generation", "improve the thermal efficiency of power plant", "9–12% cr martensitic steels", "development of and research on p91 steels", "p91 steels", "power plant", "fatigue problems", "steam warming", "temperature cycling", "thermal–mechanical analysis of p91", "p91", "two-layer and unified visco-plasticity models", "characterisation of the cyclic behaviour of the material", "the material"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "power generation industry", "thermal efficiency", "power plant", "9–12% cr martensitic steels", "p91 steels", "creep strengths", "high temperature", "cyclic operation", "fatigue problems", "bore cracking", "steam warming", "thermal gradients", "cyclic stress levels", "thermal–mechanical analysis", "cyclic behaviour", "two"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In previous publications the present authors proposed a method to incorporate the thermodynamics of ternary alloys and liquid diffusion-governed solidification kinetics into a multiphase volume average solidification model [23,24]. Back diffusion was disregarded. A way to access the thermodynamic data (e.g. Thermo-Calc [1]) through a tabulation and interpolation program ISAT (In Situ Adaptive Tabulation) was suggested. With the ISAT approach it is possible to perform an online call of the thermodynamic data and trace the formation of each individual solid phase (primary, peritectic, eutectic, etc.). As the number of calls of the thermodynamic data is equal to the product of the number of the discretized volume elements, the time steps and the calculation iterations per time step, the calculation becomes exhausting. Therefore, the current model is a modification of the previous model using a linearized phase diagram, and no online call of thermodynamic data is necessary. In addition, the model presented in this paper is extended to consider the back diffusion into the solid. With these modifications, the model can be used to perform casting process simulations with incorporated full diffusion-governed solidification kinetics for ternary alloys at a reasonable computation cost.", "ground_truth": ["incorporate the thermodynamics of ternary alloys and liquid diffusion-governed solidification kinetics into a multiphase volume average solidification model", "ternary alloys", "liquid diffusion-governed solidification kinetics", "multiphase volume average solidification model", "back diffusion", "thermodynamic data", "tabulation and interpolation program", "isat", "in situ adaptive tabulation", "isat approach", "isat", "thermodynamic data", "thermodynamic data", "discretized volume elements", "modification of the previous model using a linearized phase diagram", "current model", "online call of thermodynamic data", "thermodynamic data", "extended to consider the back diffusion into the solid", "back diffusion", "solid", "modifications", "perform casting process simulations with incorporated full diffusion-governed solidification kinetics", "casting process simulations", "full diffusion-governed solidification kinetics", "ternary alloys"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "previous publications", "thermodynamics", "ternary alloys", "liquid diffusion", "solidification kinetics", "multiphase volume average", "solidification model", "back diffusion", "thermodynamic data", "thermo-calc", "tabulation", "interpolation program", "isat (in situ adaptive tabulation)", "online call", "solid phase formation", "primary phase", "peritectic phase", "eutectic phase", "discretized volume elements", "time steps", "calculation iterations", "linearized phase diagram", "casting process simulations", "diffusion-govern"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Due to the complex nature of the thermal spray process, modelling has been playing a key role in providing some key insights for process design and operations. The relationships among processing conditions, particle characteristics, and the resulting coating properties are nonlinear and might be difficult to be unravelled by the experimental studies alone (e.g. [5–7]) Detailed information on the atomic level changes leading to changes observed at macroscale can appropriately be obtained by MD simulation and the effect of temperature and velocity can be determined more precisely. In this work, relatively simpler spray system of copper–copper particle was simulated to obtain a better understanding of particle recrystallization and solidification, and deformation mechanics and topography of the impacting particles. Using state-of-the-art methods to examine the physical mechanisms involved in the impacting behavior and structure–property relationship, it can be suggested that the consecutive layer deposition of particles can better be understood by understanding individual particle impacts. The particle–surface interaction mechanism and its relation to Reynolds number can offer information on the quality of the coating through its response to shock heating. As a general practice, engineering components are thermally sprayed in a continuous multilayer mode with cooling; therefore there is an opportunity for developing richer theoretical models for single or multiple particle impact in conjunction with actual spraying tests, so as to identify cohesive and adhesive strength, hardness and residual stresses.", "ground_truth": ["thermal spray process", "thermal spray", "modelling", "providing some key insights for process design and operations", "coating", "processing", "experimental studies", "atomic level changes", "md simulation", "md", "spray system", "copper–copper particle", "particle recrystallization and solidification", "particle", "deformation", "particles", "consecutive layer deposition", "particles", "particle", "understanding individual particle impacts", "particle–surface interaction mechanism", "reynolds number", "particle", "surface", "coating", "shock heating", "engineering components", "thermally sprayed", "cooling", "developing richer theoretical models for single or multiple particle impact", "spraying tests", "particle"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "thermal spray process", "modelling", "process design", "operations", "processing conditions", "particle characteristics", "coating properties", "nonlinear relationships", "experimental studies", "atomic level changes", "macroscale", "md simulation (molecular dynamics simulation)", "temperature", "velocity", "copper-copper particle", "particle recrystallization", "solidification"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Structural adhesives are increasingly used for bonding components within critical load bearing engineering structures such as aerospace and automotives. Typically these adhesives are based on epoxy polymers. Epoxies are inherently brittle due to their homogeneous microstructure and highly cross linked nature. Thus, there has been much research focused on improving the fracture toughness of epoxy polymers by incorporating a second minority phase at the nano-scale. These modifiers fall into one of two main categories: inorganic additives, e.g. silica [1,2], glass [3], alumina [4], nano-clays [5] and carbon nanotubes [6,7] or organic, usually rubber particles. Rubbery additives can be either core–shell rubber particles [8–10] or can form during curing via reaction induced phase separation mechanisms [11,12]. The primary energy dissipation mechanisms for rubber toughened epoxies are known to be both plastic void growth and shear band development [13]. It has also been shown that a combination of the above additives to create a hybrid material can provide synergistic toughening effects, e.g. carbon nanotubes and silica nanoparticles [14] or rubber with silica nanoparticles [15–17].", "ground_truth": ["structural adhesives", "bonding components", "critical load bearing engineering structures", "aerospace", "automotives", "adhesives", "epoxy polymers", "epoxies", "microstructure", "fracture toughness", "improving the fracture toughness of epoxy polymers", "epoxy polymers", "incorporating a second minority phase at the nano-scale", "inorganic additives", "silica", "glass", "alumina", "nano-clays", "carbon nanotubes", "organic", "rubber particles", "rubbery additives", "core–shell rubber particles", "curing", "reaction induced phase separation mechanisms", "energy dissipation mechanisms", "rubber toughened epoxies", "plastic void growth", "shear band development", "plastic void growth", "shear band development", "additives", "hybrid material", "synergistic toughening effects", "carbon nanotubes", "silica nanoparticles", "rubber with silica nanoparticles", "carbon nanotubes and silica nanoparticles"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "structural adhesives", "bonding components", "critical load bearing", "engineering structures", "aerospace", "automotives", "epoxy polymers", "brittle", "homogeneous microstructure", "highly cross-linked", "fracture toughness", "second minority phase", "nano-scale", "inorganic additives", "silica", "glass", "alumina", "nano-clays", "carbon nanotubes", "organic additives", "rubber particles", "core–shell rubber particles", "reaction induced phase separation", "energy dissipation mechanisms", "plastic void growth"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "These results demonstrate that SW-SVR predicts complicated micrometeorological data with the best prediction performance and the lowest computational complexity compared with standard algorithms. In particular, we found that dynamic aggregation of models built from very little extracted data by D-SDC is effective for compatibility of high prediction performance and low computational complexity. However, there are problems to be solved in SW-SVR. Firstly, the prediction performance of SW-SVR sometimes deteriorates despite an increase of training data. In particular, this problem occurred under the conditions that prediction horizons are 6 h as shown in Fig. 3. This is because data extracted by D-SDC involves unnecessary training data for highly accurate prediction. If D-SDC extracts the same data as the extracted data when training periods are shorter, the prediction performance of SW-SVR never deteriorates due to an increase of training data. Therefore, we must review both feature mapping and algorithms of D-SDC so as to avoid extracting unnecessary training data. Meanwhile, SW-SVR is based on a combination of several algorithms: kernel approximation, PLS regression, k-means, D-SDC, and linear SVR. Moreover, each algorithm has several parameters. Therefore, SW-SVR has more varied parameters, and it takes more time to tune the parameters. In this experiment, we used a grid search roughly so as to decide the parameters in a certain time. However, there is still room for improvement in the prediction performance by using other approaches such as a genetic algorithm instead of a grid search (Huang & Wang, 2006).", "ground_truth": ["sw-svr", "micrometeorological data", "standard algorithms", "dynamic aggregation of models", "d-sdc", "sw-svr", "sw-svr", "training data", "d-sdc", "training data", "d-sdc", "sw-svr", "training data", "d-sdc", "sw-svr", "combination of several algorithms", "kernel approximation", "pls regression", "k-means", "d-sdc", "linear svr", "sw-svr", "tune the parameters", "grid search", "genetic algorithm", "grid search"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "sw-svr", "micrometeorological data", "prediction performance", "computational complexity", "standard algorithms", "dynamic aggregation", "d-sdc", "extracted data", "training data", "prediction horizons", "unnecessary training data", "feature mapping", "algorithms", "kernel approximation", "pls regression", "k-means", "linear svr"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "•More efforts should be directed towards advancing the methods of feature extraction to overcome the influence of dynamic factors that limit the performance. The use of advanced machine learning methods such as deep neural networks and muscles synergies extraction should also be investigated on problems under the influence of multiple dynamic factors as such methods may provide substantial improvements upon the utilized time-and-frequency EMG feature extraction methods (Diener, Janke, & Schultz, 2015; Ison, Vujaklija, Whitsell, Farina, & Artemiadis, 2016; Park & Lee, 2016). Meanwhile, we showed that the performance of the learning algorithms can be improved by using feature extraction methods that rely on the angular information of muscle activation patterns. Features such as the TD-PSD and the DFT proved more successful than others in reducing the impact of the two dynamic factors that we considered in this paper. Such features can be readily implemented into a prosthesis controller for real-time control, especially that the EMG pattern recognition systems are nowadays becoming available for clinical testing, e.g. the COAPT complete control system (Kuiken et al., 2014)11https://www.coaptengineering.com/.", "ground_truth": ["feature extraction", "advanced machine learning methods", "deep neural networks", "muscles synergies extraction", "utilized time-and-frequency emg feature extraction methods", "feature extraction methods", "td-psd", "dft", "features", "prosthesis controller", "emg pattern recognition systems", "clinical testing", "coapt complete control system", "performance", "reducing the impact of the two dynamic factors", "real-time control"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "feature extraction", "dynamic factors", "performance", "machine learning", "deep neural networks", "muscle synergies extraction", "emg (electromyography)", "time-and-frequency features", "angular information", "muscle activation patterns", "td-psd (time-domain power spectral density)", "dft (discrete fourier transform)", "prosthesis controller", "real-time control", "emg"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.", "ground_truth": ["data mining", "dimensionality reduction", "principal component analysis method", "pca", "the simplest version of pca", "linear pca", "finds the mutually-uncorrelated vectors", "the projection of the samples generates the highest variances", "a set of orthogonal vectors", "the variance of the projection of the samples is maximum", "n-dimensional vector space basis", "n^ synthetic kpis", "up to n synthetic orthogonal kpis", "the first n^"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "data mining", "dimensionality reduction", "methods", "principal component analysis", "pca", "jolliffe", "2002", "n-dimensional vector space", "linear pca", "mutually-uncorrelated vectors", "projection", "variances", "orthogonal vectors", "descending order", "variance", "synthetic kpis", "orthogonal vectors", "highest"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "EM sensors exploit the difference in magnetic properties, such as relative permeability, and electrical conductivity between samples with different microstructural phase balances. In ferromagnetic steels, the change in relative permeability has a significant effect. Previously, multi-frequency EM sensors have been shown to be able to measure austenite/ferrite fraction from 0% to 100% in model (HIPped austenitic/ferritc stainless steel powder) alloys [7,8]. The large difference in magnetic properties of ferrite (ferromagnetic) and austenite (paramagnetic) phases makes the change in signal large and hence relatively easy to measure. EM sensors have also measured the levels of decarburisation (variation in ferrite content with depth) in steel rod [9,10]. The approach adopted to relate the overall steel EM sensor signal to its microstructure has been to construct a finite element (FE) model for the microstructure (phase, region size and distribution). The EM properties of the individual phases are assigned to those regions to give the overall EM properties of the steel. Within the model the particular sensor geometry is included (e.g. two-dimensional axisymmetric for a cylindrical sample and tubular sensor [10]) and the interaction with the steel and any external circuits predicted. In this way different microstructures and sensor designs can be compared.", "ground_truth": ["em sensors", "relative permeability", "difference in magnetic properties", "electrical conductivity", "microstructural phase balances", "ferromagnetic steels", "relative permeability", "multi-frequency em sensors", "stainless steel powder", "difference in magnetic properties", "ferrite", "ferromagnetic", "austenite", "paramagnetic", "em sensors", "decarburisation", "variation in ferrite content with depth", "steel rod", "steel", "em sensor signal", "fe", "finite element", "em properties of the individual phases", "steel", "overall em properties", "particular sensor geometry", "steel", "external circuits", "tubular sensor", "cylindrical sample", "microstructures", "sensor", "em sensor"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "em sensors", "magnetic properties", "relative permeability", "electrical conductivity", "microstructural phase balances", "ferromagnetic steels", "multi-frequency em sensors", "austenite/ferrite fraction", "hipped austenitic/ferritic stainless steel powder", "ferrite (ferromagnetic)", "austenite (paramagnetic)", "decarburisation", "ferrite content", "finite element (fe) model", "microstructure", "phase distribution", "em properties", "sensor geometry", "two-dimensional axisymmetric", "cylindrical sample", "tubular"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Many applications in fluid mechanics have shown that surface suction can be used as an effective flow-control mechanism. For example, Gregory and Walker [1] discuss how the introduction of suction extends the laminar-flow region over a swept wing by reducing the thickness of the boundary layer and the magnitude of crossflow velocity. Conclusions for the swept-wing flow arose from equivalent studies of the von Kármán (rotating disk) flow (see Gregory and Walker [2], Stuart [3]) and work has since continued into this and related flows using numerical and asymptotic approaches (see Ockendon [4], Dhanak [5], Bassom and Seddougui [6], Lingwood [7], Turkyilmazoglu [8], Lingwood and Garrett [9], for example). The literature shows that increasing suction has a stabilising effect on the general class of “Bödewadt, Ekman and von Kármán” (BEK) flows which results in an increase in critical Reynolds numbers for the onset of convective and absolute instabilities, a narrowing in the range of unstable parameters and a decrease in amplification rates of the unstable convective modes. The convective instability results are interpreted in terms of a delay in the onset of spiral vortices, and the absolute instability results in terms of the onset of laminar-turbulent transition (Lingwood [7,10,11]).", "ground_truth": ["fluid mechanics", "surface suction", "flow-control mechanism", "reducing the thickness of the boundary layer and the magnitude of crossflow velocity", "magnitude of crossflow velocity", "swept wing", "swept-wing", "swept-wing flow", "numerical and asymptotic approaches", "increasing suction", "increase in critical reynolds numbers", "narrowing in the range of unstable parameters", "decrease in amplification rates", "convective instability", "absolute instability", "“bödewadt", "ekman and von kármán” (bek) flows", "reducing the thickness of the boundary layer", "spiral vortices", "laminar-turbulent transition", "rotating disk"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fluid mechanics", "surface suction", "flow-control mechanism", "laminar-flow region", "swept wing", "boundary layer", "crossflow velocity", "von kármán flow", "rotating disk", "numerical approaches", "asymptotic approaches", "bödewadt", "ekman and von kármán (bek) flows", "stabilising effect", "critical reynolds numbers", "convective instabilities", "absolute instabilities", "un"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "We have developed a systematic, quantified understanding of a specific problem: the design of mobile-friendly unique identifiers. But our results also apply to the design of other text-based services. There has been a trend toward bespoke and adaptive keyboards (e.g., Dunlop and Levine, 2012; Karrenbauer and Oulasvirta, 2014; Leiva et al., 2015; Wiseman et al., 2013). More often than not, though, input devices are a fixed constraint in the design of a service. Most users are typing on the keyboard that came with their phone. Those keyboards have advantages, limitations and quirks. The mode-switching that most touchscreen keyboards require to reach numbers and capital letters is at the root of design improvements we propose in this paper. When designing services, it is vital to be aware of the fixed constraints of a system and to then focus on the aspects of a service's design that can be controlled. Making changes to input data in this way is a cheap, quick and easy way to improve user experience.", "ground_truth": ["design of mobile-friendly unique identifiers", "design of other text-based services", "bespoke and adaptive keyboards", "input devices", "design of a service", "mode-switching", "designing services"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "systematic", "quantified understanding", "mobile-friendly", "unique identifiers", "text-based services", "bespoke keyboards", "adaptive keyboards", "input devices", "fixed constraint", "touchscreen keyboards", "mode-switching", "design improvements", "user experience", "dunlop and levine (2012)", "karrenbauer and oulasvirta (2014)", "leiva et al. (2015)", "wiseman et al. (2013)"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Probabilistic and stochastic approaches can facilitate the search for local and global optima. Evolutionary algorithms, such as genetic population (Jomier et al., 2006; Rivest-Henault et al., 2012; Ruijters et al., 2009), are considered as a strategy that is “less likely to get stuck in a local optimum” (Ruijters et al., 2009). A cost function consisting of the “sum of the Gaussian-blurred intensity values in the [DSA] at the projected model points” (Jomier et al., 2006) is optimized using a genetic algorithm optimizer. Other authors “use the Condensation form of sequential Monte Carlo sampling to estimate a cost function gradient” (Florin et al., 2005) for finding the global minimum. Besides, the Kalman filter is successfully adopted (Curwen et al., 1994; Feldmar et al., 1997; Toledo et al., 1998).", "ground_truth": ["evolutionary algorithms", "genetic population", "genetic algorithm optimizer", "kalman filter", "probabilistic and stochastic approaches", "search for local and global optima", "cost function", "condensation form of sequential monte carlo sampling", "estimate a cost function gradien"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "probabilistic approaches", "stochastic approaches", "local optima", "global optima", "evolutionary algorithms", "genetic population", "genetic algorithm optimizer", "cost function", "gaussian-blurred intensity values", "dsa (digital subtraction angiography)", "condensation", "sequential monte carlo sampling", "cost function gradient", "global minimum", "kalman filter", "citations:", "jomier et al.", "2006", "rivest-henault et al.", "2012", "ruijters et al.", "2009"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "For all volunteers the AAMM technique significantly (p < 0.01) outperformed the other two methods in all of the intervals as can be seen by comparing to the error curves shown in Fig. 8 and the figures in Table 1 in the supplementary materials. Significance was assessed using a 1-tailed Wilcoxon signed rank test since the error distributions were generally not symmetric. The estimation errors for AAMM and its non-adaptive counterpart, AAMM (no adapt.), were similar in the beginning of the application phase, but as anticipated, as the application phase went on, the AAMM technique continually improved its accuracy by incorporating more and more data into the model. On average the motion estimation of AAMM improved by 22.94% in T5 with respect to its non-adaptive counterpart. However, the method has already significantly adapted to the breathing pattern in T2, i.e. after between 3 and 7 min of imaging, where motion estimations where on average 16.87% more accurate than at the beginning of the adaptation phase. By visually inspecting the curves for AAMM in Fig. 8 it can be seen that for many volunteers (in particular volunteers A, D, E, and F) the error curves start to flatten approximately around the 7 min mark. From this it can be concluded that a longer calibration scan of around 12 min would be optimal, that is the 5 min that were used for calibration in this experiment plus 7 min worth of data added during the application phase. Note that this time could be significantly reduced if a non-cardiac-gated sequence was used.", "ground_truth": ["aamm technique", "1-tailed wilcoxon signed rank test", "aamm technique", "error curves", "calibration scan", "non-cardiac-gated sequence", "error curves", "aamm", "aamm (no adapt.)", "aamm", "motion estimation", "imaging", "motion estimations", "aamm", "calibration"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "aamm technique", "significantly outperformed", "error curves", "fig. 8", "table 1", "supplementary materials", "1-tailed wilcoxon signed rank test", "error distributions", "estimation errors", "non-adaptive counterpart (aamm no adapt.)", "application phase", "accuracy", "motion estimation", "improved by 22.94%", "t5", "breathing pattern", "t2", "3 to 7 min of imaging", "16.87% more accurate", "adaptation phase", "volunteers"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "As a particular case of survey data, we used the iUTAH “Utah Water Survey,” which was implemented by participating researchers from several Utah institutions of higher education. The objectives of the survey were to document how a representative cross-section of Utah's adult population thinks about water issues. The survey included three core blocks of questions: perceptions of the adequacy of local water supplies, perceptions of the quality of local water resources, and concern about a range of water and non-water issues. A number of additional questions captured information about respondents' familiarity with water cost, lawn-watering behaviors, participation in water based recreation, and demographic attributes. Supplementary material to this paper includes a document with a description of the dataset as a whole, a document containing the complete survey instrument, and two data files containing the results and an associated codebook (see Section 4.3).", "ground_truth": ["iutah “utah water survey", "”", "utah's adult population thinks about water issues", "associated codebook", "survey", "perceptions of the adequacy of local water supplies", "perceptions of the quality of local water resources", "concern about a range of water and non-water issues", "familiarity with water cost", "awn-watering behaviors", "participation in water based recreation", "demographic attributes", "description of the dataset", "supplementary material", "complete survey instrument"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "survey data", "iutah", "utah water survey", "utah institutions", "higher education", "adult population", "water issues", "perceptions", "adequacy", "local water supplies", "water quality", "water resources", "concern", "water cost", "lawn-watering behaviors", "water-based recreation", "demographic attributes", "supplementary material", "dataset", "survey instrument", "data files", "results", "codebook", "section 4.3"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Isogeometric analysis (IGA) is a numerical simulation method which is directly based on the NURBS-based representation of CAD models. It exploits the tensor-product structure of 2- or 3-dimensional NURBS objects to parameterize the physical domain. Hence the physical domain is parameterized with respect to a rectangle or to a cube. Consequently, singularly parameterized NURBS surfaces and NURBS volumes are needed in order to represent non-quadrangular or non-hexahedral domains without splitting, thereby producing a very compact and convenient representation.The Galerkin projection introduces finite-dimensional spaces of test functions in the weak formulation of partial differential equations. In particular, the test functions used in isogeometric analysis are obtained by composing the inverse of the domain parameterization with the NURBS basis functions. In the case of singular parameterizations, however, some of the resulting test functions do not necessarily fulfill the required regularity properties. Consequently, numerical methods for the solution of partial differential equations cannot be applied properly.We discuss the regularity properties of the test functions. For one- and two-dimensional domains we consider several important classes of singularities of NURBS parameterizations. For specific cases we derive additional conditions which guarantee the regularity of the test functions. In addition we present a modification scheme for the discretized function space in case of insufficient regularity. It is also shown how these results can be applied for computational domains in higher dimensions that can be parameterized via sweeping.", "ground_truth": ["isogeometric analysis", "iga", "numerical simulation method", "nurbs-based representation of cad models", "physical domain is parameterized", "splitting", "sweeping", "galerkin projection", "formulation of partial differential equations", "isogeometric analysis", "numerical methods", "discuss the regularity properties of the test functions", "consider several important classes of singularities of nurbs parameterizations", "derive additional conditions which guarantee the regularity of the test functions", "present a modification scheme", "discretized function space", "parameterized", "cad models", "nurbs-based representation", "nurbs objects", "nurbs", "nurbs surfaces", "nurbs volumes", "nurbs", "nurbs basis functions", "nurbs", "modification scheme"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "isogeometric analysis (iga)", "numerical simulation method", "nurbs-based representation", "cad models", "tensor-product structure", "2-dimensional nurbs", "3-dimensional nurbs", "physical domain parameterization", "rectangle", "cube", "singularly parameterized nurbs surfaces", "nurbs volumes", "non-quadrangular domains", "non-hexahedral domains", "compact representation", "galerkin projection", "finite-dimensional spaces", "test functions", "weak formulation", "partial differential equations"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The above discussion also lays bare the difference of perspectives between the fusion of hard constraints and knowledge-base merging: the idea of Konieczny and Pino-Perez is to explain the fusion of plain epistemic states, understood as a set of plausible worlds, by the existence of underlying partial orderings or numerical plausibility degrees (obtained by distances), based on axioms that only use plausible sets attached to these orderings. In [67] the same authors use both hard (integrity) constraints and belief sets referring to plausible worlds, and try to extend both the AGM revision and knowledge-based merging. However, they do not envisage the merging of integrity constraints discussed in the previous section. The belief revision and merging literature takes an external point of view on cognitive processes under study. The underlying ordered structures are here a consequence of the merging postulates, but they do not appear explicitly in the axioms and they are not observable from the outside. On the contrary, our approach is to construct fusion rules that only rely on what is explicitly supplied by sources. In the sequel we consider the counterpart of our fusion postulates for ranked models, that can be expressed by means of total orders of possible worlds or by their encodings on a plausibility scale.", "ground_truth": ["knowledge-base merging", "hard constraints", "fusion of plain epistemic states", "partial orderings", "numerical plausibility degrees", "hard (integrity) constraints", "belief sets", "agm revision", "knowledge-based merging", "merging of integrity constraints", "belief revision and merging", "ordered structures", "construct fusion rules", "ranked models"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fusion", "hard constraints", "knowledge-base merging", "konieczny and pino-perez", "epistemic states", "plausible worlds", "partial orderings", "numerical plausibility degrees", "distances", "axioms", "plausible sets", "integrity constraints", "belief sets", "agm revision", "belief revision", "merging literature", "cognitive processes", "ordered structures", "merging postulates", "fusion rules", "sources", "ranked models", "total orders", "possible worlds", "plausibility scale"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Methods for anomaly detection in a local context are the conceptual opposite to the afore-described centralized methods, which rely on globally shared models. In data mining, the notion of locality is often given as distance between data values (given a specific distance metric such as Euclidean distance). A data point is compared to the value of its nearest neighbors in terms of data distance [42]. However, the notion of locality can also be given in a geographical distance between the sources of the data. Many similar values (i.e., data with small distance among each other) result in a higher density, called clusters, while values that are less similar result in a lower density. Anomalies can fall outside of any cluster but, when frequently occurring, can form a cluster too. Determining if a datum is normal or anomalous compared to local neighborhood data is a challenge.", "ground_truth": ["anomaly detection", "afore-described centralized methods", "globally shared models", "data mining", "distance between data values", "notion of locality", "a specific distance metric such as euclidean distance", "compared to the value of its nearest neighbors", "the notion of locality", "a geographical distance", "many similar values", "clusters", "values that are less similar result in a lower density", "cluster", "anomalies", "can form a cluster", "determining if a datum is normal or anomalous"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "anomaly detection", "local context", "centralized methods", "globally shared models", "data mining", "locality", "distance", "data values", "distance metric", "euclidean distance", "data point", "nearest neighbors", "data distance", "geographical distance", "sources of data", "similar values", "density", "clusters", "anomalies", "normal", "anomalous", "local neighborhood data", "challenge"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "MWSN routing protocols generally take influence from both WSN and mobile ad hoc network (MANET) routing protocols, which all share common limitations, such as bandwidth, power and cost. WSNs often share the same aim as MWSNs, in that they wish to route data from many sensors to a single sink. However, WSNs are normally considered to be static and so the associated routing protocols are often unable to cope in a mobile scenario [10]. Alternatively, MANET protocols are designed to be able to cope with the mobility of nodes, however they aim to allow end-to-end communication to occur between any two nodes [2]. This extra functionality is often not required by MWSNs and so the additional overhead is unnecessary. Combined with the high packet delivery ratios and low delays that are demanded by emerging applications, the ideal routing solution for a MWSN is one that can handle the mobility of nodes and allows data to be forwarded from the sensors to the sink in a reliable and timely manner. This set of requirements make the problem of routing in a MWSN a unique challenge, which will require new specifically designed solutions. For this reason there have been many routing protocols designed for MWSNs. As such, this section will give an overview of the current literature, which highlights the different techniques and commonly used protocols in MWSN routing.", "ground_truth": ["mwsn routing protocols", "wsn", "mobile ad hoc network", "manet", "routing protocols", "wsns", "mwsns", "sensors", "route data", "wsns", "manet protocols", "mwsns", "mwsn", "mwsns", "mwsn", "routing in a mwsn", "mwsn routing"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "mwsn routing protocols", "wsn (wireless sensor network)", "manet (mobile ad hoc network)", "bandwidth", "power", "cost", "sensors", "sink", "static networks", "mobile scenario", "node mobility", "end-to-end communication", "overhead", "packet delivery ratios", "low delays", "emerging applications", "r"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In order to test whether haptic patterns can convey or enhance the mood music of a movie, an affective movie clip corpus was required consisting of clips labeled according to the emotion conveyed in the mood music. The following database collections were examined as possible sources for the corpus: the Emotional Movie Database (EMDB) [19], and Film Stim [20]. However, these were discarded after review as unsuitable. The aim of this study is to enhance the mood in the film score, and in the case of the clips in the EMDB, no audio is provided which deemed the clips unsuitable. In the case of the Film Stim database, the clips are in French rather than English, and with no subtitles which where also deemed unsuitable since the studies are carried out with English speaking participants. Furthermore, the Film Stim selection is based on the affective content of the narrative as in most of them there is no music which is also unsuitable as discussed. From our review of available database collections, it was found that at present there is no standard corpus of affective movie clips where the affective indexing referred to the musical score of the clip.", "ground_truth": ["affective movie clip corpus", "emotional movie database", "emdb", "film stim", "test whether haptic patterns can convey or enhance the mood music", "database collections", "review", "enhance the mood", "emdb", "film stim database", "english speaking participants", "film stim selection", "review of available database collections"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "haptic patterns", "mood music", "movie", "affective movie clip corpus", "emotion", "labeled", "emotional movie database (emdb)", "film stim", "database collections", "unsuitable", "study", "film score", "audio", "french", "english", "subtitles", "english speaking participants", "affective content", "narrative", "music", "standard corpus", "affective indexing", "musical score"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Aspect-oriented Programming (AOP) can well solve the cross-cutting concerns. Because of the different features of aspect, AOP requires new techniques for testing. First, this paper proposes a model to test aspect-oriented software. In order to support the testing model of the first three steps, we propose the algorithm of selecting aspect relevant test cases. Then, we develop a new tool to implement the theoretical of automating select test case. Finally, a case of the Bank Account System is studied to illustrate our testing approach.", "ground_truth": ["aspect-oriented programming", "aop", "solve the cross-cutting concerns", "model to test aspect-oriented software", "testing model", "develop a new tool to implement the theoretical of automating select test case", "bank account system", "aop", "algorithm of selecting aspect relevant test cases", "automating select test case"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "aspect-oriented programming (aop)", "cross-cutting concerns", "aspect features", "testing techniques", "testing model", "aspect-oriented software", "algorithm", "selecting aspect relevant test cases", "testing tool", "automating test case selection", "bank account system", "testing approach"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "ground_truth": ["modification made to eliminate blooming and rusting", "blooming", "rusting", "design of a varnish plant", "pipelines", "ductsand hot air receiving chambers", "redesigning the varnish plant", "heat energy", "transferred into the air chambers", "drying ofmatchets", "absorption of the moisture", "steel", "further studies", "constant supply of hot air into the air chambers", "exhaust gas", "varnish plant", "processing plant", "varnish plant", "hardening plant", "varnish plant", "air chambers"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "varnish plant", "crocodile matchet limited", "tema", "ghana", "blooming", "rusting", "moisture content", "atmosphere", "pipelines", "ducts", "hot air receiving chambers", "exhaust gas", "heat energy (6.74kw)", "redesign", "drying", "matchets", "hardening plant"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.", "ground_truth": ["digital libraries", "e-learning", "elimination of the time and space constraints", "bricks-and-mortar libraries", "acquire such knowledge and skills", "overview of current trends in digital library research", "highlights on the impact of information technology", "library", "digital library"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "digital libraries", "societal benefits", "e-learning", "digital times", "mobile times", "time constraints", "space constraints", "traditional libraries", "bricks-and-mortar libraries", "library professionals", "information professionals", "knowledge and skills", "it influenced", "service profession", "current trends", "digital library research", "digital library characteristics", "advantages"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "According to the situation that the IT students can not meet the software industry demand for qualified personnel, a “triple-driven” three-dimensional software development practical teaching system was proposed, aiming to improve the software development capabilities and innovation sense of students. This system can effectively improve students the interest of software development and the practical skills and sense of innovation, laying a solid foundation for student after graduation to rapidly integrate into the software development process, meeting the needs of software industry.", "ground_truth": ["improve the software development capabilities and innovation sense of students", "software development", "practical skills", "software development process", "“triple-driven” three-dimensional software development practical teaching system", "software industry"], "prediction_list": ["here are the extracted keywords from the paragraph:", "it students", "software industry demand", "qualified personnel", "triple-driven", "three-dimensional", "software development", "practical teaching system", "software development capabilities", "innovation sense", "students’ interest", "practical skills", "graduation integration", "software development process", "industry needs", "these keywords capture the main concepts", "objectives", "and outcomes discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "According to the shortcomings of long time and big errors about the moving plate recognition system, we present the moving plate recognition algorithm based on principal component analysis(PCA) color extraction. On the basis of the analysis of moving plate recognition system's basic principles, it introduces the basic principles and calculation steps about PCA extraction algorithm, and discusses the feasibility of applying the algorithm to PRS in the paper. The experimental results show that the algorithm has the advantages of faster speed and higher accuracy of recognition. The algorithm provides a new thought for the research on the moving plate recognition algorithm.", "ground_truth": ["moving plate recognition system", "moving plate recognition algorithm", "principal component analysis", "pca", "color extraction.", "pca extraction algorithm", "recognition", "research on the moving plate recognition algorithm", "analysis of moving plate recognition system's basic principles", "moving plate recognition", "prs", "moving plate recognition", "applying the algorithm to prs"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "moving plate recognition system", "principal component analysis (pca)", "color extraction", "algorithm", "basic principles", "calculation steps", "feasibility", "prs (plate recognition system)", "experimental results", "faster speed", "higher accuracy", "new thought", "research", "these keywords capture the main concepts", "methods", "and findings discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "ground_truth": ["knowledge", "teaching", "learning", "implementation principle", "curriculum design", "education policy", "improving the quality of teaching and learning on sport normal students", "future research", "factors on promoting the level of teaching and learning"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "development", "sport normal students", "china", "teaching and learning", "knowledge process", "outdated", "ineffective", "improving quality", "factors", "promoting level", "implementation principle", "curriculum design", "education policy", "results", "implications", "future research", "let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A process-driven model is presented to build an instinctive and efficient higher educational administrative management system to overcome problems most universities facing. With this model, processes are identified explicitly and the routine of educational administration is broken into small tasks. Each task has designated role of executors. A process describes the activities and relationships among them. A prototype of higher educational administrative system is built with Bonita open solution. The demo shows that the process-driven higher educational administrative system helps end users understand processes they are involved and focus on what to do.", "ground_truth": ["process-driven model", "administrative management system", "overcome problems", "processes are identified", "educational administration", "bonita open solution", "administrative system", "understand processes they are involved"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "process-driven model", "higher educational administrative management system", "universities", "processes identified explicitly", "educational administration", "small tasks", "designated role of executors", "activities and relationships", "prototype", "bonita open solution", "process-driven higher educational administrative system", "end users", "understand processes", "focus on tasks", "these keywords capture the main concepts", "methodologies", "and outcomes discussed in the"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "With employment of utilizing the investigation, expert interviews and comparison, this article investigates the curricula construction, curricula design and curricula content for sports free normal students. On the basis of the investigation, this article analyzed the theoretical framework of curricular construction and proposed some suggestions. We hope that it can provide some evidences for curricula design for sports free normal students.", "ground_truth": ["investigation", "expert interviews", "investigates the curricula construction", "curricula design and curricula content", "theoretical framework", "curricula design", "investigation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "employment", "investigation", "expert interviews", "comparison", "curricula construction", "curricula design", "curricula content", "sports free normal students", "theoretical framework", "suggestions", "evidences", "these keywords capture the main concepts and focus of the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "It has been more than a century since the emergence of the lettered words. After that, with the development of economy and culture, the increase of international contacts and communication between China and foreign countries, lettered words have been appearing more frequently. Lettered words have become an indispensable part of Chinese vocabulary, such as WTO, Ka la OK and MP3. As a new phenomenon in the vocabulary system of the modern Chinese, the lettered words draws a lot of academic attention. Ecolinguistics is a new branch of linguistic, which combine the linguistic with the ecology. This paper is trying to analyze the lettered words from the perspective of Ecolinguistics. This paper will discuss the reasons of appearing the lettered words and the influence may give to modern Chinese form the ecolinguistic view.", "ground_truth": ["lettered words", "development of economy and culture", "communication", "lettered words", "lettered words", "chinese vocabulary", "wto", "ka la ok", "mp3", "vocabulary system", "lettered words", "ecolinguistics", "discuss the reasons of appearing the lettered words", "branch of linguistic", "combine the linguistic with the ecology", "analyze the lettered words from the perspective of ecolinguistics"], "prediction_list": ["here are the keywords extracted from the given scientific paragraph:", "lettered words", "emergence", "economy", "culture", "international contacts", "communication", "chinese vocabulary", "wto", "ka la ok", "mp3", "vocabulary system", "modern chinese", "academic attention", "ecolinguistics", "linguistics", "ecology", "analysis", "reasons", "influence"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The 21st century in the face of an aging population trend, the health status of the elderly is a hot issue of social concern, therefore, to explore the health status of the Chinese population aging and the elderly, elderly fitness exercise Misunderstanding study and formulate measures and methods of fitness of the elderly, promoting elderly fitness training towards a healthy, scientific direction, to promote a nationwide fitness activities carried out in order to achieve the exercise of scientific fitness of older persons.", "ground_truth": ["aging population trend", "explore the health status", "formulate measures and methods of fitness", "elderly fitness training", "promote a nationwide fitness activities", "exercise of scientific fitness"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "21st century", "aging population", "health status", "elderly", "social concern", "chinese population", "fitness exercise", "misunderstanding", "measures", "methods", "elderly fitness training", "healthy direction", "scientific direction", "nationwide fitness activities", "scientific fitness", "older persons", "these keywords capture the main themes of the paragraph", "focusing on"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we’ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "ground_truth": ["autonomic learning", "low emotional involvement", "web-based instructional environment", "construct model", "fusing cognition with emotion", "online teaching reformation", "propose a construct model", "construct model"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "autonomic learning", "learners", "emotional involvement", "web-based instructional environment", "construct model", "inter-subjectivity", "cognition", "emotion", "online teaching reformation", "quality course", "apparel production and management", "these keywords capture the main concepts and focus areas of the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, we present algorithms for automatic generation of logic reasoning questions. The algorithms are able to construct questions that are solvable with unique solutions. The algorithms employ AI techniques such as semantic networks to produce verbal questions. These algorithms are small in size and are able to replace traditional question databases. They are particularly suitable for implementation on the memory constrained mobile platforms. The algorithms can be applied to question generation for job interview, civil service exam, etc.", "ground_truth": ["generation of logic reasoning questions", "construct questions that are solvable with unique solutions", "ai techniques", "semantic networks", "traditional question databases", "memory constrained mobile platforms", "question generation for job interview", "algorithms for automatic generation of logic reasoning questions.", "verbal questions"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "algorithms", "automatic generation", "logic reasoning questions", "unique solutions", "ai techniques", "semantic networks", "verbal questions", "question databases", "memory constrained", "mobile platforms", "job interview", "civil service exam"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "By referring to many relevant data and essays, this paper aims at discussing and analyzing the importance of hip-push applied in walking race,based on the point of the view on sports biomechanics .With redard to the existing problems,the authors have made an objective analysis on the sports-biomechanics factors that can influence the race,hoping to provide a theoretical basis for the deep development and training of walking race.", "ground_truth": ["data and essays", "discussing and analyzing the importance of hip-push applied in walking race", "hip-push", "sports biomechanics", "objective analysis on the sports-biomechanics factors", "theoretical basis", "deep development and training of walking race"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "hip-push", "walking race", "sports biomechanics", "existing problems", "objective analysis", "sports-biomechanics factors", "theoretical basis", "deep development", "training", "these keywords capture the main concepts and focus of the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Amodel are proposed for modeling data-centric Web services which are powered by relational databases and interact with users according to logical formulas specifying input constraints, control-flow constraints and state/output/action rules. The Linear Temporal First-Order Logic (LTL-FO) formulas over inputs, states, outputs and actions are used to express the properties to be verified.We have proven that automatic verification of LTL-FO properties of data-centric Web services under input-bounded constraints is decidable by reducing Web services to data-centric Web applications. Thus, we can verify Web service specifications using existing verifier designed for Web applications.", "ground_truth": ["modeling data-centric web services", "relational databases", "linear temporal first-order logic", "ltl-fo", "ltl-fo", "data-centric web services", "verify web service specifications", "interact with users", "logical formulas", "control-flow constraints", "state/output/action rules", "reducing web services to data-centric web applications", "data-centric web applications", "web applications", "data-centric web services"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "amodel", "modeling", "data-centric web services", "relational databases", "logical formulas", "input constraints", "control-flow constraints", "state/output/action rules", "linear temporal first-order logic (ltl-fo)", "inputs", "states", "outputs", "actions", "properties", "automatic verification", "input-bounded constraints", "decidable", "data-centric web applications", "web service specifications", "existing verifier", "web applications"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The most important goal of the software industry is to produce successful product. During the process of production several times the product fails due to lack of proper management. This paper is exploring the role of software engineering courses in computer engineering related branches and then reasons why software developers lack project management in proper software management trainings. Our findings reflect that in majority of computer related branches like computer science, computer engineering, information system engineering there is no place for software project management course. Our findings are based on a survey of course curriculums of computer engineering, computer science and information system engineering courses taught in Turkish universities.", "ground_truth": ["produce successful product", "production", "product fails", "management", "role of software engineering courses", "software project management course", "survey of course curriculums", "software industry", "project management", "proper software management trainings", "computer engineering", "software project management", "software management", "computer engineering", "computer science and information system engineering"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "software industry", "successful product", "production process", "product failure", "proper management", "software engineering courses", "computer engineering", "software developers", "project management", "software management trainings", "computer science", "information system engineering", "software project management course", "course curriculums", "turkish universities", "survey", "findings", "let me know if you'd"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Analyzing the significance of macroscopical dynamic monitoring of new add construction land, considering the influence of various factors, this paper selected Yinchuan Plain for a typical experimental zone, built knowledge base of remote sensing images interpretation, used multi-temporal remote sensing images, carried through interactive interpretation of change patterns of new add construction land and field validation. Interpretation results of 20m scale remote sensing image show that the minimum spot average area of new construction land change monitored by 20m scale remote sensing data is about 6 acres. The ability 20m scale remote sensing data identifies new increased construction land change further strengthens, shows in the recognition of the smallest spot area reduces and the recognition accuracy increases.", "ground_truth": ["yinchuan plain", "remote sensing images interpretation", "multi-temporal remote sensing images", "interactive interpretation of change patterns", "construction land", "interpretation", "remote sensing image", "remote sensing data", "identifies new increased construction land", "recognition", "construction land", "field validation", "construction land", "remote sensing data", "recognition of the smallest spot area", "remote sensing images", "knowledge base"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "macroscopical dynamic monitoring", "new add construction land", "influence of various factors", "yinchuan plain", "typical experimental zone", "knowledge base", "remote sensing images interpretation", "multi-temporal remote sensing images", "interactive interpretation", "change patterns", "field validation", "20m scale remote sensing image", "minimum spot average area", "new construction land change", "recognition ability"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Evolutionary Algorithms are the stochastic optimization methods, simulating the behavior of natural evolution. These algorithms are basically population based search procedures efficiently dealing with complex search spaces having robust and powerful search mechanism. EAs are highly applicable in multiobjective optimization problem which are having conflicting objectives. This paper reviews the work carried out for diversity and convergence issues in EMO.", "ground_truth": ["evolutionary algorithms", "stochastic optimization methods", "natural evolution", "population based search procedures", "search mechanism", "optimization problem", "reviews the work carried out for diversity and convergence issues in emo", "eas", "emo"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "evolutionary algorithms", "stochastic optimization methods", "natural evolution", "population-based search procedures", "complex search spaces", "robust search mechanism", "powerful search mechanism", "multiobjective optimization problem", "conflicting objectives", "diversity", "convergence issues", "emo (evolutionary multiobjective optimization)"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, coordination problem of agricultural products supply chain with stochastic yield is studied based on prices compensation strategy. The agricultural producing is influenced by the natural conditions, and the yield is uncertain. While agricultural products is rigid demand goods, the fluctuations of yield cause greater volatility of prices. The two- echelon supply chain with one supplier and one retailor is studied, and the mathematical model is constructed. The model showed that prices compensation strategy is Pareto improvement for agricultural products supply chain with stochastic yield, and it also incentive agricultural products supplier to rise the production plan and balance the profit allocation of supply chain.", "ground_truth": ["agricultural products", "stochastic yield", "prices compensation strategy", "yield", "agricultural producing", "agricultural products", "fluctuations of yield", "two- echelon supply chain", "mathematical model is constructed", "prices compensation strategy", "agricultural products supply chain", "stochastic yield", "agricultural products", "production plan", "profit allocation", "coordination problem", "coordination problem of agricultural products supply chain", "mathematical model", "supply chain", "agricultural products"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "coordination problem", "agricultural products supply chain", "stochastic yield", "prices compensation strategy", "agricultural producing", "natural conditions", "uncertain yield", "rigid demand goods", "yield fluctuations", "price volatility", "two-echelon supply chain", "supplier", "retailer", "mathematical model", "pareto improvement", "production plan", "profit allocation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "It is difficult in directly predicting permeability from porosity in tight sandstones due to the poor relationship between core derived porosity and permeability that caused by the extreme heterogeneity. The classical SDR (Schlumberger Doll Research) and Timur-Coates models are all unusable because not enough core samples were drilled for lab NMR experimental measurements to calibrate the involved model parameters. Based on the classification scale method (CSM), after the target tight sandstones are classified into two types, the relationship between core porosity and permeability is established for every type of formations, and the corresponding permeability estimation models are established. Field examples show that the classification scale method is effective in estimating tight sandstone permeability.", "ground_truth": ["sandstones", "porosity", "permeability", "sdr", "schlumberger doll research", "timur-coates models", "nmr", "calibrate", "csm", "classification scale method", "sandstones", "porosity", "permeability", "permeability estimation models", "field examples", "classification scale method", "sandstone", "permeability", "permeability", "porosity", "experimental measurements", "calibrate the involved model parameters"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "permeability", "porosity", "tight sandstones", "core derived porosity", "heterogeneity", "sdr (schlumberger doll research) model", "timur-coates model", "core samples", "lab nmr experimental measurements", "model parameters", "classification scale method (csm)", "formation types", "permeability estimation models", "field examples"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The opportunity offered by digital technologies to make deep rationalization in purchase of supplies is becoming indispensable in competition between enterprises, considering positive effects in reducing the costs of the companies that have adopted the E-Procurement. As it has been confirmed by numerous case studies, automation of procedures for the purchase through e-procurement technology enables companies to achieve a reduction in costs (average 8-12%) of total purchases. So web-based models are playing a critical role within companies, especially in the generation of value of supply chain. This article focuses on the role of e-procurement within a supply chain showing, through simulations, the advantages and difficulties of implementing a systematic use of the Internet and defining the basic structure of an e-supply chain.", "ground_truth": ["rationalization", "supplies", "reducing the costs", "case studies", "automation of procedures", "web-based models", "generation of value of supply chain", "e-procurement", "role of e-procurement", "supply chain", "simulations", "e-supply chain", "e-procurement technology"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "digital technologies", "deep rationalization", "purchase of supplies", "competition between enterprises", "reducing costs", "e-procurement", "case studies", "automation of procedures", "e-procurement technology", "cost reduction", "total purchases", "web-based models", "generation of value", "supply chain", "role of e-procurement", "simulations"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the present paper, a hypergraph model for the structural system modeling and reconfigurability analysis has been presented. At first, we represent each system equation by a hyperedge, and then we extend the modeling hypergraph with others colored hyperedges (red and blue) which allows us to perform the analysis task. Based on the bottom up analysis hypergraph model, it's very easy to check the system reconfigurability in the presence of fault by verifying the existence of paths from the affected hyperedge to specifics blue hyperedges passing through specifics red hyperedges. The method is illustrated through a pedagogical example.", "ground_truth": ["hypergraph model", "structural system modeling", "reconfigurability analysis", "modeling hypergraph", "analysis task", "bottom up analysis hypergraph model", "system reconfigurability", "verifying the existence of paths from the affected hyperedge", "pedagogical example"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "hypergraph model", "structural system modeling", "reconfigurability analysis", "system equation", "hyperedge", "colored hyperedges", "red hyperedges", "blue hyperedges", "bottom-up analysis", "system reconfigurability", "fault", "existence of paths", "affected hyperedge", "pedagogical example", "these keywords capture the main concepts and methods discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This paper suggests a design of high quality real-time rotation face detection architecture for gesture recognition of smart TV. For high performance rotated face detection, the multiple-MCT(Modified Census Transform) architecture, which is robust against lighting change, was used. The Adaboost learning algorithm was used for creating optimized learning data. The proposed hardware structure was composed of Color Space Converter, Image Resizer, Noise Filter, Memory Controller Interface, Image Rotator, Image Scaler, MCT Generator, Candidate Detector, Confidence Switch, Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processer. As a result, suggested face detection device can conduct real-time processing at speed of at least 30 frames per second.", "ground_truth": ["design of high quality real-time rotation face detection architecture for gesture recognition of smart tv", "face detection", "mct", "modified census transform", "adaboost learning algorithm", "creating optimized learning data", "color space converter", "image resizer", "", "noise filter", "memory controller interface", "image rotator", "image scaler", "mct generator", "candidate detector", "", "confidence switch", "confidence mappe", "position resizer", "", "data grouper", "overlay processor", "color overlay processer", "face detection", "real-time processing", "hardware structure", "rotation face detection architecture", "gesture recognition"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "high quality", "real-time", "rotation face detection", "architecture", "gesture recognition", "smart tv", "high performance", "rotated face detection", "multiple-mct", "modified census transform", "robust against lighting change", "adaboost learning algorithm", "optimized learning data", "hardware structure", "color space converter", "image resizer", "noise"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Based on expectation-maximization algorithm, parameter estimation was proposed for data-driven nonlinear models in this work. On this basis, particle filters were used to approximately calculate integrals, deriving EM algorithm based on particle filter. And the effectiveness of using the proposed algorithm for the soft sensor of COx content in tail gas of PX oxidation side reactions was verified through simulation results.", "ground_truth": ["expectation-maximization algorithm", "data-driven nonlinear models", "parameter estimation", "particle filters", "em algorithm", "cox", "simulation", "px oxidation", "px", "particle filter", "soft sensor", "tail gas"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "expectation-maximization algorithm", "parameter estimation", "data-driven nonlinear models", "particle filters", "integrals (approximate calculation)", "em algorithm (based on particle filter)", "soft sensor", "cox content", "tail gas", "px oxidation side reactions", "simulation results", "these keywords capture the main concepts", "methods", "and applications discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, we present a project aiming at integrating immersive virtual reality technologies into a three-dimensional virtual world. We use an educational platform vAcademia as a test bed for the project, and focus on improving the learning process and, subsequently – the outcomes. We aim at increasing the immersiveness of 3D virtual world experience by applying motion tracking for controlling the avatar and two technologies for natural navigation: immersive projection and head-mounted display. In addition, we propose the major types of learning scenarios for the use of the designed systems.", "ground_truth": ["integrating immersive virtual reality technologies into a three-dimensional virtual world", "vacademia", "learning process", "increasing the immersiveness of 3d virtual world experience", "motion tracking", "controlling the avatar", "immersive projection", "major types of learning scenarios", "natural navigation", "virtual reality technologies", "educational platform", "head-mounted display"], "prediction_list": ["here are the extracted keywords from the paragraph:", "immersive virtual reality", "three-dimensional virtual world", "educational platform", "vacademia", "learning process", "learning outcomes", "immersiveness", "3d virtual world", "motion tracking", "avatar control", "natural navigation", "immersive projection", "head-mounted display", "learning scenarios", "designed systems"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In order to solve the problem that the diesel engine PT fuel system is unable to field maintain, developed a portable signal acquisition and analysis system for diesel engine PT fuel system. Firstly, the PT pump work Principle was analyzed, and the PT pump failure mapping relation between reason and failure phenomenon was analyzed; Secondly, the diesel engine PT pump failure fuel pressure characteristics were analyzed; Lastly, using the portable signal acquisition and analysis system to diagnose the diesel engine PT fuel system, experiment results show that the system can correctly detect the diesel engine PT fuel system state.", "ground_truth": ["problem that the diesel engine pt fuel system is unable to field maintain", "signal acquisition", "analysis", "diesel engine pt fuel system", "pt pump work principle was analyzed", "failure fuel pressure characteristics were analyzed", "signal acquisition", "analysis system", "diagnose the diesel engine pt fuel system", "detect the diesel engine pt fuel system state", "diesel engine pt fuel", "diesel engine pt fuel", "pt pump", "pt pump", "diesel engine pt pump", "diesel engine pt fuel", "portable signal acquisition and analysis system"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "diesel engine", "pt fuel system", "field maintain", "portable signal acquisition", "analysis system", "pt pump", "work principle", "failure mapping relation", "failure phenomenon", "fuel pressure characteristics", "diagnose", "experiment results", "system detection", "state", "these keywords capture the main technical concepts and research focus of the paragraph. let me know if you need any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The behavior of cellular beam is described using design methods according to BS: 5950, considering particularly the strength of tee sections and web post element. Such behavior is derived from parametric study involving finite element analysis using software ANSYS. The design method is based on plastic analysis of beam section at ultimate loads and elastic analysis at serviceability loads. The procedure of design of cellular beam is illustrated and an example based on design method is worked out and its verification is done for checking the suitability.", "ground_truth": ["ansys", "plastic analysis of beam section", "elastic analysis", "cellular beam", "design methods", "finite element analysis", "procedure of design of cellular beam is illustrated", "cellular beam", "design method", "design method"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "cellular beam", "design methods", "bs: 5950", "strength", "tee sections", "web post element", "parametric study", "finite element analysis", "ansys", "plastic analysis", "beam section", "ultimate loads", "elastic analysis", "serviceability loads", "design procedure", "example", "verification", "suitability", "these keywords"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The low-carbon economic development has become the trend and orientation of regional economic development. As the residents of Heilongjiang province, their consumption is the most direct way to achieve the low-carbon lifestyle. Based on the research and discussion of the connotation of low-carbon consumption and its culture, behaviour, preferences and habits, it is concluded that the low-carbon consumption requires us to abide by the life-style of knowledge and culture. Therefore, it is obvious that the development of low-carbon economy is a complex and systematic project, involved the economic development mode, technological innovation mode, consumption values and changes of lifestyle.", "ground_truth": ["low-carbon consumption", "low-carbon economic development", "regional economic development", "low-carbon consumption", "low-carbon economy", "development of low-carbon economy", "economic development", "technological innovation", "carbon", "carbon", "carbon", "carbon", "carbon"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "low-carbon economic development", "regional economic development", "heilongjiang province", "consumption", "low-carbon lifestyle", "low-carbon consumption", "culture", "behaviour", "preferences", "habits", "life-style", "knowledge", "systematic project", "economic development mode", "technological innovation mode", "consumption values", "lifestyle changes", "these keywords capture the"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The paper presents the results of studies of the effect of multiwalled carbon nanotubes 18-20nm in concentrations of 1 and 10mg / ml for diatoms Pseudo-nitzschia pungens (clone PP-07) and golden alga Isochrysis galbana (clone TISO). The toxic effects of multiwalled nanotubes on both types of algae is revealed, which results in a decrease of the linear dimensions of cells, chloroplasts, and a reduced number of cells when incubated over 24h (Pseudo-nitzschia pungens) and 36hours (Isochrysis galbana).", "ground_truth": ["studies", "carbon nanotubes", "pseudo-nitzschia pungens", "clone pp-07", "golden alga isochrysis galbana", "clone tiso", "toxic effects", "nanotubes", "algae", "pseudo-nitzschia pungens", "isochrysis galbana", "incubated", "presents the results of studies of the effect of multiwalled carbon nanotubes", "cells", "chloroplasts", "cells"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "multiwalled carbon nanotubes", "18-20nm", "concentrations", "1 mg/ml", "10 mg/ml", "diatoms", "pseudo-nitzschia pungens", "clone pp-07", "golden alga", "isochrysis galbana", "clone tiso", "toxic effects", "linear dimensions", "cells", "chloroplasts"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The retrospective assessment of environmental carrying capacity aims to obtain the historical development situation of reclamation domain, it's an essential tool for improving the managed level and guiding the environmental management of reclamation. In this paper, a synthetic assessment method based on cloud theory is applied to evaluate the single factor and multiple factors environmental carrying capacity in Caofeidian marine district, Tangshan Bay, China. With the field data of five assessment indexes in recent six years, the assessment results are obtained which show that the marine reclamation has a certain impact for the marine environment.", "ground_truth": ["retrospective assessment of environmental carrying capacity", "environmental management of reclamation", "synthetic assessment method based on cloud theory is applied", "cloud theory", "environmental carrying capacity", "field data", "marine reclamation", "obtain the historical development situation of reclamation domain", "synthetic assessment method", "improving the managed level", "guiding the environmental management of reclamation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "retrospective assessment", "environmental carrying capacity", "historical development", "reclamation domain", "managed level", "environmental management", "synthetic assessment method", "cloud theory", "single factor", "multiple factors", "caofeidian marine district", "tangshan bay", "china", "field data", "assessment indexes", "six years", "assessment results", "marine"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This study is focused on the water-gas shift reaction (WGSR), occurring in the chemical kinetics equipment, which is used to increase hydrogen recovery from industrial processes. The research deals with comparing hydrogen recovery with the use of three different catalysts. The amount of the produced hydrogen depends considerably on the reaction state and the catalyst composition. To improve the course of the reaction, natural catalysts – calcite, coal char (unburned residues from coal) and modified olivine – are added to the gasification process and heated to the process temperature of 800, 850 and 900oC.", "ground_truth": ["water-gas shift reaction", "wgsr", "chemical kinetics equipment", "hydrogen recovery", "industrial processes", "hydrogen recovery", "catalysts", "calcite", "coal char", "natural catalysts", "modified olivine", "gasification process", "heated", "water", "gas", "hydrogen", "hydrogen", "hydrogen", "reaction", "catalyst", "reaction", "unburned residues from coal"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "water-gas shift reaction (wgsr)", "chemical kinetics equipment", "hydrogen recovery", "industrial processes", "catalysts", "reaction state", "catalyst composition", "natural catalysts", "calcite", "coal char", "unburned residues", "modified olivine", "gasification process", "process temperature", "800°c", "850°c"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Several inorganic flocculating agents, including FeSO4, Al2(SO4)3, FeCl3 and an organic coagulant aid PAM, were used to treat the wastewater from domestic anima and poultry breeding in this paper. The ideal operating conditions were attained by single factor experiment and orthogonal design experiment. And the ideal operating conditions are follows: the dose of FeSO4 and PAM is 135.2mg/L and 0.384mg/L respectively when keeping the pH 10; and the corresponding removal rate is 55% and 60% for COD and turbidity. Based on the experimental results, this paper analyzes the main factors that affect wastewater flocculation treatment.", "ground_truth": ["feso4", "inorganic flocculating agents", "al2(so4)3", "", "fecl3", "organic coagulant aid", "pam", "treat the wastewater", "single factor experiment", "orthogonal design experiment", "feso4", "pam", "removal rate", "analyzes the main factors", "wastewater flocculation treatment", "cod", "wastewater flocculation"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "inorganic flocculating agents", "feso4", "al2(so4)3", "fecl3", "organic coagulant aid", "pam", "wastewater treatment", "domestic animal and poultry breeding", "single factor experiment", "orthogonal design experiment", "ideal operating conditions", "dose", "ph", "removal rate", "cod (chemical oxygen demand)", "turbidity", "wastewater flocculation treatment", "experimental results", "main factors"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Many models have been propounded for forecasting lightning. Though majority of the model had shown accuracy, the response time in detecting natural phenomenon is quite low. In this model, we used the mathematical experimentation of the micro scale plasmas to develop the macro scale atmospheric plasma which we believe is a major influence of lightning. The Schrödinger-electrostatic algorithm was propounded to further increase both the accuracy and alacrity of detecting natural phenomena. According to our theoretical experimentation, the air density plays a major role in lightning forecast. Our guess was verified using the Davis Weather Station to track the air density both in the upper and lower atmosphere. The air density in the upper atmosphere showed prospect as a vital factor for lightning forecast.", "ground_truth": ["forecasting lightning", "model", "models", "natural phenomenon", "mathematical experimentation", "micro scale plasmas", "schrödinger-electrostatic algorithm", "detecting natural phenomena.", "theoretical experimentation", "davis weather station", "macro scale atmospheric plasma", "lightning", "air", "lightning", "air", "air", "upper atmosphere", "lightning", "track the air density"], "prediction_list": ["here are the extracted keywords from the paragraph:", "models", "forecasting lightning", "accuracy", "response time", "detecting natural phenomenon", "mathematical experimentation", "micro scale plasmas", "macro scale atmospheric plasma", "influence of lightning", "schrödinger-electrostatic algorithm", "theoretical experimentation", "air density", "lightning forecast", "davis weather station", "upper atmosphere", "lower atmosphere", "vital factor"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Design semantics is an integration of human mode of existence and view on culture and art, which means it is a unity of art and science. Design semantics is the annotation of form and the reflection of its symbolic meaning, which means it is an explanation of the deposited human cultural spirit. Chinese art stresses Expression, Force and Qi. In China, people advocate “to learn from nature”, “to look up to observe the sun, the moon and stars, and look down to observe the surroundings”, and take “Nature and Man in One” as the highest state of spirit. Design semantics is expressed in space environment design through a symbiotic philosophical view that natural and artificial forms are complementary and interactive. This form of design leads humans back to a better state of living, i.e. Nature and Man in One.", "ground_truth": ["design semantics", "design semantics", "to learn from nature", "design semantics", "space environment design", "unity of art and science", "annotation of form and the reflection of its symbolic meaning", "explanation of the deposited human cultural spirit", "natural and artificial forms", "integration of human mode of existence and view on culture and art", "observe the surroundings"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "design semantics", "human mode of existence", "culture and art", "unity of art and science", "annotation of form", "symbolic meaning", "human cultural spirit", "chinese art", "expression", "force", "qi", "learn from nature", "observe surroundings", "nature and man in one", "highest state of spirit", "space environment design", "symbiotic philosophical view", "natural forms", "artificial forms", "complementary and interactive", "better state of living"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "It has been acknowledged that megalopolises are playing a leading role in the processes of both economic development and culture change. Thereupon, the new emphases on sustainability of transportation system in megalopolis are creating new demands for adequate approach to measure its performance and diagnosis potential drawbacks. By examining the descriptions of sustainable transport system as well as its evaluating approach, a framework with the general applicability and easily accessible data resource for evaluating sustainability of transport system in megalopolis is developed based on nature of regional structure and the feature transport demand in megalopolis. The proposed framework is applied in the analysis and comparison of Jing-Jin-Ji and Yangtze River Delta..", "ground_truth": ["economic development", "culture change", "sustainability of transportation system", "transport system", "transport system", "proposed framework is applied", "approach to measure its performance and diagnosis potential drawbacks", "framework"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "megalopolises", "economic development", "culture change", "sustainability", "transportation system", "performance measurement", "diagnosis", "drawbacks", "sustainable transport system", "evaluating approach", "framework", "general applicability", "data resource", "regional structure", "transport demand", "analysis", "comparison", "jing-jin-ji", "yangt"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU «STANKIN» for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU «STANKIN» for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.", "ground_truth": ["intelligent management systems", "increasing energy efficiency", "ac and dc grids", "devices composing the systems", "devices composing the systems are considered", "technical characteristics are described", "problems of creating and introducing intelligent management systems", "intelligent electric power distribution systems", "creating and introducing intelligent management systems", "ntelligent management systems", "increasing energy efficiency", "intelligent electric power distribution systems", "ac and dc grids", "devices composing the systems", "devices composing the systems are considered", "technical characteristics are described", "mstu «stankin»", "mstu «stankin»"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "intelligent management systems", "energy efficiency", "industry", "operating principles", "intelligent electric power distribution systems", "mstu «stankin»", "ac grids", "dc grids", "industrial plants", "essential devices", "technical characteristics", "experimental results", "let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Along with the expansion of computer-based climate simulations, efficient visualization and analysis of massive climate data are becoming more important than ever. In this paper, we try to explore the factors behide climate changes by combining window query and time-varying data mining techniques. With constant query time and acceptable storage cost, the algorithms presented support various queries on 3d time-varying datasets, such as average, min, and max value. A new time-varying data analysis algorithm is given, which is especially suitable for analyzing big data. All these algorithms have been implemented on and integrated into a visual analysis system, with tiled-LCD ultra-resolution display. Experimental results on several datasets from practical applications are presented.", "ground_truth": ["computer-based climate simulations", "massive climate data", "explore the factors behide climate changes", "combining window query and time-varying data mining techniques", "algorithms", "queries", "3d time-varying datasets", "time-varying data analysis algorithm", "algorithms", "implemented on and integrated into a visual analysis system", "tiled-lcd ultra-resolution display", "expansion of computer-based climate simulations", "visualization and analysis of massive climate data", "several datasets from practical applications", "analyzing big data"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "computer-based climate simulations", "efficient visualization", "massive climate data", "climate changes", "window query", "time-varying data mining techniques", "constant query time", "acceptable storage cost", "3d time-varying datasets", "average value", "min value", "max value", "time-varying data analysis algorithm", "big data", "visual analysis system", "tiled-lcd ultra-resolution display", "experimental results", "datasets", "practical applications"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel efﬁciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "ground_truth": ["improving as well as evaluating the performance", "high performance computing", "migrating them to cloud environments", "cloud environments", "poor network performance", "heterogeneous and dynamic environments", "execution of hpc applications in cloud", "cloud", "proposes a new approach to improve the performance and scalability of hpc applications", "amazon's hpc cloud", "speed up", "scale up", "response rate", "parallel efﬁciency", "cloud", "hpc cluster", "ec2 cloud system", "deploying on-demand", "small sized hpc applications", "hpc", "high performance and cloud computing", "hpc applications", "hpc applications", "hpc applications"], "prediction_list": ["here are the extracted keywords from the paragraph:", "high performance computing (hpc)", "cloud environments", "performance evaluation", "network performance", "heterogeneous environments", "dynamic environments", "hpc applications", "amazon's hpc cloud", "performance improvement", "scalability", "speed up", "scale up", "parallel efficiency", "dedicated hpc cluster", "ec2 cloud system", "on-demand deployment", "small-sized hpc"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the paper we present an extended version of the graph-based unsupervised Word Sense Disambiguation algorithm. The algorithm is based on the spreading activation scheme applied to the graphs dynamically built on the basis of the text words and a large wordnet. The algorithm, originally proposed for English and Princeton WordNet, was adapted to Polish and plWordNet. An extension based on the knowledge acquired from the corpus-derived Measure of Semantic Relatedness was proposed. The extended algorithm was evaluated against the manually disambiguated corpus. We observed improvement in the case of the disambiguation performed for shorter text contexts. In addition the algorithm application expressed improvement in document clustering task.", "ground_truth": ["extended version of the graph-based unsupervised word sense disambiguation algorithm", "word sense disambiguation algorithm", "algorithm", "spreading activation scheme", "text words", "large wordnet", "built on the basis of the text words and a large wordne", "algorithm", "english and princeton wordnet", "polish and plwordnet", "extension", "corpus-derived measure of semantic relatedness", "extended algorithm", "manually disambiguated corpus", "disambiguation", "document clustering task"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "extended version", "graph-based", "unsupervised", "word sense disambiguation", "algorithm", "spreading activation scheme", "graphs", "dynamically built", "text words", "large wordnet", "english", "princeton wordnet", "polish", "plwordnet", "extension", "knowledge", "corpus-derived", "measure of semantic relatedness", "extended algorithm", "evaluated", "manually disambiguated corpus", "improvement", "disambiguation", "shorter text contexts", "algorithm application", "document clustering task"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Sentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of natural language processing field. In this paper, we present a method that generates sentence reduction and applying in Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.", "ground_truth": ["sentence reduction", "text summarization", "natural language processing", "generates sentence reduction", "vietnamese text summarization", "bayesian network model", "bayesian network model", "best likelihood short sentence", "compare difference of probability", "980 sentences", "generating sentence reduction"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "sentence reduction", "text summarization", "researchers", "scholars", "natural language processing", "method", "vietnamese text summarization", "bayesian network model", "best likelihood", "short sentence", "probability", "experimental results", "effectively", "understandable", "readable", "grammar", "let me know if you'd like any refinements or additional details!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Knowledge Management (KM) is one of the hotspots for research in the past decade. In most cases, the number of users in a Knowledge Management System (KMS) is very large, and they are from varied departments, even other companies. In this paper, some defects when existing methods about access control and recommendation are deployed in KMS are analyzed to show that these widely-used approaches need to be extended. To overcome the deficiencies of previous work, this paper proposes an extended Role-Based Access Control (RBAC) method and a hybrid recommendation approach for Knowledge Management System. Also, a real-life system is presented to verify the proposed methodology.", "ground_truth": ["knowledge management", "km", "knowledge management system", "kms", "defects when existing methods about access control and recommendation are deployed in kms are analyzed", "extended", "role-based access contro", "rbac", "hybrid recommendation approach", "knowledge management system", "real-life system", "verify the proposed methodology", "knowledge management", "kms"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "knowledge management (km)", "hotspots", "research", "past decade", "knowledge management system (kms)", "users", "varied departments", "companies", "defects", "access control", "recommendation", "existing methods", "widely-used approaches", "extended", "deficiencies", "previous work", "role-based access control (rbac)", "hybrid recommendation approach", "real-life system", "methodology", "verify"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Some nonlinear wave equations are more difficult to investigate mathematically, as no general analytical method for their solutions exists. The Exponential Time Differencing (ETD) technique requires minimum stages to obtain the requiredaccurateness, which suggests an efficient technique relatingto computational duration thatensures remarkable stability characteristicsupon resolving nonlinear wave equations. This article solves the diagonal example of Kawahara equation via the ETD Runge-Kutta 4 technique. Implementation of this technique is proposed by short Matlab programs.", "ground_truth": ["nonlinear wave equations", "investigate mathematically", "analytical method", "exponential time differencing", "etd", "minimum stages", "computational duration", "stability characteristicsupon", "nonlinear wave equations", "resolving nonlinear wave equations", "solves the diagonal example of kawahara equation", "etd runge-kutta 4 technique", "matlab programs"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "nonlinear wave equations", "analytical method", "exponential time differencing (etd)", "computational duration", "stability characteristics", "kawahara equation", "etd runge-kutta 4", "matlab programs"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "ground_truth": ["contractor selection", "project time and cost", "quality", "project managers", "determine the project time and cost", "un-experienced managers", "learnable property", "quantify the quality obtained for a gas well drilling project", "gas well drilling project", "objectives", "(time", "cost", "quality", "contractor selection problem", "optimization problem", "nsga-ii algorithm", "sensitivity analysis", "project time and cost"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "contractor selection", "project", "decision", "project time", "project cost", "quality", "project managers", "un-experienced managers", "learnable property", "quantify quality", "gas well drilling project", "three objectives", "optimization problem", "nsga-ii algorithm", "solution", "sensitivity analysis", "algorithm parameters", "these keywords capture the main concepts and"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, a regression analysis based method is proposed to calculate the Journal Influence Score. This Influence Score is used to measure the scientific influence of scholarly journals. Journal Influence Score is calculated by using various factors in a weighted manner. The Score is then compared with the SCImago Journal Score. The results show that the error is small between the existing and proposed methods, proving that the model is a feasible and effective way of calculating scientific impact of journals.", "ground_truth": ["regression analysis", "calculate the journal influence score", "journal influence score", "influence score", "scholarly journals", "scientific influence", "journal influence score", "compared with the scimago journal score", "scimago journal score", "calculating scientific impact of journals", "measure the scientific influence of scholarly journals", "regression analysis based method", "using various factors in a weighted manner"], "prediction_list": ["here are the extracted keywords from the paragraph:", "regression analysis", "journal influence score", "scientific influence", "scholarly journals", "weighted factors", "scimago journal score", "error comparison", "existing methods", "proposed methods", "feasible model", "effective calculation", "scientific impact", "journals"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.", "ground_truth": ["tele-operated mobile robot system", "old age surveillance", "autonomous mode", "navigates in the environment", "search for unusual situation", "elderly people", "lying on the floor", "patient", "informs the user", "switches the control mode from autonomous to haptic based", "autonomous mode", "visual sensor and landmarks to monitor", "microphone", "speaker", "monitor", "communicate", "remote place", "vital sensors", "patient's condition", "robot", "robot", "tele-operated mobile robot system", "robot", "robot"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "tele-operated mobile robot", "old age surveillance", "autonomous mode", "robot navigation", "unusual situation detection", "elderly people", "patient monitoring", "haptic-based user control", "visual sensor", "landmarks", "environment monitoring", "microphone", "speaker", "monitor", "remote communication", "vital sensors", "patient condition"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Recently, a network virtualization technology has attracted considerable attention as one of new generation network technologies. In this paper, in order to permit the rapid changing for a topology of a virtual network, we propose a new virtual network construction method based on the shortest path betweenness. In our proposed method, at first, a service provider receives a user's request for the reconfiguration of the constructed virtual network. In this case, the service provider reconfigures the topology of the constructed virtual network rapidly based on shortest path betweenness. We evaluate the performance of our proposed method with simulation, and we show the effectiveness of our proposed method.", "ground_truth": ["network virtualization technology", "new generation network technologies", "rapid changing for a topology of a virtual network", "virtual network construction method", "shortest path betweenness", "proposed method", "service provider", "reconfiguration of the constructed virtual network", "constructed virtual network", "service provider", "reconfigures the topology of the constructed virtual network", "constructed virtual network", "shortest path betweenness", "simulation", "virtual network"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "network virtualization technology", "new generation network technologies", "virtual network construction method", "shortest path betweenness", "service provider", "user's request", "reconfiguration", "topology", "simulation", "performance evaluation", "effectiveness", "these keywords capture the main concepts and technical aspects discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Security issues of data hosted in a Cloud Computing provider remain hidden seen excessive marketing that led to a totally unrealistic view of cloud computing security. Although Cloud Computing has not yet reached the level of maturity expected by its customers, and that the problems of confidentiality, integrity, reliability and consistency (CIRC) are still open, the researchers in this field have already considered a future cloud strategy which aims: a better QoS, reliability and high availability, it is the Multi-Clouds, Cloud of Clouds or Interclouds.This paper will present the security limitations in the single Cloud and the usefulness of adopting rather Multi-Clouds strategy to reduce security risks, through the use of DepSky which is a virtual storage system that ensures better availability and high confidentiality of data.", "ground_truth": ["data", "cloud computing provider", "excessive marketing", "cloud computing security", "cloud computing", "confidentiality", "integrity", "reliability and consistency", "circ", "future cloud strategy", "multi-clouds", "cloud of clouds", "interclouds", "security limitations", "single cloud", "usefulness of adopting rather multi-clouds strategy", "multi-clouds strategy", "security risks", "depsky", "virtual storage system", "better availability and high confidentiality", "present the security limitations in the single cloud"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "security issues", "data hosted", "cloud computing", "provider", "marketing", "unrealistic view", "maturity", "confidentiality", "integrity", "reliability", "consistency (circ)", "researchers", "future cloud strategy", "qos (quality of service)", "high availability", "multi-clouds", "cloud of clouds", "inter"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This paper presents general results on the Java source code snippet detection problem. We propose the tool which uses graph and subgraph isomorphism detection. A number of solutions for all of these tasks have been proposed in the literature. However, although that all these solutions are really fast, they compare just the constant static trees. Our solution offers to enter an input sample dynamically with the Scripthon language while preserving an acceptable speed. We used several optimizations to achieve very low number of comparisons during the matching algorithm.", "ground_truth": ["java source code snippet detection", "graph and subgraph isomorphism detection", "compare just the constant static trees", "enter an input sample dynamically", "scripthon language", "preserving an acceptable speed", "several optimizations", "comparisons", "matching algorithm"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "java", "source code", "snippet detection", "graph", "subgraph isomorphism", "detection", "solutions", "literature", "fast", "constant static trees", "input sample", "dynamically", "scripthon language", "acceptable speed", "optimizations", "low number of comparisons", "matching algorithm"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, adaptive beamforming techniques for smart antennas based upon Least Mean Squares (LMS), Sample Matrix Inversion (SMI), Recursive Least Squares (RLS) and Conjugate Gradient Method (CGM) are discussed and analyzed. The beamforming performance is studied by varying the element spacing and the number of antenna array elements for each algorithm. These four algorithms are compared for their rate of convergence, beamforming and null steering performance (beamwidth, null depths and maximum side lobe level).", "ground_truth": ["adaptive beamforming techniques", "smart antennas", "least mean squares", "lms", "sample matrix inversion", "smi", "recursive least squares", "rls", "conjugate gradient method", "cgm", "discussed and analyzed", "beamforming performance", "varying the element spacing and the number of antenna array elements", "algorithm", "algorithms", "rate of convergence", "beamforming", "null steering performance", "beamwidth", "null depths", "maximum side lobe level"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "adaptive beamforming", "smart antennas", "least mean squares (lms)", "sample matrix inversion (smi)", "recursive least squares (rls)", "conjugate gradient method (cgm)", "beamforming performance", "element spacing", "number of antenna array elements", "rate of convergence", "null steering performance", "beamwidth", "null depths", "maximum side lobe level"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, three different approaches for implementing a quantum search algorithm by adiabatic evolution are shown. As expected, either one of them can provide a quadratic speed up as opposed to the classical search algorithm. This implies that adiabatic evolution based quantum computation gives more feasibilities than the quantum circuit model, although the equivalence between them has already been proven in the corresponding literature.", "ground_truth": ["implementing a quantum search algorithm by adiabatic evolution", "three different approaches", "quadratic speed up", "classical search algorithm", "adiabatic evolution", "quantum computation", "feasibilities", "quantum circuit model", "quantum search algorithm", "adiabatic evolution"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "quantum search algorithm", "adiabatic evolution", "quadratic speed up", "classical search algorithm", "quantum computation", "quantum circuit model", "equivalence"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Video-oculography (VOG) is one of eye movement measurement methods. A key problem of VOG is to accurately estimate the pupil center. Then a pupil location method based on morphology and Canny algorithm was proposed for a WIFI-based VOG system which was developed our latest work. Moreover, a healthy volunteer was introduced to do sinusoidal tracking test to evaluate the pupil location method. Experimental results showed that the method could well trace eye movement and meet the anticipated results with stimulation.", "ground_truth": ["video-oculography", "vog", "eye movement measurement methods", "vog", "accurately estimate the pupil center", "pupil location method", "morphology", "canny algorithm", "wifi-based vog system", "healthy volunteer", "sinusoidal tracking test", "pupil location method", "trace eye movement", "stimulation", "eye movement measurement", "vog", "evaluate the pupil location method"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "video-oculography (vog)", "eye movement measurement", "pupil center estimation", "morphology", "canny algorithm", "wifi-based vog system", "sinusoidal tracking test", "healthy volunteer", "experimental results", "eye movement tracing", "stimulation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The Hamiltonian approach and the variational approach are utilized to treat the relativistic harmonic oscillator for the amplitude-frequency relationship. The nice reliability is shown by the result comparison with that from open literature. The simplicity and efficiency of the methods are also disclosed for different range of the initial amplitude during looking for the amplitude-frequency relationship for the nonlinear relativistic harmonic oscillator.", "ground_truth": ["hamiltonian approach", "variational approach", "treat the relativistic harmonic oscillator", "amplitude-frequency relationship", "result comparison", "open literature", "initial amplitude", "amplitude-frequency relationship", "nonlinear relativistic harmonic oscillator", "relativistic harmonic oscillator"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "hamiltonian approach", "variational approach", "relativistic harmonic oscillator", "amplitude-frequency relationship", "reliability", "result comparison", "open literature", "simplicity", "efficiency", "initial amplitude", "nonlinear relativistic harmonic oscillator", "these keywords capture the main concepts and focus of the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This paper make the explained variables our financial stress index consist of the synchronous variables financial systemic risk, and make the explanatory variables the macroeconomic variable, currency credit variable, asset price variable and the macroeconomic variable of correlative economic powers, then use stepwise regression method to establish the financial systemic risk best predict equation, thus set up the reasonable and practical financial systemic risk early-warning index system; besides, use the best prediction equations predicts the financial systemic risk status in 2011. The predicted results show that Chinese financial systemic risk is on the rise in the first three quarters and higher than the peak of 2008; financial systemic risk start to decline since the fourth quarter.", "ground_truth": ["financial stress index", "synchronous variables financial systemic risk", "macroeconomic variable", "currency credit variable", "asset price variable", "macroeconomic variable of correlative economic powers", "establish the financial systemic risk best predict equation", "stepwise regression method", "financial systemic risk best predict equation", "financial systemic risk early-warning index system", "best prediction equations", "set up the reasonable and practical financial systemic risk early-warning index system"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "explained variables", "financial stress index", "synchronous variables", "financial systemic risk", "explanatory variables", "macroeconomic variable", "currency credit variable", "asset price variable", "correlative economic powers", "stepwise regression method", "best predict equation", "early-warning index system", "prediction equations", "financial systemic risk status", "2011", "predicted results"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In Obstacle detection is based on inverse perspective mapping and homography. Obstacle classification is based on fuzzy neural network. The estimation of the vanishing point relies on feature extraction strategy. The method exploits the geometrical relations between the elements in the scene so that obstacle can be detected. The estimated homography of the road plane between successive images is used for image alignment. A new fuzzy decision fusion method with fuzzy attribution for obstacle detection and classification application is described The fuzzy decision function modifies parameters with auto-adapted algorithm to get better classification probability It is shown that the method can achieve better classification result.", "ground_truth": ["obstacle detection", "inverse perspective mapping and homography", "obstacle classification", "fuzzy neural network", "estimation of the vanishing point", "feature extraction strategy", "geometrical relations", "elements", "obstacle", "scene", "detected", "estimated homography", "road plane", "images", "image alignment", "fuzzy decision fusion method", "fuzzy attribution for obstacle detection and classification application", "the fuzzy decision function", "auto-adapted algorithm", "classification", "obstacle detection and classification application", "classification"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "obstacle detection", "inverse perspective mapping", "homography", "obstacle classification", "fuzzy neural network", "vanishing point", "feature extraction", "geometrical relations", "road plane", "successive images", "image alignment", "fuzzy decision fusion", "fuzzy attribution", "auto-adapted algorithm", "classification probability", "classification result"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The load of beam pumping unit is changeable, which is often in a state of light load. Reducing a certain voltage can improve the power factor and efficiency of the beam pumping unit when in light load .We can change the voltage by changing the thyristor trigger angle. It is complex and unacceptable to analyze the change of the cycles of the load overall. So we can divide the load of the whole cycle into several equal parts, each can be thought of as a constant load. The most optimal voltage for the current load can be calculated by genetic algorithm. When each load is in the most optimal voltage, we can get the whole optimal voltage changeable rule. Then it produces the result of energy saving.", "ground_truth": ["beam pumping unit", "light load", "improve the power factor and efficiency of the beam pumping unit", "beam pumping unit", "light load", "change the voltage", "changing the thyristor trigger angle", "analyze the change of the cycles of the load overall", "divide the load", "equal parts", "constant load", "optimal voltage", "current load", "calculated by genetic algorithm", "genetic algorithm", "optimal voltage", "whole optimal voltage changeable rule.", "energy saving", "beam pumping", "reducing a certain voltage", "thyristor trigger angle"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "beam pumping unit", "load", "changeable", "light load", "voltage reduction", "power factor", "efficiency", "thyristor trigger angle", "load cycles", "constant load", "optimal voltage", "genetic algorithm", "voltage changeable rule", "energy saving", "these keywords capture the main technical concepts discussed in the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Based on the description model of object-orientation-based direction relation in two-dimensional space, the description mode of object-orientation-based direction relation in three-dimensional space is proposed. The basic idea is that the actual direction region is modeled as an open shape. The computation related to the world boundary of spatial direction region is eliminated, and the processing of the direction predicates is converted into the processing of topological operations between open shapes and closed geometry objects. The algorithms of topological operations between open shapes and closed geometry objects are presented and the theoretical proof for the correctness and completeness of the algorithms is performed.", "ground_truth": ["description model", "object-orientation-based direction relation", "two-dimensional space", "description mode of object-orientation-based direction relation in three-dimensional space is proposed", "description mode", "direction region", "open shape", "computation", "world boundary", "spatial direction region", "processing of the direction predicates", "processing of topological operations", "open shapes", "geometry objects", "algorithms of topological operations", "open shapes", "geometry objects", "theoretical proof for the correctness and completeness of the algorithms", "algorithms"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "object-orientation-based direction relation", "two-dimensional space", "three-dimensional space", "description mode", "direction region", "open shape", "world boundary", "spatial direction region", "direction predicates", "topological operations", "closed geometry objects", "algorithms", "correctness", "completeness", "theoretical proof", "these keywords capture the core concepts and technical aspects discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The paper deals with the computation of distribution network components reliability parameters. Knowledge of the component reliability parameters in power networks is necessary for the reliability computation and also for reliability-centered maintenance system. Component reliability parameters are possible to retrieve only with accurate databases of distribution companies. Such a database includes records of outages and interruptions in power networks. It is impossible to retrieve reliability parameters from this data in a direct way because of heterogeneity. In this paper, we introduce some results of databases calculations. We apply this framework for the retrieving of parameters from outage data in the Czech and Slovak republics. There are also actual results.", "ground_truth": ["computation of distribution network components reliability parameters", "computation", "component reliability parameters", "power networks", "reliability computation", "reliability-centered maintenance system", "component reliability parameters", "accurate databases of distribution companies", "retrieve", "database", "records of outages and interruptions", "power networks", "retrieve reliability parameters", "data", "heterogeneity", "introduce some results of databases calculations", "framework for the retrieving of parameters from outage data"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "computation", "distribution network", "components", "reliability parameters", "power networks", "reliability computation", "reliability-centered maintenance system", "accurate databases", "distribution companies", "outages", "interruptions", "heterogeneity", "databases calculations", "framework", "outage data", "czech republic", "slovak republic", "actual results"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "ground_truth": ["conceptual methodology to control liquid state", "al-si alloys", "melting and holding sub-process", "pressure die-casting process", "determine the characteristic of the holding furnace", "holding furnace", "weight percent", "wt %", "alloys", "application of methodology of research", "characteristic of holding furnace", "real condition", "foundry", "horizontal cold chamber machine", "chemical analysis", "spectrophotometer", "extend this methodology in practice", "clh 400.1", "spectrolab jr.ccd 2000"], "prediction_list": ["here are the extracted keywords from the paragraph:", "conceptual methodology", "liquid state", "al-si alloys", "melting", "holding sub-process", "pressure die-casting process", "holding furnace", "weight percent (wt %)", "alloys", "elements", "methodology of research", "real conditions", "foundry", "horizontal cold chamber machine", "clh 400.1", "chemical analysis", "spectrophotometer"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Our country is rich of line galloping, there are many important galloping data failed to collect systematically and completely because there is no unified management platform. After the galloping occurrence in 2009–2010's winter the department of productive of the State Grid Corporation organized a lot of human to carry out the research of galloping information, this work is time–consuming and inefficient. The State Grid Corporation has used the production management system (PMS) which is a powerful and easy to use. With the help of the system we can create a galloping database which can save resources and storage the galloping data. To build and put it into application of database can provide technical support for line galloping prevention and galloping research work.", "ground_truth": ["line galloping", "galloping data", "collect systematically and completely", "unified management platform", "galloping occurrence", "carry out the research of galloping information", "", "production management system", "pms", "create a galloping database", "save resources and storage the galloping data", "galloping data", "application of database", "technical support", "line galloping prevention", "galloping research work", "galloping", "galloping", "galloping"], "prediction_list": ["here are the extracted keywords from the paragraph:", "line galloping", "galloping data", "unified management platform", "state grid corporation", "galloping information", "time-consuming", "inefficient", "production management system (pms)", "galloping database", "resources", "data storage", "technical support", "galloping prevention", "galloping research", "let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This paper presents a non-fragile controller design method based on system quadratic performance optimization. For the additive controller gain variations, the necessary and sufficient conditions for the existence of non-fragile state feedback controller are given and transformed to the LMI problems, which simplifies the solutions to obtain non-fragile state feedback controllers. The flight control simulation results prove the reliability and validity of the method.", "ground_truth": ["non-fragile controller design method", "system quadratic performance optimization", "additive controller gain variations", "non-fragile state feedback controller", "lmi problems", "transformed", "simplifies the solutions", "non-fragile state feedback controllers", "flight control simulation"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "non-fragile controller", "design method", "system quadratic performance optimization", "additive controller gain variations", "necessary and sufficient conditions", "state feedback controller", "lmi problems", "solutions", "flight control simulation", "reliability", "validity", "these keywords capture the core concepts and technical aspects discussed in the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A fuzzy-Hammerstein model predictive control method is proposed for a continuous stirred-tank reactor (CSTR). In this paper T-S fuzzy model is used to approximate the static nonlinear characteristics of Hammerstein model, and a linear autoregressive model is used to solve the results of optimal control. The designed nonlinear predictive controller using Hammerstein model make good use of the ability of universal approach nonlinear of T-S model, and divide the question of nonlinear predictive control into the nonlinear model recongnization and the question of linear predictive control. The application results of CSTR process show the proposed control method has good control performance compared to PID controller.", "ground_truth": ["fuzzy-hammerstein model predictive control method", "fuzzy-hammerstein model", "continuous stirred-tank reactor", "cstr", "t-s fuzzy model", "static nonlinear characteristics of hammerstein model", "linear autoregressive mode", "optimal control", "nonlinear predictive controller", "hammerstein model", "hammerstein model", "universal approach nonlinear", "t-s model", "nonlinear predictive control", "nonlinear model recongnization", "linear predictive control", "cstr", "cstr process", "control method", "pid controller"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "fuzzy-hammerstein model", "model predictive control", "continuous stirred-tank reactor (cstr)", "t-s fuzzy model", "static nonlinear characteristics", "hammerstein model", "linear autoregressive model", "optimal control", "nonlinear predictive controller", "universal approximation", "nonlinear model recognition", "linear predictive control", "cstr process", "control performance", "pid controller", "these"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The key point of robot dynamics is optimal design and control. The efficiency of robot dynamics has been the goal of researchers in recent years. Screws are used to describe dynamic problems in this paper, and an O(N) recursive robot forward dynamic algorithm is given on this. It can be easily extended to tree topology, closed loop and spatial robot systems. And three classic methods of robot dynamics are compared for easy of use. The results show that dynamics described with screws are helpful in high efficient dynamics modelling. The dynamical expressions based on screws are concise and clear. It's efficiency is high of O(N) and is linear to the degree of freedom. With the improvement of computation efficiency, it will make the real-time dynamics control become possible.", "ground_truth": ["robot dynamics", "robot dynamics", "efficiency of robot dynamics", "screws", "describe dynamic problems", "o(n) recursive robot", "o(n) recursive robot forward dynamic algorithm", "tree topology", "closed loop and spatial robot systems", "robot dynamics", "dynamics described with screws", "screws", "high efficient dynamics modelling.", "dynamical expressions", "screws", "linear to the degree of freedom", "improvement of computation efficiency", "real-time dynamics control"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "robot dynamics", "optimal design", "control", "efficiency", "researchers", "screws", "dynamic problems", "o(n) recursive algorithm", "robot forward dynamic algorithm", "tree topology", "closed loop", "spatial robot systems", "classic methods", "dynamics modelling", "high efficiency", "concise expressions", "degree of freedom", "computation efficiency", "real-time dynamics control"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "An algorithm of multi-axis NC tool-path generation for subdivision surfaces is proposed. The algorithm includes two steps: model building and tool path generation. In the section of model building, in order to obtain the deformed surface, the deformation vector is computed which is associated with the curvature and the slope of cutter location surface. In the procedure of tool path generation, the slicing procedure is adopted to get the CL points. In addition, the inversely converted method is used. The method is tested by some examples with actual machining. The results show that the method can effectively reduce the error of the scallop height for subdivision surface and obtain the better shape and quality. In addition, the computational complexity and is scalable and robust.", "ground_truth": ["algorithm of multi-axis nc tool-path generation", "subdivision surfaces", "algorithm of multi-axis nc tool-path generation for subdivision surfaces", "algorithm", "model building", "tool path generation", "model building", "obtain the deformed surface", "deformation vector", "deformation vector is computed", "curvature and the slope of cutter location surface", "cutter location surface", "tool path generation", "slicing procedure", "cl points", "inversely converted method", "tested by some examples with actual machining", "scallop height", "subdivision surface", "some examples with actual machining"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "algorithm", "multi-axis nc tool-path generation", "subdivision surfaces", "model building", "tool path generation", "deformed surface", "deformation vector", "curvature", "slope", "cutter location surface (cl surface)", "slicing procedure", "cl points (cutter location points)", "inversely converted method", "actual machining", "scallop height error"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Modeling or approximating high dimensional, computationally-expensive problems faces an exponentially increasing difficulty, the “curse of dimensionality”. This paper proposes a new form of high dimensional model representation (HDMR) by utilizing the support vector regression (SVR), termed as adaptive SVR-HMDR, to conquer this dilemma. The proposed model could reveal explicit correlations among different input variables of the underlying function which is unknown or expensive for computation. Taking advantage of HDMR's hierarchical structure, it could alleviate the exponential increasing difficulty, and gain satisfying accuracy with small set of samples by SVR. Numerical examples of different dimensionality are given to illustrate the principle, procedure and performance of SVR-HDMR.", "ground_truth": ["modeling or approximating high dimensional", "computationally-expensive problems", "curse of dimensionality", "new form of high dimensional model representation", "high dimensional model representation", "hdmr", "support vector regression", "svr", "adaptive svr-hmdr", "conquer this dilemma", "explicit correlations", "input variables", "computation", "hdmr", "alleviate the exponential increasing difficulty", "svr", "illustrate the principle", "procedure and performance", "svr-hdmr", "modeling", "approximating"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "modeling", "approximating", "high dimensional", "computationally-expensive problems", "curse of dimensionality", "high dimensional model representation (hdmr)", "support vector regression (svr)", "adaptive svr-hdmr", "explicit correlations", "input variables", "underlying function", "hierarchical structure", "exponential increasing difficulty", "accuracy", "small set of samples"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Metal–intermetallic laminated (MIL) composites are fabricated upon reaction sintering of titanium and aluminum foils of various thicknesses. The intermetallic phase of Al3Ti forming during the above processing gives high hardness and stiffness to the composite, while unreacted titanium provides the necessary high strength and ductility. Some results of studies of microstructure and some mechanical properties of layered composites are presented on the example of Ti-Al system. Static and dynamic tests results are discussed for the case when the intermetallic reaction was interrupted in the course of intermetallic sintering and also for the case when it was completed.", "ground_truth": ["metal–intermetallic laminated", "mil", "fabricated upon reaction sintering", "titanium and aluminum foils", "intermetallic phase", "al3ti forming", "processing", "composite", "unreacted titanium", "studies of microstructure", "layered composites", "presented on the example of ti-al system", "ti-al system", "static and dynamic tests", "intermetallic reaction", "interrupted", "intermetallic sintering", "al3ti", "static and dynamic tests results are discussed", "reaction sintering"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "metal–intermetallic laminated (mil) composites", "reaction sintering", "titanium", "aluminum", "foils", "thicknesses", "intermetallic phase", "al3ti", "hardness", "stiffness", "unreacted titanium", "strength", "ductility", "microstructure", "mechanical properties", "layered composites", "ti-al system", "static tests", "dynamic tests", "intermetallic reaction", "interrupted reaction", "completed reaction", "intermetallic sintering", "let me"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "A design method for network attack and defense simulation platform is discussed in this paper. Firstly the component and function of the platform are analyzed. Then Visio second development method is used to construct the virtual network topology. The parsing of virtual network topology is also researched and the relative flow sheet is described. Lastly an example is carried out to test performance of the platform. Simulation results show the effectiveness of the proposed method.", "ground_truth": ["design method", "network attack", "defense simulation platform", "visio second development method", "construct the virtual network topology", "parsing of virtual network topology", "relative flow sheet", "test performance of the platform", "test performance", "platform", "virtual network", "virtual network"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "design method", "network attack and defense", "simulation platform", "component and function", "visio second development", "virtual network topology", "parsing", "flow sheet", "performance test", "simulation results", "effectiveness", "these keywords capture the main concepts and technical aspects discussed in the paragraph."]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The existing GO methodology algorithm is theoretical, and hard to solve with computer. In this paper, we research a new method to get the reliability of system based on GO methodology. According to some properties of the operators in GO chart, GO chart can be transformed into series structure, then the minimal path sets are induced based on Enumeration method from first operator to last one. It is very convenient for computer to calculate the system reliability with the new method based on minimal path sets. The case study indicates the method is suitable for practical engineering, which can be used to possess the quantitative analysis of complex GO methodology models.", "ground_truth": ["go methodology algorithm", "new method to get the reliability of system based on go methodology", "go methodology", "go chart", "go chart", "series structure", "minimal path sets are induced", "enumeration method", "calculate the system reliability", "minimal path sets", "practical engineering", "quantitative analysis", "complex go methodology models"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "go methodology", "algorithm", "theoretical", "computer", "reliability", "system", "operators", "go chart", "series structure", "minimal path sets", "enumeration method", "quantitative analysis", "complex models", "case study", "practical engineering"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "ground_truth": ["hidden nodes", "generalization of elm", "time consumption", "hidden nodes", "trial-and-error", "novel algorithm is proposed", "optimize the hidden node number", "hidden node", "generalization", "optimization process", "structural risk minimization principle", "algorithm", "novel algorithm", "hidden nodes", "accuracy comparisons", "obtain the optimal number of hidden nodes", "pso", "6 datasets", "elm"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "hidden nodes", "generalization", "elm (extreme learning machine)", "time consumption", "optimal number", "trial-and-error", "novel algorithm", "optimize", "pso (particle swarm optimization)", "structural risk minimization", "simulation results", "reasonable", "feasible", "datasets", "benchmark problems", "accuracy comparisons"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "For providing the government with effective monitoring of the trends of the economic variables in the future and good reference for developing a reasonable policy, in this paper, we establish a time series model on China's Foreign Direct Investment (FDI) by using wavelet analysis and intervention analysis and time series analysis and predict the trend of FDI in the next several years. This model eliminates the interference of noise for predicting by using wavelet analysis, and describes the autocorrelation and time-varying volatility of the financial time series by using ARIMA- GARCH-M model. The simulation results show that this model explains the dynamic structure of China's FDI trends well.", "ground_truth": ["effective monitoring", "trends of the economic variables", "developing a reasonable policy", "establish a time series model", "foreign direct investment", "fdi", "wavelet analysis", "intervention analysis", "time series analysis", "predict the trend of fdi", "fdi", "interference of noise", "wavelet analysis", "autocorrelation", "time-varying volatility", "financial time series", "arima- garch-m model", "fdi", "time series model"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "government", "monitoring", "trends", "economic variables", "policy", "time series model", "china", "foreign direct investment (fdi)", "wavelet analysis", "intervention analysis", "time series analysis", "prediction", "noise interference", "autocorrelation", "time-varying volatility", "financial time series", "arima-garch-m model"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Monitoring the wear condition of the tramway superstructure is one of the key points to guarantee an adequate safety level of the light rail transport system. The purpose of this paper is to suggest a new non-conventionalprocedure for measuring the transverse profile of rails in operation by means of image-processing technique. This methodological approach is based on the “information” contained in high-resolution photographic images of tracks and on specific algorithms which allow to obtain the exact geometric profile of the rails and therefore to measure the state of the rail-head extrados wear.", "ground_truth": ["wear condition", "tramway superstructure", "light rail transport system", "safety level", "monitoring the wear condition", "new non-conventionalprocedure", "transverse profile", "rails", "image-processing technique", "information”", "high-resolution photographic images", "tracks", "specific algorithms", "rails"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "wear condition", "tramway superstructure", "safety level", "light rail transport system", "non-conventional procedure", "measuring", "transverse profile", "rails", "operation", "image-processing technique", "methodological approach", "high-resolution photographic images", "tracks", "specific algorithms", "geometric profile", "rail-head extrados wear"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Robust and automatic thresholding of gray level images has been commonly used in the field of pattern recognition and computer vision for objects detecting, tracking and recognizing. The Otsu scheme, a widely used image thresholding technique, provides approving results for segmenting a gray level image with only one modal distribution in gray level histogram. However, it provides poor results if the histogram of a gray level is non-bimodal. For enhancing the performance of the Otsu algorithm further, in this work, an improved median-based Otsu image thresholding algorithm is presented. Finally extensive tests are performed and the experiments show that our method obtain more satisfactory results than the original Otsu thresholding algorithm.", "ground_truth": ["thresholding", "gray level images", "pattern recognition", "computer vision", "detecting", "tracking", "recognizing", "otsu scheme", "image thresholding technique", "segmenting", "gray level image", "modal distribution", "gray level histogram.", "enhancing the performance", "otsu algorithm", "median-based otsu image thresholding algorithm", "extensive tests", "otsu thresholding algorithm", "objects detecting", "tracking and recognizing", "robust and automatic thresholding", "histogram"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "robust", "automatic thresholding", "gray level images", "pattern recognition", "computer vision", "objects detecting", "tracking", "recognizing", "otsu scheme", "image thresholding technique", "segmenting", "modal distribution", "gray level histogram", "non-bimodal", "performance enhancement", "improved median-based otsu", "algorithm", "extensive tests", "experiments", "satisfactory results", "original otsu thresholding"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "In this paper, a novel position estimation method of prism was proposed for single-lens stereovision system. The prism with multi faces was considered as a single optical system composed of some refractive planes. A transformation matrix which can express the relationship between an object point and its image by the refraction of prism was derived based on geometrical optics, and a mathematical model was introduced which can denote the position of prism with arbitrary faces only by 7 parameters. This model can extend the application of single-lens stereovision system using prism to a more widely area. Experimentation results are presented to prove the effectiveness and robustness of our proposed model.", "ground_truth": ["position estimation method", "prism", "single-lens stereovision system", "prism", "single optical system", "refractive planes", "transformation matrix", "express the relationship", "refraction of prism", "geometrical optics", "mathematical model", "prism", "extend the application of single-lens stereovision system", "single-lens stereovision system", "prism", "effectiveness", "robustness", "object point", "image", "prism", "single-lens stereovision system"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "position estimation method", "prism", "single-lens stereovision system", "multi faces", "optical system", "refractive planes", "transformation matrix", "object point", "image refraction", "geometrical optics", "mathematical model", "7 parameters", "arbitrary faces", "application extension", "experimentation results", "effectiveness", "robustness"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Power Grid reasoning expert system is a complex system. To solve knowledge sharing of knowledge Base in expert system, we abstract and analyze the power grid security investigation procedure by using ontology Technology. With ontology-based Power Grid knowledge base, we establish associated relationship of procedure vocabularies. In this paper, we introduce and analyze of semantic reasoning tools such as Jena. The reasoner mechanism and inference rules of grammar has been included and explained. At last we give a specific application of security investigation procedure ontology and reasoning.", "ground_truth": ["power grid reasoning expert system", "system", "knowledge base", "expert system", "abstract and analyze the power grid security investigation procedure", "power grid security investigation procedure", "ontology technology", "ontology-based power grid knowledge base", "associated relationship of procedure vocabularies", "semantic reasoning tools", "jena", "reasoner mechanism", "inference rules of grammar", "specific application of security investigation procedure ontology and reasoning", "security investigation procedure ontology and reasoning", "knowledge sharing of knowledge base in expert system", "introduce and analyze of semantic reasoning tools"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "power grid", "reasoning", "expert system", "knowledge sharing", "knowledge base", "ontology technology", "security investigation procedure", "semantic reasoning tools", "jena", "reasoner mechanism", "inference rules", "grammar", "application", "these keywords capture the main concepts discussed in the paragraph. let me know if you'd like any refinements!"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "This phase was completed in 2005. Previous contracts had been procured with the contractor providing the detailed design. For this system the design was undertaken by Mott MacDonald. It was developed by looking at the systems installed previously and calculating what was actually required to achieve cathodic protection of the piers. This resulted in a significant reduction in the number of zones and monitoring probes. The varying amounts of steelwork in the beams had previously lead to up to 5 zones per beam, with multiple layers of mesh to achieve the design current density. On review of the data the operating current density was similar in all zones and so this was reduced to a single zone per beam. The encapsulation was susceptible to ASR and contained post tensioning and so it was decided to use a galvanic system based on Galvashield CC anodes from Fosroc. Our design did not include an option to allow depolarization of the galvanic system, but the contractor supplied one, such that the anodes could be remotely disconnected. The control unit was from Electrotech CP and operated via a broadband connection provided by the contractor.", "ground_truth": ["detailed design", "looking at the systems installed previously and calculating", "systems installed previously", "significant reduction in the number of zones and monitoring probes", "varying amounts of steelwork", "multiple layers of mesh", "data", "cathodic protection of the piers", "encapsulation", "galvanic system", "depolarization of the galvanic system", "broadband connection", "asr", "galvashield cc anodes", "galvanic system"], "prediction_list": ["here are the extracted keywords from the paragraph:", "2005", "contracts", "contractor", "detailed design", "mott macdonald", "systems", "cathodic protection", "piers", "reduction", "zones", "monitoring probes", "steelwork", "beams", "mesh", "design current density", "operating current density", "asr (alkali-silica reaction)", "post tensioning", "galvanic system", "galvashield cc", "fosroc", "depolarization", "remotely disconnected", "control unit", "electrotech cp", "broadband connection"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "An innovative sound wall system was developed in the University of Western Ontario, and was examined to serve as a vertical extension to the existing sound walls. The wall system (denoted as flexi-wall) consists of stay-in-place poly-blocks as formwork, light polyurethane foam (LPF) reinforced with steel rebars as structural cores and polyurea as a coating of the wall surfaces (Fig. 1). Poly-blocks are interlocking light-weight blocks which are stacked up layer by layer and act as formwork for the LPF cores. The poly-block is 20×20×80cm3 and includes four cylindrical voids with 14cm diameter. It is made of molded low-density polyurethane and weighs approximately 1kg. The poly-blocks are fire-resistant blocks and have an excellent capability to absorb, mitigate and reflect a wide range of noises with unmatched frequency of reflective noise. Polyurea coating is an abrasion-resistant finishing layer, which is sprayed on the surfaces of the wall and sets within 2–3min. This layer also enhances the surface resistance of poly-blocks against stone impact, weathering, fire development, chemicals and penetration. LPF is an expanding liquid mixture which is injected into the poly-block voids and cures within 10min. Steel rebars are epoxied into holes drilled in the existing sound wall to connect the wall extension to its base.", "ground_truth": ["sound wall system", "existing sound walls", "wall system", "flexi-wall", "stay-in-place poly-blocks", "light polyurethane foam", "lpf", "steel rebars", "structural cores", "polyurea", "coating", "wall surfaces", "poly-blocks", "interlocking light-weight blocks", "lpf cores", "poly-block", "cylindrical voids", "molded low-density polyurethane", "poly-blocks", "fire-resistant blocks", "absorb", "mitigate", "reflect", "polyurea coating", "abrasion-resistant finishing layer", "sprayed", "surfaces of the wall", "layer", "enhances the surface resistance", "poly-blocks", "stone impact", "weathering", "fire development", "chemicals", "penetration", "lpf", "expanding liquid mixture", "poly-block voids", "cures", "steel rebars", "holes", "existing sound wall", "wall extension", "base"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "innovative sound wall system", "university of western ontario", "vertical extension", "existing sound walls", "flexi-wall", "stay-in-place poly-blocks", "formwork", "light polyurethane foam (lpf)", "steel rebars", "structural cores", "polyurea", "coating", "wall surfaces", "interlocking", "lightweight blocks", "lpf cores", "cylindrical voids", "molded low-density polyurethane", "fire-resistant blocks", "absorb noise", "mitigate noise", "reflect noise", "reflective noise", "abrasion-resistant"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Another important reason for the damages incurred by the RC buildings is workmanship defects. It is understood that granulometry of the handmade concretes was not in compliance with the standards since the aggregate utilized in them was not sieved. Also the compaction process was not properly implemented in general in the installment of concrete in RC buildings. This situation resulted in the concrete to exhibit an excessively porous structure. The most fundamental rules of thumb of construction, namely concrete cover, was not taken care of in formwork workmanship. Faults in the connections of stirrups to the longitudinal bars, unstaggered formation of stirrup hooks in beams and columns, the perpendicular angles of the hooks, inadequately anchorage lengths of the stirrup hooks and longitudinal bars, and the use of cold joints were the other frequently encountered workmanship defects (Figs. 19–22).", "ground_truth": ["workmanship defects", "granulometry of the handmade concretes", "compaction process", "installment of concrete", "concrete cover", "faults in the connections of stirrups", "unstaggered formation of stirrup", "perpendicular angles", "inadequately anchorage lengths", "use of cold joints", "workmanship defects", "concrete", "stirrup hooks", "longitudinal bars", "hooks", "beams", "columns", "hooks"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "damages", "rc buildings", "workmanship defects", "granulometry", "handmade concretes", "standards compliance", "aggregate", "sieved", "compaction process", "concrete installment", "porous structure", "construction rules", "concrete cover", "formwork workmanship", "stirrups", "longitudinal bars", "unstaggered formation"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "The exponential relationships reported in the plots may be used to convert the dielectric values to air void values as prescribed in previous studies [1–3]. The AC pavement composite permittivity reduces, along with the reflection coefficient, as the volumetric proportion of air increases as compared to the remaining components. However, the method relies on an empirical fit, determined on a case-by-case basis, since the permittivity of the remaining components depends on the mix design (aggregate type, binder content, etc.). Long term studies in Finland concluded that this empirical fit is an exponential relationship [1]. The exponential fits, using a sufficient amount of cores, can be used to map the air void content variation in a similar manner to the dielectric maps shown in Fig. 4. Only 4 cores were feasible due to various factors involved with testing the final lift of an in-service pavement. More cores are needed for more stable exponential coefficients, although the limited cores show that the predicted relationships are similar for the measured dielectric range in this case-study. Since both regressions predict air void content at a maximum difference of 0.56%, which is within the uncertainty of the core measurement precision of 0.7%, use of either the initial or repeat run regression predictions are appropriate.", "ground_truth": ["exponential relationships", "convert the dielectric values to air void values", "ac pavement composite permittivity", "volumetric proportion of air", "mix design", "long term studies in finland", "exponential relationship", "sufficient amount of cores", "map the air void content variation", "dielectric maps", "testing the final lift", "predicted relationships", "more stable exponential coefficients", "measured dielectric range", "regressions", "air void content", "core measurement precision", "initial or repeat run regression predictions"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "exponential relationships", "dielectric values", "air void values", "previous studies", "ac pavement", "composite permittivity", "reflection coefficient", "volumetric proportion", "empirical fit", "case-by-case basis", "mix design", "aggregate type", "binder content", "long term studies", "finland", "exponential relationship", "exponential fits", "cores", "air void content variation", "dielectric maps", "in-service pavement", "stable exponential coefficients", "predicted relationships", "case-study", "regressions", "maximum difference", "uncertainty", "core"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1–0.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.", "ground_truth": ["microct", "preliminary results", "visualization of defects", "porosity in am components", "porosity structures in parts", "average porosity", "build direction", "electron beam raster and overlap pattern.", "recent reports", "porosity distribution", "electron beam melted samples", "am parts", "porosity images", "characterize am parts", "comparison of the part to its design model", "characterization of surface roughness", "demonstrate a specific type of defect", "very low average porosity levels", "not follow the build direction", "porosity structure changes", "hot isostatic pressing", "hip", "microct", "microct"], "prediction_list": ["here are the extracted keywords from the scientific paragraph:", "microct", "am parts", "defects", "porosity", "visualization", "components", "improper settings", "average porosity", "build direction", "electron beam raster", "overlap pattern", "porosity distribution", "build strategy", "electron beam melted samples", "porosity images", "porosity structure", "design model", "surface roughness", "hot isostatic pressing (hip)", "hip treatment", "low average porosity"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Aeroengine turbine disks often consist of paramagnetic, that means non-ferromagnetic Nickel based alloys. Sometimes, parasitic small ferromagnetic particles can be included in these disks that may decrease the mechanical stability. For this reason, in case of a suspicion disks are to be analysed with respect to ferromagnetic inclusions. These inclusions generate a magnetic density which can be measured by a flux gate magnetometer using the magnetic remanence method [1]. The detection principle of ferromagnetic impurities in non-magnetic metallic materials is based on their remanence. Before such a measurement can be carried out, the aeroengine turbine disks are premagnetised in axial direction. As ferromagnetic materials show the well-known hysteresis behaviour, those materials can be magnetised by a strong magnetic field which drives the magnetic material into saturation. When removing the magnetic field, the remanence is left. This remaining flux density is used to detect them in non-magnetic materials.", "ground_truth": ["aeroengine turbine disks", "paramagnetic", "non-ferromagnetic nickel based alloys", "parasitic small ferromagnetic particles", "decrease the mechanical stability", "ferromagnetic inclusions", "magnetic density", "flux gate magnetometer", "magnetic remanence method", "detection principle of ferromagnetic impurities", "premagnetised", "hysteresis behaviour", "strong magnetic field", "measurement", "removing the magnetic field", "remaining flux density"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "aeroengine turbine disks", "paramagnetic", "non-ferromagnetic", "nickel based alloys", "ferromagnetic particles", "mechanical stability", "ferromagnetic inclusions", "magnetic density", "flux gate magnetometer", "magnetic remanence method", "ferromagnetic impurities", "non-magnetic metallic materials", "remanence", "premagnetised", "axial direction", "hysteresis behaviour", "magnetic field", "saturation", "remaining flux density", "detection"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Although the presented model is developed and tested with a-C:H layers in mind, it is not necessarily limited to them. Moreover, the only assumptions are chemical reactions between the gas and the solid forming volatiles, the loss of these volatiles from the material and the two stated boundary conditions of gas influx at a single outer surface and the possibility of reactions throughout the bulk. Porosity and significant gas inventories were observed not only for carbon [12] but, e.g. also for beryllium co-deposits [25] and can be expected for other co-deposits formed in plasma devices [1]. Thus, TCR and its description by the presented model may be applicable to all deposits. If a layer has constituents that are not forming volatiles with the reactive gas, e.g. W and Be with O2, these constituents cannot be removed by TCR, as they will not be removed from the deposit. This can influence the removal of other deposit constituents and the time evolution of the process can change. The new understanding of TCR may, for the first time, allow applying the method in a controlled way to nuclear fusion devices, possibly solving the tritium retention issue especially related to carbon based materials.", "ground_truth": ["a-c:h layers", "chemical reactions", "gas", "solid forming volatiles", "gas", "gas influx", "reactions", "single outer surface", "gas inventories", "carbon", "beryllium co-deposits", "plasma devices", "tcr", "reactive gas", "w and be with o2", "constituents that are not forming volatiles with the reactive gas", "tcr", "deposit constituents", "tcr", "nuclear fusion devices", "tritium retention issue", "tritium", "carbon based materials", "tritium retention", "porosity"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "model", "a-c:h layers", "chemical reactions", "gas", "solid", "volatiles", "material loss", "boundary conditions", "gas influx", "outer surface", "reactions", "bulk", "porosity", "gas inventories", "carbon", "beryllium co-deposits", "plasma devices", "tcr (thermo-chem"]}
{"instruction": "Extract all keywords from the given scientific paragraph.", "input": "Power and particle exhaust are crucial for the viability of any future fusion power plant concept. Heat in fusion reactors must be extracted through a wall and cannot be exhausted volumetrically, which limits the allowed power density in fusion reactors [1] and is a severe technical challenge in itself [2]. In addition, structural material changes resulting from neutron irradiation cause degradation in the heat exhaust capabilities of existing designs [3] and static surfaces can suffer severely from erosion due to impinging plasma particles [4,5]. It is concluded that conventional concepts and materials for plasma facing components (PFCs) reach their limits in terms of material lifetime and power exhaust at approximately 20MW/m2, which is presumably dramatically reduced to <10MW/m2 due to neutron damage in a D-T reactor [6] or even only half that value [7].", "ground_truth": ["fusion reactors", "fusion reactors", "static surfaces", "power and particle exhaust", "neutron irradiation", "plasma facing components", "pfcs", "d-t reactor", "structural material changes", "structural material", "impinging plasma particles", "neutron", "neutron damage", "fusion power plant", "severe technical challenge"], "prediction_list": ["here are the extracted keywords from the given scientific paragraph:", "power exhaust", "particle exhaust", "fusion power plant", "fusion reactors", "heat extraction", "power density", "technical challenge", "structural material changes", "neutron irradiation", "heat exhaust capabilities", "erosion", "plasma particles", "plasma-facing components (pfcs)", "material lifetime", "d-t reactor", "neutron damage", "mw/m² (power load limits)", "these keywords capture the main concepts and technical terms discussed in the paragraph."]}
